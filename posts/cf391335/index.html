<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="icon" type="image/png" sizes="32x32" href="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/favicon.ico">
  <link rel="alternate" href="/atom.xml" title="Houmin" type="application/atom+xml">
  <meta name="google-site-verification" content="zdGhdEF7jHoJW58lsdN6l9JrQFjJFwakCIc7TbbosV0">
  <meta name="msvalidate.01" content="2F527B379ED5537861D0D38C2C754C2B">
  <meta name="baidu-site-verification" content="xAag2PqzKE">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":true},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":true,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="原生的 k8s 基于 Device Plugin 和 Extended Resource 机制实现了在容器中使用GPU，但是只支持GPU的独占使用，不允许在Pod间共享GPU，这大大降低了对集群中GPU的利用率。为了在集群层面共享GPU，我们需要实现GPU资源的隔离与调度，本文将依次介绍阿里的 GPUShare 与腾讯的 GPUManager，分析其实现机制。">
<meta name="keywords" content="k8s,GPU,device plugin,资源隔离,scheduler extender">
<meta property="og:type" content="article">
<meta property="og:title" content="【Kubernetes】GPU 共享">
<meta property="og:url" content="http://houmin.cc/posts/cf391335/index.html">
<meta property="og:site_name" content="Houmin">
<meta property="og:description" content="原生的 k8s 基于 Device Plugin 和 Extended Resource 机制实现了在容器中使用GPU，但是只支持GPU的独占使用，不允许在Pod间共享GPU，这大大降低了对集群中GPU的利用率。为了在集群层面共享GPU，我们需要实现GPU资源的隔离与调度，本文将依次介绍阿里的 GPUShare 与腾讯的 GPUManager，分析其实现机制。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share.jpg">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-filter.jpg">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-bind.jpg">
<meta property="og:image" content="https://github.com/AliyunContainerService/gpushare-scheduler-extender/raw/master/docs/designs/sequence.jpg">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-gpu-manager.png">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-19_gpu-manager-predicate.png">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png">
<meta property="og:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-vcuda.png">
<meta property="og:updated_time" content="2020-12-08T02:24:52.471Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share.jpg">

<link rel="canonical" href="http://houmin.cc/posts/cf391335/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>【Kubernetes】GPU 共享 | Houmin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


  <script src="/js/photoswipe.min.js?v="></script>
  <script src="/js/photoswipe-ui-default.min.js?v="></script>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Houmin</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">Yesterday You Said Tomorrow</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-album">

    <a href="/album" rel="section"><i class="fa fa-fw fa-camera"></i>相册</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-fw fa-film"></i>观影</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-fw fa-book"></i>阅读</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://houmin.cc/posts/cf391335/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/avatar.png">
      <meta itemprop="name" content="Houmin">
      <meta itemprop="description" content="丈夫拥书万卷，何假南面百城">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Houmin">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          【Kubernetes】GPU 共享
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-18 11:11:07" itemprop="dateCreated datePublished" datetime="2020-11-18T11:11:07+08:00">2020-11-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/" itemprop="url" rel="index">
                    <span itemprop="name">术业专攻</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/posts/cf391335/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/posts/cf391335/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>31k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>57 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>原生的 k8s 基于 <code>Device Plugin</code> 和 <code>Extended Resource</code> 机制实现了在容器中使用GPU，但是只支持GPU的独占使用，不允许在Pod间共享GPU，这大大降低了对集群中GPU的利用率。为了在集群层面共享GPU，我们需要实现GPU资源的隔离与调度，本文将依次介绍阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 与腾讯的 <a href="https://github.com/tkestack/gpu-manager" target="_blank" rel="external nofollow noopener noreferrer">GPUManager</a>，分析其实现机制。</p>
<a id="more"></a>
<h2 id="阿里GPUShare"><a href="#阿里GPUShare" class="headerlink" title="阿里GPUShare"></a>阿里GPUShare</h2><p>阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 基于 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="external nofollow noopener noreferrer">Nvidia Docker2</a> 和他们的 <a href="https://docs.google.com/document/d/1ZgKH_K4SEfdiE_OfxQ836s4yQWxZfSjS288Tq9YIWCA/edit#heading=h.r88v2xgacqr" target="_blank" rel="external nofollow noopener noreferrer">gpu sharing design</a> 设计而实现的，为了使用阿里的GPUShare，首先需要配置Node上的 Docker Runtime 并安装 <code>NVIDIA Docker 2</code>，具体过程可以参考 <a href="../574111db">在Docker中使用GPU</a>。</p>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><h4 id="假设条件"><a href="#假设条件" class="headerlink" title="假设条件"></a>假设条件</h4><ul>
<li>尽管GPU可以从 CUDA Cores 和 GPU Memory 两个维度来衡量GPU的能力，<strong>在推理的场景，我们可以假定CUDA core的数量和GPU  Memory的大小是成比例的</strong></li>
<li>在模型开发和推理的场景下，<strong>用户申请的GPU资源不超过1个GPU，也就是说 resource limit 是 一个GPU</strong></li>
<li>每个Node上所有卡的GPU Memory相同，这样可以通过 <code>gpuTotalMemory</code> 和 <code>gpuTotalCount</code> 算出Node上每张卡的GPU Memory</li>
</ul>
<h4 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h4><ul>
<li><p>设计里定义了两种 <code>Extended Resource</code>：</p>
<ul>
<li><code>aliyun.com/gpu-mem</code>： 单位从 <code>number of GPUs</code> 变更为 <code>amount of GPU memory in MiB</code>，如果一个Node有多个GPU设备，这里计算的是总的GPU Memory</li>
<li><code>aliyun.com/gpu-count</code>：对应于Node上的GPU 设备的数目</li>
</ul>
</li>
<li>基于k8s原生的Scheduler Extender、Extended Resource、DevicePlugin机制来实现</li>
<li>这个方案只实现GPU的共享，不实现算力和显存的隔离，如果想实现隔离，在阿里云可以搭配 <a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">cGPU</a> 一起使用</li>
</ul>
<h4 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h4><p>下图是整个设计的核心组件：</p>
<ul>
<li>GPU Share Scheduler Extender：基于k8s scheduler extender机制，作用于调度过程的<code>Filter</code>和<code>Bind</code>阶段，用于决定某个Node上的一个GPU设备是否可以提供足够的GPU Memory，并将GPU分配的结果记录到Pod Spec 的 Annotation中</li>
<li>GPU Share Device Plugin：基于k8s device plugin机制，根据GPU Share Scheduler Extender记录在Pod Spec的Annotation，实现GPU 设备的 Allocation。</li>
</ul>
<p><img alt="GPU Share Design" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share.jpg"></p>
<h3 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h3><h4 id="设备资源报告"><a href="#设备资源报告" class="headerlink" title="设备资源报告"></a>设备资源报告</h4><p><code>GPU Share Device Plugin</code> 基于 <code>nvml</code> 库来查询每个Node上GPU设备的数目和每个GPU设备的GPU Memory。</p>
<p>这些资源状况被通过 <code>ListAndWatch()</code> 汇报给 Kubelet，然后 kubelet 会上报给 APIServer，这时候执行 <code>kubectl get node</code> 可以看到在 <code>status</code> 看到相关的<code>Extended Resource</code>字段：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Node</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.4</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">gpushare:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podCIDR:</span> <span class="number">172.16</span><span class="number">.1</span><span class="number">.0</span><span class="string">/26</span></span><br><span class="line">  <span class="attr">podCIDRs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.1</span><span class="number">.0</span><span class="string">/26</span></span><br><span class="line">  <span class="attr">providerID:</span> <span class="string">qcloud:///800002/ins-hsmsc4x9</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">allocatable:</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-count:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-mem:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">5926m</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">"47438316671"</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">54222084Ki</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-count:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-mem:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"6"</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">51473868Ki</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">57448708Ki</span></span><br><span class="line">    <span class="string">...</span></span><br></pre></td></tr></table></figure>
<h4 id="调度插件扩展"><a href="#调度插件扩展" class="headerlink" title="调度插件扩展"></a>调度插件扩展</h4><p>用户申请GPU的时候，在 Extended Resource 中只填写 <code>gpu-mem</code>，下面部署一个单机版的Tensorflow：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">tensorflow/tensorflow:2.2.1-gpu-py3-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">4</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">            <span class="attr">aliyun.com/gpu-mem:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">jupyter-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8888</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br></pre></td></tr></table></figure>
<h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><p>当kube-scheduler运行完所有的Filter函数后，就会调用 <code>GPU Share Extender</code> 的 Filter 函数。在原生的过滤中，kube-scheduler会计算是否有足够的Extended Resource（算的是总共的GPU Memory），但是不能知道是否某个GPU设备有足够的资源，这时候就需要调度器插件来实现。以下图为例：</p>
<ul>
<li>用户申请了8138MiB的GPU Memory，对于原生调度器，N1节点只剩下  (16276 * 2 - 16276 - 12207 = 4069) 的GPU资源，不满足 Extended Resource可用的条件，N1节点被过滤掉</li>
<li>接下来的N2节点和N3节点剩余的总的资源数都有8138MiB，那么该选择哪一个呢</li>
<li>在 <code>GPU Share Extender</code> 的过滤中，他需要找到有单个GPU能够满足用户申请的资源，当检查到N2节点的时候，发现虽然总的GPU Memory有8138MiB，但是每个GPU设备都只剩4096MiB了，不能满足单设备8138的需求，所以N2被过滤掉</li>
<li>扫描到N3节点，发现GPU0满足8138MiB的需求，符合要求</li>
</ul>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-filter.jpg"></p>
<blockquote>
<p><strong>这里有一个问题：当一个Node上有多张卡的时候，Scheduler Extender是如何知道每张卡当前可用的Capacity的呢？</strong></p>
</blockquote>
<p>我们看一下Extender在 Filter 阶段执行的函数，对于要创建的Pod，当前Node检查自己拥有的所有可用GPU，一旦有一个GPU的可用显存大于申请的显存，那么当前Node是可以被调度的。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// check if the pod can be allocated on the node</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">Assume</span><span class="params">(pod *v1.Pod)</span> <span class="params">(allocatable <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">	allocatable = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">	n.rwmu.RLock()</span><br><span class="line">	<span class="keyword">defer</span> n.rwmu.RUnlock()</span><br><span class="line"></span><br><span class="line">	availableGPUs := n.getAvailableGPUs()</span><br><span class="line">	reqGPU := <span class="keyword">uint</span>(utils.GetGPUMemoryFromPodResource(pod))</span><br><span class="line">	log.Printf(<span class="string">"debug: AvailableGPUs: %v in node %s"</span>, availableGPUs, n.name)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(availableGPUs) &gt; <span class="number">0</span> &#123;</span><br><span class="line">		<span class="keyword">for</span> devID := <span class="number">0</span>; devID &lt; <span class="built_in">len</span>(n.devs); devID++ &#123;</span><br><span class="line">			availableGPU, ok := availableGPUs[devID]</span><br><span class="line">			<span class="keyword">if</span> ok &#123;</span><br><span class="line">				<span class="keyword">if</span> availableGPU &gt;= reqGPU &#123;</span><br><span class="line">					allocatable = <span class="literal">true</span></span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> allocatable</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来的一个问题是，每个Node可用的GPU显存是如何得到的呢？我们进入到 <code>getAvailableGPUs</code> 继续看：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getAvailableGPUs</span><span class="params">()</span> <span class="params">(availableGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">	allGPUs := n.getAllGPUs()</span><br><span class="line">	usedGPUs := n.getUsedGPUs()</span><br><span class="line">	unhealthyGPUs := n.getUnhealthyGPUs()</span><br><span class="line">	availableGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> id, totalGPUMem := <span class="keyword">range</span> allGPUs &#123;</span><br><span class="line">		<span class="keyword">if</span> usedGPUMem, found := usedGPUs[id]; found &#123;</span><br><span class="line">			availableGPUs[id] = totalGPUMem - usedGPUMem</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	log.Printf(<span class="string">"info: available GPU list %v before removing unhealty GPUs"</span>, availableGPUs)</span><br><span class="line">	<span class="keyword">for</span> id, _ := <span class="keyword">range</span> unhealthyGPUs &#123;</span><br><span class="line">		log.Printf(<span class="string">"info: delete dev %d from availble GPU list"</span>, id)</span><br><span class="line">		<span class="built_in">delete</span>(availableGPUs, id)</span><br><span class="line">	&#125;</span><br><span class="line">	log.Printf(<span class="string">"info: available GPU list %v after removing unhealty GPUs"</span>, availableGPUs)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> availableGPUs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里可以看到，<code>Scheduler Extender</code> 内部维护了当前Node上所有的GPU显存状态和已经用了的GPU显存状态信息：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// device index: gpu memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getUsedGPUs</span><span class="params">()</span> <span class="params">(usedGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">	usedGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> n.devs &#123;</span><br><span class="line">		usedGPUs[dev.idx] = dev.GetUsedGPUMemory()</span><br><span class="line">	&#125;</span><br><span class="line">	log.Printf(<span class="string">"info: getUsedGPUs: %v in node %s, and devs %v"</span>, usedGPUs, n.name, n.devs)</span><br><span class="line">	<span class="keyword">return</span> usedGPUs</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// device index: gpu memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getAllGPUs</span><span class="params">()</span> <span class="params">(allGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">	allGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> n.devs &#123;</span><br><span class="line">		allGPUs[dev.idx] = dev.totalGPUMem</span><br><span class="line">	&#125;</span><br><span class="line">	log.Printf(<span class="string">"info: getAllGPUs: %v in node %s, and dev %v"</span>, allGPUs, n.name, n.devs)</span><br><span class="line">	<span class="keyword">return</span> allGPUs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于 <code>GetUsedGPUMemory</code>，是<code>Scheduler Extender</code> 内部维护的 <code>DeviceInfo</code> 所记录的，这里的 <code>d.podMap</code> 会在每次Extender执行 <code>Bind</code> 的时候，将对应的Pod添加到对应的Node上的 <code>DeviceInfo</code>中：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(d *DeviceInfo)</span> <span class="title">GetUsedGPUMemory</span><span class="params">()</span> <span class="params">(gpuMem <span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">	log.Printf(<span class="string">"debug: GetUsedGPUMemory() podMap %v, and its address is %p"</span>, d.podMap, d)</span><br><span class="line">	d.rwmu.RLock()</span><br><span class="line">	<span class="keyword">defer</span> d.rwmu.RUnlock()</span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> d.podMap &#123;</span><br><span class="line">		<span class="keyword">if</span> pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed &#123;</span><br><span class="line">			log.Printf(<span class="string">"debug: skip the pod %s in ns %s due to its status is %s"</span>, pod.Name, pod.Namespace, pod.Status.Phase)</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// gpuMem += utils.GetGPUMemoryFromPodEnv(pod)</span></span><br><span class="line">		gpuMem += utils.GetGPUMemoryFromPodAnnotation(pod)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> gpuMem</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再总结总结，本质上是 <code>Scheduler Extender</code> 维护了一个 <code>devs</code> 这么一个数据结构，使得它可以知道当前Node上每个GPU设备的显存状态。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// NodeInfo is node level aggregated information.</span></span><br><span class="line"><span class="keyword">type</span> NodeInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">	name           <span class="keyword">string</span></span><br><span class="line">	node           *v1.Node</span><br><span class="line">	devs           <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo</span><br><span class="line">	gpuCount       <span class="keyword">int</span></span><br><span class="line">	gpuTotalMemory <span class="keyword">int</span></span><br><span class="line">	rwmu           *sync.RWMutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么问题来了，我们通过ApiServer，只能知道对应Node上的 <code>gpuCount</code> 和 <code>gpuTotalMemory</code>，而不知道每张卡各自的显存的。这个 <code>devs</code> 是怎么初始化得到每张卡的显存信息呢的呢？继续看代码：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create Node Level</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewNodeInfo</span><span class="params">(node *v1.Node)</span> *<span class="title">NodeInfo</span></span> &#123;</span><br><span class="line">	log.Printf(<span class="string">"debug: NewNodeInfo() creates nodeInfo for %s"</span>, node.Name)</span><br><span class="line"></span><br><span class="line">	devMap := <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; utils.GetGPUCountInNode(node); i++ &#123;</span><br><span class="line">		devMap[i] = newDeviceInfo(i, <span class="keyword">uint</span>(utils.GetTotalGPUMemory(node)/utils.GetGPUCountInNode(node)))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(devMap) == <span class="number">0</span> &#123;</span><br><span class="line">		log.Printf(<span class="string">"warn: node %s with nodeinfo %v has no devices"</span>,</span><br><span class="line">			node.Name,</span><br><span class="line">			node)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;NodeInfo&#123;</span><br><span class="line">		name:           node.Name,</span><br><span class="line">		node:           node,</span><br><span class="line">		devs:           devMap,</span><br><span class="line">		gpuCount:       utils.GetGPUCountInNode(node),</span><br><span class="line">		gpuTotalMemory: utils.GetTotalGPUMemory(node),</span><br><span class="line">		rwmu:           <span class="built_in">new</span>(sync.RWMutex),</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，<strong>这里在初始化的时候，默认设定每张GPU卡的显存大小一样，通过平均得到每张卡的心存信息。</strong></p>
<h5 id="Bind"><a href="#Bind" class="headerlink" title="Bind"></a>Bind</h5><ul>
<li>当调度器发现有Node符合要求，这时候会把Pod和Node Bind到一起，<code>GPU Share Extender</code> 需要做两件事情：<ul>
<li>根据 <code>binpack</code> 原则找到Node上对应的GPU设备，并将 GPU Device ID记录到 Pod的 Annotation中 <code>ALIYUN_GPU_ID</code>。他也会将Pod使用的GPU Memory记录到Pod Annotation中：<code>ALIYUN_COM_GPU_MEM_POD</code> 和 <code>ALIYUN_COM_GPU_MEM_ASSUME_TIME</code></li>
<li>Bind the Node and Pod with kubernetes API</li>
</ul>
</li>
<li>如果没有找到合适的Node符合要求，那么就不会做Bind操作</li>
</ul>
<p>以下图为例，N1中有4个GPU，其中GPU0（12207），GPU1（8138）、GPU2（4069）和GPU3（16276）, GPU2因为资源不够被过滤掉，剩下的3个GPU根据 Binpack 原则，我们选用GPU1（图里面 Annotation错了，不是0，而是1）</p>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-bind.jpg"></p>
<p>我们看一看在找GPU设备的时候是如何操作的，可以看到这里通过 <code>candidateGPUMemory &gt; availableGPU</code> 这里实现了 <code>binpack</code>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// allocate the GPU ID to the pod</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">allocateGPUID</span><span class="params">(pod *v1.Pod)</span> <span class="params">(candidateDevID <span class="keyword">int</span>, found <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">	reqGPU := <span class="keyword">uint</span>(<span class="number">0</span>)</span><br><span class="line">	found = <span class="literal">false</span></span><br><span class="line">	candidateDevID = <span class="number">-1</span></span><br><span class="line">	candidateGPUMemory := <span class="keyword">uint</span>(<span class="number">0</span>)</span><br><span class="line">	availableGPUs := n.getAvailableGPUs()</span><br><span class="line"></span><br><span class="line">	reqGPU = <span class="keyword">uint</span>(utils.GetGPUMemoryFromPodResource(pod))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> reqGPU &gt; <span class="keyword">uint</span>(<span class="number">0</span>) &#123;</span><br><span class="line">		log.Printf(<span class="string">"info: reqGPU for pod %s in ns %s: %d"</span>, pod.Name, pod.Namespace, reqGPU)</span><br><span class="line">		log.Printf(<span class="string">"info: AvailableGPUs: %v in node %s"</span>, availableGPUs, n.name)</span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">len</span>(availableGPUs) &gt; <span class="number">0</span> &#123;</span><br><span class="line">			<span class="keyword">for</span> devID := <span class="number">0</span>; devID &lt; <span class="built_in">len</span>(n.devs); devID++ &#123;</span><br><span class="line">				availableGPU, ok := availableGPUs[devID]</span><br><span class="line">				<span class="keyword">if</span> ok &#123;</span><br><span class="line">					<span class="keyword">if</span> availableGPU &gt;= reqGPU &#123;</span><br><span class="line">						<span class="keyword">if</span> candidateDevID == <span class="number">-1</span> || candidateGPUMemory &gt; availableGPU &#123;</span><br><span class="line">							candidateDevID = devID</span><br><span class="line">							candidateGPUMemory = availableGPU</span><br><span class="line">						&#125;</span><br><span class="line"></span><br><span class="line">						found = <span class="literal">true</span></span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> found &#123;</span><br><span class="line">			log.Printf(<span class="string">"info: Find candidate dev id %d for pod %s in ns %s successfully."</span>,</span><br><span class="line">				candidateDevID,</span><br><span class="line">				pod.Name,</span><br><span class="line">				pod.Namespace)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			log.Printf(<span class="string">"warn: Failed to find available GPUs %d for the pod %s in the namespace %s"</span>,</span><br><span class="line">				reqGPU,</span><br><span class="line">				pod.Name,</span><br><span class="line">				pod.Namespace)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> candidateDevID, found</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Kubelet创建Pod"><a href="#Kubelet创建Pod" class="headerlink" title="Kubelet创建Pod"></a>Kubelet创建Pod</h4><p>接下来由Kubelet在创建container前调用 <code>GPU Share Device Plugin</code> 的 <code>Allocate</code> 函数，参数是申请的GPU Memory的数量。</p>
<p>Pod运行成功后，执行 <code>kubectl get pod</code> 可以看到：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_ASSIGNED:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_ASSUME_TIME:</span> <span class="string">"1606125285243248618"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_DEV:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_IDX:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_POD:</span> <span class="string">"3"</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Device Plugin 从 k8s apiserver 拿到所有Pending的Pod中属于GPU Share的Pod，并且按照 AssumedTimestamp排序</p>
</li>
<li><p>选择符合Allocation传入的GPU Memory的Pod，如果有多个，选择最早的那个Pod</p>
</li>
<li><p>标记 <code>ALIYUN_COM_GPU_MEM_ASSIGNED</code> 为 True</p>
</li>
<li><p>把 DeviceID 作为下NVIDIA_VISIBLE_DEVICES环境变量告诉 Nvidia Docker2，并且创建容器</p>
</li>
</ul>
<p><img alt data-src="https://github.com/AliyunContainerService/gpushare-scheduler-extender/raw/master/docs/designs/sequence.jpg"></p>
<blockquote>
<p><strong>这里问题是device plugin的allocate接口参数是什么，是否包含pod信息，是否包含pod annotation？</strong></p>
</blockquote>
<p>查看 Device Plugin 的代码，这一个申请的GPU Memory的数量让我很疑惑，为何要这么算？</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, req := <span class="keyword">range</span> reqs.ContainerRequests &#123;</span><br><span class="line">	podReqGPU += <span class="keyword">uint</span>(<span class="built_in">len</span>(req.DevicesIDs))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>继续看 <code>Device Plugin</code> 的 <code>DeviceIDs</code> 是如何生成的。这里调用了 <code>nvml library</code> 可以探测到本Node上拥有的GPU有多少个，每个显存是多少。接下来 <code>Device Plugin</code> 会创建一系列的 <code>FakeDeviceID</code>，并将这个DeviceIDs返回给 Kubelet，这就解释了为什么要通过上面的方法计算申请的 GPU Memory，这里的Memory以MiB为单位。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getDevices</span><span class="params">()</span> <span class="params">([]*pluginapi.Device, <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">	n, err := nvml.GetDeviceCount()</span><br><span class="line">	check(err)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> devs []*pluginapi.Device</span><br><span class="line">	realDevNames := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> i := <span class="keyword">uint</span>(<span class="number">0</span>); i &lt; n; i++ &#123;</span><br><span class="line">		d, err := nvml.NewDevice(i)</span><br><span class="line">		check(err)</span><br><span class="line">		<span class="comment">// realDevNames = append(realDevNames, d.UUID)</span></span><br><span class="line">		<span class="keyword">var</span> id <span class="keyword">uint</span></span><br><span class="line">		log.Infof(<span class="string">"Deivce %s's Path is %s"</span>, d.UUID, d.Path)</span><br><span class="line">		_, err = fmt.Sscanf(d.Path, <span class="string">"/dev/nvidia%d"</span>, &amp;id)</span><br><span class="line">		check(err)</span><br><span class="line">		realDevNames[d.UUID] = id</span><br><span class="line">		<span class="comment">// var KiB uint64 = 1024</span></span><br><span class="line">		log.Infof(<span class="string">"# device Memory: %d"</span>, <span class="keyword">uint</span>(*d.Memory))</span><br><span class="line">		<span class="keyword">if</span> getGPUMemory() == <span class="keyword">uint</span>(<span class="number">0</span>) &#123;</span><br><span class="line">			setGPUMemory(<span class="keyword">uint</span>(*d.Memory))</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span> j := <span class="keyword">uint</span>(<span class="number">0</span>); j &lt; getGPUMemory(); j++ &#123;</span><br><span class="line">			fakeID := generateFakeDeviceID(d.UUID, j)</span><br><span class="line">			<span class="keyword">if</span> j == <span class="number">0</span> &#123;</span><br><span class="line">				log.Infoln(<span class="string">"# Add first device ID: "</span> + fakeID)</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">if</span> j == getGPUMemory()<span class="number">-1</span> &#123;</span><br><span class="line">				log.Infoln(<span class="string">"# Add last device ID: "</span> + fakeID)</span><br><span class="line">			&#125;</span><br><span class="line">			devs = <span class="built_in">append</span>(devs, &amp;pluginapi.Device&#123;</span><br><span class="line">				ID:     fakeID,</span><br><span class="line">				Health: pluginapi.Healthy,</span><br><span class="line">			&#125;)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> devs, realDevNames</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们看一下 <code>Device Plugin</code> 是如何找到对应的Pod的，可以看到一旦碰到有Pod申请的GPU显存与Kubelet传入的显存大小一致，那么则找到对应的Pod了。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pods, err := getCandidatePods()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">   log.Infof(<span class="string">"invalid allocation requst: Failed to find candidate pods due to %v"</span>, err)</span><br><span class="line">   <span class="keyword">return</span> buildErrResponse(reqs, podReqGPU), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">   <span class="keyword">if</span> getGPUMemoryFromPodResource(pod) == podReqGPU &#123;</span><br><span class="line">      log.Infof(<span class="string">"Found Assumed GPU shared Pod %s in ns %s with GPU Memory %d"</span>,</span><br><span class="line">         pod.Name,</span><br><span class="line">         pod.Namespace,</span><br><span class="line">         podReqGPU)</span><br><span class="line">      assumePod = pod</span><br><span class="line">      found = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的 <code>getCandidatePods</code>就是List所有Pending的Pod中 Assume Memory的，并且按照时间排序：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pick up the gpushare pod with assigned status is false, and</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getCandidatePods</span><span class="params">()</span> <span class="params">([]*v1.Pod, error)</span></span> &#123;</span><br><span class="line">	candidatePods := []*v1.Pod&#123;&#125;</span><br><span class="line">	allPods, err := getPendingPodsInNode()</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> candidatePods, err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> _, pod := <span class="keyword">range</span> allPods &#123;</span><br><span class="line">		current := pod</span><br><span class="line">		<span class="keyword">if</span> isGPUMemoryAssumedPod(&amp;current) &#123;</span><br><span class="line">			candidatePods = <span class="built_in">append</span>(candidatePods, &amp;current)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">	<span class="keyword">return</span> makePodOrderdByAge(candidatePods), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>那么这里有一个问题：如果在同一个Node有两个Pod <poda, podb>，都申请了相同的GPU显存大小，比如3G，那么kubelet是在创建容器的时候，是如何保证两个Pod不混淆的呢？混淆会有问题吗，kubelet建Pod的时候到底是怎么搞的？是谁触发了kubelet创建容器？</poda,></strong></p>
</blockquote>
<hr>
<h2 id="腾讯GPUManager"><a href="#腾讯GPUManager" class="headerlink" title="腾讯GPUManager"></a>腾讯GPUManager</h2><p>GPU Manager 提供一个 All-in-One 的 GPU 管理器，基于 Kubernetes DevicePlugin 插件系统实现，该管理器提供了分配并共享 GPU、GPU 指标查询、容器运行前的 GPU 相关设备准备等功能，支持用户在 Kubernetes 集群中使用 GPU 设备。</p>
<ul>
<li><strong>拓扑分配</strong>：提供基于 GPU 拓扑分配功能，当用户分配超过1张 GPU 卡的应用，可以选择拓扑连接最快的方式分配 GPU 设备。</li>
<li><strong>GPU 共享</strong>：允许用户提交小于1张卡资源的任务，并提供 QoS 保证。</li>
<li><strong>应用 GPU 指标的查询</strong>：用户可以访问主机端口（默认为 5678）的 <code>/metrics</code> 路径，可以为 Prometheus 提供 GPU 指标的收集功能，访问 <code>/usage</code> 路径可以进行可读性的容器状况查询。</li>
</ul>
<h3 id="架构设计-1"><a href="#架构设计-1" class="headerlink" title="架构设计"></a>架构设计</h3><h4 id="设计原则-1"><a href="#设计原则-1" class="headerlink" title="设计原则"></a>设计原则</h4><ul>
<li><p>设计里定义了两种 <code>Extended Resource</code>：</p>
<ul>
<li><code>tencent.com/vcuda-core</code> ： <code>vcuda-core</code>对应的是使用率，单张卡有100个core</li>
<li><code>tencent.com/vcuda-memory</code> ：<code>vcuda-memory</code> 是显存，每个单位是256MB的显存</li>
<li>如果申请的资源为50%利用率，7680MB显存，<code>tencent.com/vcuda-core</code> 填写50，<code>tencent.com/vcuda-memory</code> 填写成30</li>
<li>同样支持原来的独占卡的方式，只需要在core的地方填写100的整数倍，memory值填写大于0的任意值</li>
</ul>
</li>
<li>基于k8s原生的Scheduler Extender、Extended Resource、DevicePlugin机制来实现</li>
<li>这个方案同时实现GPU的共享与算力和显存的隔离，类似于阿里云 <a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">cGPU</a> 加上GPUShare 一起使用</li>
</ul>
<h4 id="核心组件-1"><a href="#核心组件-1" class="headerlink" title="核心组件"></a>核心组件</h4><p>GaiaGPU的实现主要分为两个部分：Kubernetes 部分 和 vCUDA 部分</p>
<ul>
<li>Kubernetes部分基于 Kubernetes 的 Extended Resources、Device Plugin 和 Scheduler Extender机制，实现了下面两个项目<ul>
<li><a href="https://github.com/tkestack/gpu-manager" target="_blank" rel="external nofollow noopener noreferrer">GPU Manager </a>：实现为一个 Device Plugin，与 NVIDIA 的 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="external nofollow noopener noreferrer">k8s-device-plugin</a> 相比，不需要额外配置 <code>nvidia-docker2</code>，使用的是原生的 <code>runc</code></li>
<li><a href="https://github.com/tkestack/gpu-admission" target="_blank" rel="external nofollow noopener noreferrer">GPU Admission</a>：实现为一个Scheduler Extender，注意这里的Extender在论文中没有提到，下图中的GPU Scheduler实现的是topology的选卡，属于现在GPU Manager项目的一部分，与这里的调度器插件无关</li>
</ul>
</li>
<li>vCUDA 部分通过 <a href="https://github.com/tkestack/vcuda-controller" target="_blank" rel="external nofollow noopener noreferrer">vcuda-controller</a> 来实现，作为 NVIDIA 的 CUDA 库的封装</li>
</ul>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-gpu-manager.png"></p>
<h3 id="具体过程-1"><a href="#具体过程-1" class="headerlink" title="具体过程"></a>具体过程</h3><h4 id="设备资源上报"><a href="#设备资源上报" class="headerlink" title="设备资源上报"></a>设备资源上报</h4><ul>
<li>与阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 一样，GPU Manager 在 <code>ListAndWatch</code> 返回给Kubelet的也不是实际的GPU设备，而是 <code>a list of vGPUs</code>，</li>
<li>GPU被虚拟化为两个资源维度，memory 和 computing resource<ul>
<li>memory：以256M内存作为单位，每个memory unit叫做 <code>vmemory</code> device</li>
<li>computing resource：将一个物理GPU划分为100个 <code>vprocessor</code> devices，每个 <code>vprocessor</code> 占有 1%的GPU利用率</li>
</ul>
</li>
<li>用户申请具有GPU的Pod资源Manifest如下：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">vcuda</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">vcuda-test</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">['/usr/local/nvidia/bin/nvidia-smi']</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="number">50</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="number">50</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png"></p>
<p> 下面看具体代码，首先是向 <code>kubelet</code> 注册：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *managerImpl)</span> <span class="title">RegisterToKubelet</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	socketFile := filepath.Join(m.config.DevicePluginPath, types.KubeletSocket)</span><br><span class="line">	dialOptions := []grpc.DialOption&#123;grpc.WithInsecure(), grpc.WithDialer(utils.UnixDial), grpc.WithBlock(), grpc.WithTimeout(time.Second * <span class="number">5</span>)&#125;</span><br><span class="line"></span><br><span class="line">	conn, err := grpc.Dial(socketFile, dialOptions...)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> err</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line">	client := pluginapi.NewRegistrationClient(conn)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, srv := <span class="keyword">range</span> m.bundleServer &#123;</span><br><span class="line">		req := &amp;pluginapi.RegisterRequest&#123;</span><br><span class="line">			Version:      pluginapi.Version,</span><br><span class="line">			Endpoint:     path.Base(srv.SocketName()),</span><br><span class="line">			ResourceName: srv.ResourceName(),</span><br><span class="line">			Options:      &amp;pluginapi.DevicePluginOptions&#123;PreStartRequired: <span class="literal">true</span>&#125;,</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		glog.V(<span class="number">2</span>).Infof(<span class="string">"Register to kubelet with endpoint %s"</span>, req.Endpoint)</span><br><span class="line">		_, err = client.Register(context.Background(), req)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有一个 <code>m.bundleServer</code>，分别是 <code>vcore</code> 和 <code>vmemory</code> 的 gRPC Server。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *managerImpl)</span> <span class="title">setupGRPCService</span><span class="params">()</span></span> &#123;</span><br><span class="line">	vcoreServer := newVcoreServer(m)</span><br><span class="line">	vmemoryServer := newVmemoryServer(m)</span><br><span class="line"></span><br><span class="line">	m.bundleServer[types.VCoreAnnotation] = vcoreServer</span><br><span class="line">	m.bundleServer[types.VMemoryAnnotation] = vmemoryServer</span><br><span class="line"></span><br><span class="line">	displayapi.RegisterGPUDisplayServer(m.srv, m)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来看 <code>ListAndWatch</code> 的实现，对于两种资源，它会去检查 <code>capacity()</code>里面包含对应 <code>resourceName</code> 的：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ListAndWatchWithResourceName send devices for request resource back to server</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">ListAndWatchWithResourceName</span><span class="params">(resourceName <span class="keyword">string</span>, e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	devs := <span class="built_in">make</span>([]*pluginapi.Device, <span class="number">0</span>)</span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> ta.capacity() &#123;</span><br><span class="line">		<span class="keyword">if</span> strings.HasPrefix(dev.ID, resourceName) &#123;</span><br><span class="line">			devs = <span class="built_in">append</span>(devs, dev)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: devs&#125;)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// We don't send unhealthy state</span></span><br><span class="line">	<span class="keyword">for</span> &#123;</span><br><span class="line">		time.Sleep(time.Second)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	glog.V(<span class="number">2</span>).Infof(<span class="string">"ListAndWatch %s exit"</span>, resourceName)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么这里的 <code>ta.capicity()</code> 是如何得到的呢？这里维护了一个拓扑树，树根是物理的Host，树叶是物理的GPU。这里根据树叶上GPU的数目和总的显存大小，构建了 <code>vcore</code> 设备 和 <code>vmemory</code> 设备，命名以各自的资源名为前缀。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">capacity</span><span class="params">()</span> <span class="params">(devs []*pluginapi.Device)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		gpuDevices, memoryDevices []*pluginapi.Device</span><br><span class="line">		totalMemory               <span class="keyword">int64</span></span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	nodes := ta.tree.Leaves()</span><br><span class="line">	<span class="keyword">for</span> i := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">		totalMemory += <span class="keyword">int64</span>(nodes[i].Meta.TotalMemory)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	totalCores := <span class="built_in">len</span>(nodes) * nvtree.HundredCore</span><br><span class="line">	gpuDevices = <span class="built_in">make</span>([]*pluginapi.Device, totalCores)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; totalCores; i++ &#123;</span><br><span class="line">		gpuDevices[i] = &amp;pluginapi.Device&#123;</span><br><span class="line">			ID:     fmt.Sprintf(<span class="string">"%s-%d"</span>, types.VCoreAnnotation, i),</span><br><span class="line">			Health: pluginapi.Healthy,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	totalMemoryBlocks := totalMemory / types.MemoryBlockSize</span><br><span class="line">	memoryDevices = <span class="built_in">make</span>([]*pluginapi.Device, totalMemoryBlocks)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="keyword">int64</span>(<span class="number">0</span>); i &lt; totalMemoryBlocks; i++ &#123;</span><br><span class="line">		memoryDevices[i] = &amp;pluginapi.Device&#123;</span><br><span class="line">			ID:     fmt.Sprintf(<span class="string">"%s-%d-%d"</span>, types.VMemoryAnnotation, types.MemoryBlockSize, i),</span><br><span class="line">			Health: pluginapi.Healthy,</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	devs = <span class="built_in">append</span>(devs, gpuDevices...)</span><br><span class="line">	devs = <span class="built_in">append</span>(devs, memoryDevices...)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="调度插件扩展-1"><a href="#调度插件扩展-1" class="headerlink" title="调度插件扩展"></a>调度插件扩展</h4><h5 id="细粒度Quota准入"><a href="#细粒度Quota准入" class="headerlink" title="细粒度Quota准入"></a>细粒度Quota准入</h5><p><code>GPU Quota Admission</code> 作为调度器插件，实现了更细粒度的quota调度准入维度。用户通过配置一个 <code>ConfigMap</code>，对每个 <code>Namespace</code>可用的GPU卡的配额做规划，同时也定义了资源池，这样在调度的时候就可以实现按照资源池及GPU型号进行策略调度。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"A"</span>: &#123;</span><br><span class="line">    <span class="attr">"pool"</span>: [<span class="string">"public"</span>], <span class="comment">// Pods in namespace 'A' could use pool 'public'</span></span><br><span class="line">    <span class="attr">"quota"</span>: &#123;</span><br><span class="line">      <span class="attr">"M40"</span>: <span class="number">2</span>,</span><br><span class="line">      <span class="attr">"P100"</span>: <span class="number">3</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"B"</span>: &#123;</span><br><span class="line">    <span class="attr">"pool"</span>: [ <span class="string">"wx"</span> ], <span class="comment">// Pods in namespace 'B' could use pool 'wx'</span></span><br><span class="line">    <span class="attr">"quota"</span>: &#123;</span><br><span class="line">      <span class="attr">"M40"</span>: <span class="number">8</span>,</span><br><span class="line">      <span class="attr">"P100"</span>: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>具体在调度的时候，对每一个Pod，根据Namespace可以筛选出一系列含有GPU的Pods，然后当前Namespace下，对于某种GPU Model（比如P100），计算已经使用了的GPU大小，根据 <code>ConfigMap</code> 定义的配额，找到没超出。通过这个，得到所有没超出Quota的Models。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> NamespaceQuota <span class="keyword">struct</span> &#123;</span><br><span class="line">	Quota <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> <span class="string">`json:"quota"`</span></span><br><span class="line">	Pool []<span class="keyword">string</span> <span class="string">`json:"pool"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gpuFilter *GPUFilter)</span> <span class="title">filterGPUModel</span><span class="params">(pod *corev1.Pod, namespaceQuota NamespaceQuota)</span> <span class="params">([]<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> filteredGPUModels []<span class="keyword">string</span></span><br><span class="line">	<span class="keyword">for</span> gpuModel, limit := <span class="keyword">range</span> namespaceQuota.Quota &#123;</span><br><span class="line">		limit = limit * VirtualGPUTimes</span><br><span class="line">	  nodeSelector, err := metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">			MatchLabels: <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>&#123;gpuFilter.conf.GPUModelLabel: gpuModel&#125;&#125;)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">		pods, err := gpuFilter.listPodsOnNodes(nodeSelector, pod.Namespace)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">		gpuUsed := calculateGPUUsage(<span class="built_in">append</span>(pods, pod))</span><br><span class="line">		<span class="keyword">if</span> gpuUsed &lt;= limit &#123;</span><br><span class="line">			filteredGPUModels = <span class="built_in">append</span>(filteredGPUModels, gpuModel)</span><br><span class="line">		&#125;</span><br><span class="line">		glog.V(<span class="number">4</span>).Infof(<span class="string">"Pods in namespace %s will use %d %s GPU cards after adding this pod, quota is %d"</span>,</span><br><span class="line">			pod.Namespace, gpuUsed, gpuModel, limit)</span><br><span class="line">	&#125;</span><br><span class="line">	glog.V(<span class="number">4</span>).Infof(<span class="string">"These GPU models could be used by pod %s: %+v"</span>, pod.Name, filteredGPUModels)</span><br><span class="line">	<span class="keyword">return</span> filteredGPUModels, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来在 Filter阶段，根据上面的可用 <code>GPU Models</code> 和定义的 <code>Quota Pool</code>，</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gpuFilter *GPUFilter)</span> <span class="title">filterNodes</span><span class="params">(nodes []corev1.Node, gpuModels, pools []<span class="keyword">string</span>)</span> <span class="params">(filteredNodes []corev1.Node, failedNodesMap schedulerapi.FailedNodesMap, err error)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> gpuModelSelector, poolSelector labels.Selector</span><br><span class="line"></span><br><span class="line">	glog.V(<span class="number">4</span>).Infof(<span class="string">"Filter nodes with gpuModels(%+v) and pools(%+v)"</span>, gpuModels, pools)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(gpuModels) != <span class="number">0</span> &#123;</span><br><span class="line">		gpuModelSelector, err = metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">			MatchExpressions: []metav1.LabelSelectorRequirement&#123;&#123;</span><br><span class="line">				Key:      gpuFilter.conf.GPUModelLabel,</span><br><span class="line">				Operator: metav1.LabelSelectorOpIn,</span><br><span class="line">				Values:   gpuModels,</span><br><span class="line">			&#125;&#125;&#125;)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		gpuModelSelector = labels.Nothing()</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// If pool is empty, it means that pod could use every pool, it is OK to leave it as a empty selector.</span></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(pools) != <span class="number">0</span> &#123;</span><br><span class="line">		poolSelector, err = metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">			MatchExpressions: []metav1.LabelSelectorRequirement&#123;&#123;</span><br><span class="line">				Key:      gpuFilter.conf.GPUPoolLabel,</span><br><span class="line">				Operator: metav1.LabelSelectorOpIn,</span><br><span class="line">				Values:   pools,</span><br><span class="line">			&#125;&#125;&#125;)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		poolSelector = labels.Everything()</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	failedNodesMap = schedulerapi.FailedNodesMap&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, node := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">		<span class="keyword">if</span> gpuModelSelector.Matches(labels.Set(node.Labels)) &amp;&amp; poolSelector.Matches(labels.Set(node.Labels)) &#123;</span><br><span class="line">			filteredNodes = <span class="built_in">append</span>(filteredNodes, node)</span><br><span class="line">			glog.V(<span class="number">5</span>).Infof(<span class="string">"Add %s to filteredNodes"</span>, node.Name)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			failedNodesMap[node.Name] = <span class="string">"ExceedsGPUQuota"</span></span><br><span class="line">			glog.V(<span class="number">5</span>).Infof(<span class="string">"Add %s to failedNodesMap"</span>, node.Name)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> filteredNodes, failedNodesMap, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这一步，也就是实现了细粒度的Quota调度准入控制。</p>
<h5 id="避免GPU碎片化"><a href="#避免GPU碎片化" class="headerlink" title="避免GPU碎片化"></a>避免GPU碎片化</h5><p>为此我们增加了GPU predicate controller来尽可能的降低系统默认调度策略带来的碎片化问题。</p>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-19_gpu-manager-predicate.png"></p>
<p>我们看看它是如何实现的，首先在 <code>deviceFilter</code>的入口里面，拿到当前Node上存在的所有Pod：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pods, err := gpuFilter.ListPodsOnNode(node)</span><br><span class="line">...</span><br><span class="line">nodeInfo := device.NewNodeInfo(node, pods)</span><br><span class="line">alloc := algorithm.NewAllocator(nodeInfo)</span><br><span class="line">newPod, err := alloc.Allocate(pod)</span><br></pre></td></tr></table></figure>
<p>接下来构建一个 <code>NodeInfo</code> 结构体，里面包含有当前Node的所有信息，这里记录了Node上所有的GPU显存和GPU设备数目。这个是通过Node Status里面两个扩展资源计算出来的。<strong>GPU Manager 方案也是认为每台机器上的GPU的不同卡的显存大小是相同的，这样可以算出每张卡的显存大小</strong>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> NodeInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">	name        <span class="keyword">string</span></span><br><span class="line">	node        *v1.Node</span><br><span class="line">	devs        <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo</span><br><span class="line">	deviceCount <span class="keyword">int</span></span><br><span class="line">	totalMemory <span class="keyword">uint</span></span><br><span class="line">	usedCore    <span class="keyword">uint</span></span><br><span class="line">	usedMemory  <span class="keyword">uint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>NodeInfo</code> 里面还有一个 <code>DeviceInfo</code> 的map，用于记录每张卡的使用情况。这里在初始化这个 <code>NodeInfo</code> 数据结构的时候也会根据传入的 <code>pods</code> 信息更新 <code>DeviceInfo</code> 的设备使用情况。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> DeviceInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">	id          <span class="keyword">int</span></span><br><span class="line">	totalMemory <span class="keyword">uint</span></span><br><span class="line">	usedMemory  <span class="keyword">uint</span></span><br><span class="line">	usedCore    <span class="keyword">uint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来就是每个 <code>Allocate</code> 函数的实现，对于Pod里面的每一个容器，都会分配得到一个 <code>devIDs</code> 列表，然后得到对Pod打上Annotation：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *allocator)</span> <span class="title">Allocate</span><span class="params">(pod *v1.Pod)</span> <span class="params">(*v1.Pod, error)</span></span> &#123;</span><br><span class="line">	newPod := pod.DeepCopy()</span><br><span class="line">	<span class="keyword">for</span> i, c := <span class="keyword">range</span> newPod.Spec.Containers &#123;</span><br><span class="line">		<span class="keyword">if</span> !util.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line">		devIDs := []<span class="keyword">string</span>&#123;&#125;</span><br><span class="line">		devs, err := alloc.AllocateOne(&amp;c)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			glog.Infof(<span class="string">"failed to allocate for pod %s(%s)"</span>, newPod.Name, c.Name)</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span> _, dev := <span class="keyword">range</span> devs &#123;</span><br><span class="line">			devIDs = <span class="built_in">append</span>(devIDs, strconv.Itoa(dev.GetID()))</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> newPod.Annotations == <span class="literal">nil</span> &#123;</span><br><span class="line">			newPod.Annotations = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>)</span><br><span class="line">		&#125;</span><br><span class="line">		newPod.Annotations[util.PredicateGPUIndexPrefix+strconv.Itoa(i)] = strings.Join(devIDs, <span class="string">","</span>)</span><br><span class="line">	&#125;</span><br><span class="line">	newPod.Annotations[util.GPUAssigned] = <span class="string">"false"</span></span><br><span class="line">	newPod.Annotations[util.PredicateTimeAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, time.Now().UnixNano())</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> newPod, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来的问题就是，这里的 <code>AllocateOne</code> 是如何实现的呢？对于每个容器，根据其申请的GPU资源，可以分为GPU是共享模式还是独占模式，然后调用 <code>Evaluate</code>去得到 <code>devs</code>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *allocator)</span> <span class="title">AllocateOne</span><span class="params">(container *v1.Container)</span> <span class="params">([]*device.DeviceInfo, error)</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		devs           []*device.DeviceInfo</span><br><span class="line">		sharedMode     <span class="keyword">bool</span></span><br><span class="line">		vcore, vmemory <span class="keyword">uint</span></span><br><span class="line">	)</span><br><span class="line">	node := alloc.nodeInfo.GetNode()</span><br><span class="line">	nodeTotalMemory := util.GetCapacityOfNode(node, util.VMemoryAnnotation)</span><br><span class="line">	deviceCount := util.GetGPUDeviceCountOfNode(node)</span><br><span class="line">	deviceTotalMemory := <span class="keyword">uint</span>(nodeTotalMemory / deviceCount)</span><br><span class="line">	needCores := util.GetGPUResourceOfContainer(container, util.VCoreAnnotation)</span><br><span class="line">	needMemory := util.GetGPUResourceOfContainer(container, util.VMemoryAnnotation)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">switch</span> &#123;</span><br><span class="line">	<span class="keyword">case</span> needCores &lt; util.HundredCore:</span><br><span class="line">		eval := NewShareMode(alloc.nodeInfo)</span><br><span class="line">		devs = eval.Evaluate(needCores, needMemory)</span><br><span class="line">		sharedMode = <span class="literal">true</span></span><br><span class="line">	<span class="keyword">default</span>:</span><br><span class="line">		eval := NewExclusiveMode(alloc.nodeInfo)</span><br><span class="line">		devs = eval.Evaluate(needCores, needMemory)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(devs) == <span class="number">0</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"failed to allocate for container %s"</span>, container.Name)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> sharedMode &#123;</span><br><span class="line">		vcore = needCores</span><br><span class="line">		vmemory = needMemory</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		vcore = util.HundredCore</span><br><span class="line">		vmemory = deviceTotalMemory</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> devs &#123;</span><br><span class="line">		err := alloc.nodeInfo.AddUsedResources(dev.GetID(), vcore, vmemory)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			glog.Infof(<span class="string">"failed to update used resource for node %s dev %d due to %v"</span>, node.Name, dev.GetID(), err)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> devs, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以共享模式为例，这里拿到当前Node的所有 <code>Device</code>，分别根据最少可用的<code>cores</code>和可用的<code>memory</code>来排序，如果有满足用户需要的设备，则加入到 <code>devs</code> 里面，最后将这个 <code>list</code> 返回给用户。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(al *shareMode)</span> <span class="title">Evaluate</span><span class="params">(cores <span class="keyword">uint</span>, memory <span class="keyword">uint</span>)</span> []*<span class="title">device</span>.<span class="title">DeviceInfo</span></span> &#123;</span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		devs        []*device.DeviceInfo</span><br><span class="line">		deviceCount = al.node.GetDeviceCount()</span><br><span class="line">		tmpStore    = <span class="built_in">make</span>([]*device.DeviceInfo, deviceCount)</span><br><span class="line">		sorter      = shareModeSort(device.ByAllocatableCores, device.ByAllocatableMemory, device.ByID)</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; deviceCount; i++ &#123;</span><br><span class="line">		tmpStore[i] = al.node.GetDeviceMap()[i]</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	sorter.Sort(tmpStore)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> tmpStore &#123;</span><br><span class="line">		<span class="keyword">if</span> dev.AllocatableCores() &gt;= cores &amp;&amp; dev.AllocatableMemory() &gt;= memory &#123;</span><br><span class="line">			glog.V(<span class="number">4</span>).Infof(<span class="string">"Pick up %d , cores: %d, memory: %d"</span>, dev.GetID(), dev.AllocatableCores(), dev.AllocatableMemory())</span><br><span class="line">			devs = <span class="built_in">append</span>(devs, dev)</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> devs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到这里在调度过程中，选择最先满足的那个，一旦满足则跳出选择。这是因为这里的 <code>devs</code> 已经按照最少可用的资源来匹配了，通过这种方式可以减少碎片化。</p>
<h4 id="Kubelet创建Pod-1"><a href="#Kubelet创建Pod-1" class="headerlink" title="Kubelet创建Pod"></a>Kubelet创建Pod</h4><p>用户创建Pod之后，经过调度找到对应的Node，这时候Kubelet向DevicePlugin执行Allocate函数。因为Kubelet看到的是虚拟的Devices，这里需要有一个从虚拟Device到实际GPU Device的映射，这里就是上图中GPU Manager做的事情，然后发送一个Request给GPU Scheduler，根据拓扑关系选择最合适的GPU，然后GPU Manager将 AllocateResponse返回给Kubelet。</p>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png"></p>
<p>我们先看 <code>Allocate</code> 的实现，这段代码比较长，但是实现的逻辑也不难：</p>
<ul>
<li>Allocate传入的参数是 <code>deviceIDs</code> 这样里一个List，<strong>里面只有 <code>vcore</code> 这种设备</strong> （代码是这样的，需要进一步看一看 kubelet）</li>
<li>Pod可能有多个Container，这里每次只处理一个容器<ul>
<li>如果还有未处理的Pod，先解决未处理Pod中的容器</li>
<li>否则从当前Node上的Pod遍历，选择与用户申请的 <code>vcore</code> 相同的容器</li>
</ul>
</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">Allocate</span><span class="params">(_ context.Context, reqs *pluginapi.AllocateRequest)</span> <span class="params">(*pluginapi.AllocateResponse, error)</span></span> &#123;</span><br><span class="line">	ta.Lock()</span><br><span class="line">	<span class="keyword">defer</span> ta.Unlock()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> (</span><br><span class="line">		reqCount           <span class="keyword">uint</span></span><br><span class="line">		candidatePod       *v1.Pod</span><br><span class="line">		candidateContainer *v1.Container</span><br><span class="line">		found              <span class="keyword">bool</span></span><br><span class="line">	)</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(reqs.ContainerRequests) &lt; <span class="number">1</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"empty container request"</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// k8s send allocate request for one container at a time</span></span><br><span class="line">	req := reqs.ContainerRequests[<span class="number">0</span>]</span><br><span class="line">	resps := &amp;pluginapi.AllocateResponse&#123;&#125;</span><br><span class="line">	reqCount = <span class="keyword">uint</span>(<span class="built_in">len</span>(req.DevicesIDs))</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> ta.unfinishedPod != <span class="literal">nil</span> &#123;</span><br><span class="line">		candidatePod = ta.unfinishedPod</span><br><span class="line">		cache := ta.allocatedPod.GetCache(<span class="keyword">string</span>(candidatePod.UID))</span><br><span class="line">		<span class="keyword">for</span> i, c := <span class="keyword">range</span> candidatePod.Spec.Containers &#123;</span><br><span class="line">			<span class="keyword">if</span> _, ok := cache[c.Name]; ok &#123;</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> !utils.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> reqCount != utils.GetGPUResourceOfContainer(&amp;candidatePod.Spec.Containers[i], types.VCoreAnnotation) &#123;</span><br><span class="line">				<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">			&#125;</span><br><span class="line">			candidateContainer = &amp;candidatePod.Spec.Containers[i]</span><br><span class="line">			found = <span class="literal">true</span></span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		pods, err := getCandidatePods(ta.k8sClient, ta.config.Hostname)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			msg := fmt.Sprintf(<span class="string">"Failed to find candidate pods due to %v"</span>, err)</span><br><span class="line">			glog.Infof(msg)</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">			<span class="keyword">if</span> found &#123;</span><br><span class="line">				<span class="keyword">break</span></span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span> i, c := <span class="keyword">range</span> pod.Spec.Containers &#123;</span><br><span class="line">				<span class="keyword">if</span> !utils.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line">					<span class="keyword">continue</span></span><br><span class="line">				&#125;</span><br><span class="line">				podCache := ta.allocatedPod.GetCache(<span class="keyword">string</span>(pod.UID))</span><br><span class="line">				<span class="keyword">if</span> podCache != <span class="literal">nil</span> &#123;</span><br><span class="line">					<span class="keyword">if</span> _, ok := podCache[c.Name]; ok &#123;</span><br><span class="line">						glog.Infof(<span class="string">"container %s of pod %s has been allocate, continue to next"</span>, c.Name, pod.UID)</span><br><span class="line">						<span class="keyword">continue</span></span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span> utils.GetGPUResourceOfContainer(&amp;pod.Spec.Containers[i], types.VCoreAnnotation) == reqCount &#123;</span><br><span class="line">					glog.Infof(<span class="string">"Found candidate Pod %s(%s) with device count %d"</span>, pod.UID, c.Name, reqCount)</span><br><span class="line">					candidatePod = pod</span><br><span class="line">					candidateContainer = &amp;pod.Spec.Containers[i]</span><br><span class="line">					found = <span class="literal">true</span></span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>找到这样的一个容器之后，拿到容器申请的 <code>vmemory</code>，每一个虚拟的 <code>vmemory</code> 作为一个设备加入到 <code>req.DevicesIDs</code> 中，继续调用 <code>allocateOne</code>:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line">		<span class="comment">// get vmemory info from container spec</span></span><br><span class="line">		vmemory := utils.GetGPUResourceOfContainer(candidateContainer, types.VMemoryAnnotation)</span><br><span class="line">		<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="keyword">int</span>(vmemory); i++ &#123;</span><br><span class="line">			req.DevicesIDs = <span class="built_in">append</span>(req.DevicesIDs, types.VMemoryAnnotation)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		resp, err := ta.allocateOne(candidatePod, candidateContainer, req)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			glog.Errorf(err.Error())</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">		&#125;</span><br><span class="line">		resps.ContainerResponses = <span class="built_in">append</span>(resps.ContainerResponses, resp)</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		msg := fmt.Sprintf(<span class="string">"candidate pod not found for request %v, allocation failed"</span>, reqs)</span><br><span class="line">		glog.Infof(msg)</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> resps, ni</span><br></pre></td></tr></table></figure>
<p>具体的 <code>Allocate</code> 实现在 <code>allocateOne</code> 里面，根据Pod计算出其申请的 <code>needCores</code> 和 <code>needMemory</code> 之后，根据三种情况有不同的分配策略。注意这里还是在拓扑树上面操作，拓扑树树根是物理的Host，树叶是物理的GPU</p>
<ul>
<li>申请的资源超过一张卡，这时候分配的策略是尽可能减少卡之间的通信开销</li>
<li>申请的资源等于一张卡，这时候的分配策略是尽可能减少拓扑树里面产生没有兄弟节点的叶节点</li>
<li>申请的资源小于一张卡，这时候的分配策略是尽可能减少卡资源的碎片化</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> needCores &gt; nvtree.HundredCore:</span><br><span class="line">	eval, ok := ta.evaluators[<span class="string">"link"</span>]</span><br><span class="line">	<span class="comment">// 这种场景下needCores must be multiple of nvtree.HundredCore</span></span><br><span class="line">	nodes = eval.Evaluate(needCores, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">case</span> needCores == nvtree.HundredCore:</span><br><span class="line">	eval, ok := ta.evaluators[<span class="string">"fragment"</span>]</span><br><span class="line">	nodes = eval.Evaluate(needCores, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">	<span class="comment">// evaluate in share mode</span></span><br><span class="line">	shareMode = <span class="literal">true</span></span><br><span class="line">	eval, ok := ta.evaluators[<span class="string">"share"</span>]</span><br><span class="line">	nodes = eval.Evaluate(needCores, needMemory)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这里的 <code>Evaluate</code> 返回的是 <code>NvidiaNode</code> 这样的 GPU 节点，通过这个结构可以构建一个拓扑树：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//NvidiaNode represents a node of Nvidia GPU</span></span><br><span class="line"><span class="keyword">type</span> NvidiaNode <span class="keyword">struct</span> &#123;</span><br><span class="line">	Meta            DeviceMeta</span><br><span class="line">	AllocatableMeta SchedulerCache</span><br><span class="line"></span><br><span class="line">	Parent   *NvidiaNode</span><br><span class="line">	Children []*NvidiaNode</span><br><span class="line">	Mask     <span class="keyword">uint32</span></span><br><span class="line"></span><br><span class="line">	pendingReset <span class="keyword">bool</span></span><br><span class="line">	vchildren    <span class="keyword">map</span>[<span class="keyword">int</span>]*NvidiaNode</span><br><span class="line">	ntype        nvml.GpuTopologyLevel</span><br><span class="line">	tree         *NvidiaTree</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于这里具体的分配算法此处就不再详述了，抓住主脉络。</p>
<p>接下来构建 <code>pluginapi.ContainerAllocateResponse</code>，这里会分别设置环境变量，挂载的目录，找到的设备，以及<code>Annotation</code>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ctntResp := &amp;pluginapi.ContainerAllocateResponse&#123;</span><br><span class="line">		Envs:        <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>),</span><br><span class="line">		Mounts:      <span class="built_in">make</span>([]*pluginapi.Mount, <span class="number">0</span>),</span><br><span class="line">		Devices:     <span class="built_in">make</span>([]*pluginapi.DeviceSpec, <span class="number">0</span>),</span><br><span class="line">		Annotations: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>),</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>首先是 <code>Devices</code> 字段：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">allocatedDevices := sets.NewString()</span><br><span class="line">deviceList := <span class="built_in">make</span>([]<span class="keyword">string</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> _, n := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">	name := n.MinorName()</span><br><span class="line">	glog.V(<span class="number">2</span>).Infof(<span class="string">"Allocate %s for %s(%s), Meta (%d:%d)"</span>, name, pod.UID, container.Name, n.Meta.ID, n.Meta.MinorID)</span><br><span class="line"></span><br><span class="line">	ctntResp.Annotations[types.VCoreAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, needCores)</span><br><span class="line">	ctntResp.Annotations[types.VMemoryAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, needMemory)</span><br><span class="line"></span><br><span class="line">	ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">		ContainerPath: name,</span><br><span class="line">		HostPath:      name,</span><br><span class="line">		Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">	&#125;)</span><br><span class="line">	deviceList = <span class="built_in">append</span>(deviceList, n.Meta.UUID)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> !allocated &#123;</span><br><span class="line">		ta.tree.MarkOccupied(n, needCores, needMemory)</span><br><span class="line">	&#125;</span><br><span class="line">	allocatedDevices.Insert(name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里还有一些控制设备：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Append control device</span></span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">	ContainerPath: types.NvidiaCtlDevice,</span><br><span class="line">	HostPath:      types.NvidiaCtlDevice,</span><br><span class="line">	Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">	ContainerPath: types.NvidiaUVMDevice,</span><br><span class="line">	HostPath:      types.NvidiaUVMDevice,</span><br><span class="line">	Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Append default device</span></span><br><span class="line"><span class="keyword">if</span> cfg, found := ta.extraConfig[<span class="string">"default"</span>]; found &#123;</span><br><span class="line">	<span class="keyword">for</span> _, dev := <span class="keyword">range</span> cfg.Devices &#123;</span><br><span class="line">		ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">			ContainerPath: dev,</span><br><span class="line">			HostPath:      dev,</span><br><span class="line">			Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">		&#125;)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接着是 <code>Annotations</code> 字段：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ctntResp.Annotations[types.VDeviceAnnotation] = vDeviceAnnotationStr(nodes)</span><br><span class="line"><span class="keyword">if</span> !allocated &#123;</span><br><span class="line">	ta.allocatedPod.Insert(<span class="keyword">string</span>(pod.UID), container.Name, &amp;cache.Info&#123;</span><br><span class="line">		Devices: allocatedDevices.UnsortedList(),</span><br><span class="line">		Cores:   needCores,</span><br><span class="line">		Memory:  needMemory,</span><br><span class="line">	&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后是 <code>Envs</code> 字段</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LD_LIBRARY_PATH</span></span><br><span class="line">ctntResp.Envs[<span class="string">"LD_LIBRARY_PATH"</span>] = <span class="string">"/usr/local/nvidia/lib64"</span></span><br><span class="line"><span class="keyword">for</span> _, env := <span class="keyword">range</span> container.Env &#123;</span><br><span class="line">	<span class="keyword">if</span> env.Name == <span class="string">"compat32"</span> &amp;&amp; strings.ToLower(env.Value) == <span class="string">"true"</span> &#123;</span><br><span class="line">		ctntResp.Envs[<span class="string">"LD_LIBRARY_PATH"</span>] = <span class="string">"/usr/local/nvidia/lib"</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NVIDIA_VISIBLE_DEVICES</span></span><br><span class="line">ctntResp.Envs[<span class="string">"NVIDIA_VISIBLE_DEVICES"</span>] = strings.Join(deviceList, <span class="string">","</span>)</span><br></pre></td></tr></table></figure>
<p>最后是 <code>Mounts</code> 字段，这里给GPU容器配置一个volume挂载点来提供CUDA Library以及配置环境变量<code>LD_LIBRARY_PATH</code> 告诉应用哪里去找到 <code>CUDA Library</code>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> shareMode &#123;</span><br><span class="line">	ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">		ContainerPath: <span class="string">"/usr/local/nvidia"</span>,</span><br><span class="line">		HostPath:      types.DriverLibraryPath,</span><br><span class="line">		ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">	&#125;)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">	ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">		ContainerPath: <span class="string">"/usr/local/nvidia"</span>,</span><br><span class="line">		HostPath:      types.DriverOriginLibraryPath,</span><br><span class="line">		ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">	&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">	ContainerPath: types.VCUDA_MOUNTPOINT,</span><br><span class="line">	HostPath:      filepath.Join(ta.config.VirtualManagerPath, <span class="keyword">string</span>(pod.UID)),</span><br><span class="line">	ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="vGPU-Manager"><a href="#vGPU-Manager" class="headerlink" title="vGPU Manager"></a>vGPU Manager</h4><p><code>vGPU Manager</code> 作为 <code>GPU Manager</code> 这个 <code>DaemonSet</code> 的一部分，负责下发容器配置和监控容器分配的vGPU。上一步在拓扑分配器确定好每个容器的资源配置之后，<code>vGPU Manager</code> 负责为每个容器在 host 上创建一个独立的目录，这个目录以容器的名称命名，并且会被包括在 <code>AllocateResponse</code> 中返回给 kubelet，对就是上面那段代码做的事情。</p>
<p><code>vGPU Manager</code> 会维护一个使用了GPU的并且仍然活着的容器列表，还会去周期性的检查他们。一旦有容器挂掉，就会将这个容器移出列表并且删去目录。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//                Host                     |                Container</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//  .-----------.                          |</span></span><br><span class="line"><span class="comment">//  | allocator |----------.               |             ___________</span></span><br><span class="line"><span class="comment">//  '-----------'   PodUID |               |             \          \</span></span><br><span class="line"><span class="comment">//                         v               |              ) User App )--------.</span></span><br><span class="line"><span class="comment">//                .-----------------.      |             /__________/         |</span></span><br><span class="line"><span class="comment">//     .----------| virtual-manager |      |                                  |</span></span><br><span class="line"><span class="comment">//     |          '-----------------'      |                                  |</span></span><br><span class="line"><span class="comment">// $VirtualManagerPath/PodUID              |                                  |</span></span><br><span class="line"><span class="comment">//     |                                   |       read /proc/self/cgroup     |</span></span><br><span class="line"><span class="comment">//     |  .------------------.             |       to get PodUID, ContainerID |</span></span><br><span class="line"><span class="comment">//     '-&gt;| create directory |------.      |                                  |</span></span><br><span class="line"><span class="comment">//        '------------------'      |      |                                  |</span></span><br><span class="line"><span class="comment">//                                  |      |                                  |</span></span><br><span class="line"><span class="comment">//                 .----------------'      |       .----------------------.   |</span></span><br><span class="line"><span class="comment">//                 |                       |       | fork call gpu-client |&lt;--'</span></span><br><span class="line"><span class="comment">//                 |                       |       '----------------------'</span></span><br><span class="line"><span class="comment">//                 v                       |                   |</span></span><br><span class="line"><span class="comment">//    .------------------------.           |                   |</span></span><br><span class="line"><span class="comment">//   ( wait for client register )&lt;-------PodUID, ContainerID---'</span></span><br><span class="line"><span class="comment">//    '------------------------'           |</span></span><br><span class="line"><span class="comment">//                 |                       |</span></span><br><span class="line"><span class="comment">//                 v                       |</span></span><br><span class="line"><span class="comment">//   .--------------------------.          |</span></span><br><span class="line"><span class="comment">//   | locate pod and container |          |</span></span><br><span class="line"><span class="comment">//   '--------------------------'          |</span></span><br><span class="line"><span class="comment">//                 |                       |</span></span><br><span class="line"><span class="comment">//                 v                       |</span></span><br><span class="line"><span class="comment">//   .---------------------------.         |</span></span><br><span class="line"><span class="comment">//   | write down configure and  |         |</span></span><br><span class="line"><span class="comment">//   | pid file with containerID |         |</span></span><br><span class="line"><span class="comment">//   | as name                   |         |</span></span><br><span class="line"><span class="comment">//   '---------------------------'         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         v</span></span><br></pre></td></tr></table></figure>
<h3 id="vGPU-Library"><a href="#vGPU-Library" class="headerlink" title="vGPU Library"></a>vGPU Library</h3><p>论文中的 <code>vGPU Library</code>，具体实现为 <a href="https://github.com/tkestack/vcuda-controller" target="_blank" rel="external nofollow noopener noreferrer">vcuda-controller</a> ，它运行在容器中用于管理部署在容器中的GPU资源。这个 <code>vGPU Library</code> 本质上就是自己封装了 <code>CUDA Library</code>，劫持了 <code>memory-related</code> API 和 <code>computing-related</code> API，下表显示了劫持的API。</p>
<p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-vcuda.png"></p>
<p><code>vCUDA</code> 在调用相应API时检查：</p>
<ul>
<li>对于显存，一旦该任务申请显存后占用的显存大小大于config中的设置，就报错。</li>
<li>对于计算资源，存在硬隔离和软隔离两种方式<ul>
<li>共同点是当任务使用的GPU SM利用率超出资源上限，则暂缓下发API调用。</li>
<li>不同点是如果有资源空闲，软隔离允许任务超过设置，动态计算资源上限。而硬隔离则不允许超出设置量。</li>
</ul>
</li>
</ul>
<p>这里对于其具体实现按下不表。</p>
<p>一个令人疑惑的问题是，在GPU Manager中，用户的容器是如何能够使用这个动态库的呢？具体有两个问题：</p>
<ul>
<li>这个库从哪里来？<ul>
<li><code>GPU Manager</code> 作为 <code>DaemonSet</code> 会在其Image中将我们自定义的库打包进去，然后挂载到Node上的一个目录。</li>
</ul>
</li>
<li>容器中的应用是如何感知到的？<ul>
<li>这里主要是通过在创建容器的时候，设置 <code>LD_LIBRARY_PATH</code> ，将其指向这个自定义的动态库的地址。</li>
</ul>
</li>
</ul>
<h3 id="资源监控统计"><a href="#资源监控统计" class="headerlink" title="资源监控统计"></a>资源监控统计</h3><p>这部分代码还没有看。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/designs/designs.md" target="_blank" rel="external nofollow noopener noreferrer">阿里GPUShare设计文档</a></li>
<li><a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">阿里共享调度使用文档</a></li>
<li><a href="https://ieeexplore.ieee.org/document/8672318" target="_blank" rel="external nofollow noopener noreferrer">Gaia GPUManager论文</a></li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章推荐</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/3f069334/" rel="bookmark">【Kubernetes】Device Plugin</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/3bc1a603/" rel="bookmark">【Kubernetes】ApiServer 启动分析</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/574111db/" rel="bookmark">【Kubernetes】在Docker中使用GPU</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/d8b96fe4/" rel="bookmark">【Kubernetes】开篇</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/b0614056/" rel="bookmark">【Kubernetes】API Install</a></div>
    </li>
  </ul>

      
        <div class="reward-container">
  <div></div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/wechatpay.png" alt="Houmin 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/alipay.jpg" alt="Houmin 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Houmin
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://houmin.cc/posts/cf391335/" title="【Kubernetes】GPU 共享">http://houmin.cc/posts/cf391335/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/k8s/" rel="tag"><i class="fa fa-tag"></i> k8s</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/device-plugin/" rel="tag"><i class="fa fa-tag"></i> device plugin</a>
              <a href="/tags/%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB/" rel="tag"><i class="fa fa-tag"></i> 资源隔离</a>
              <a href="/tags/scheduler-extender/" rel="tag"><i class="fa fa-tag"></i> scheduler extender</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/posts/574111db/" rel="next" title="【Kubernetes】在Docker中使用GPU">
                  <i class="fa fa-chevron-left"></i> 【Kubernetes】在Docker中使用GPU
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/posts/b0b2b640/" rel="prev" title="めぐる季节">
                  めぐる季节 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#阿里GPUShare"><span class="nav-number">1.</span> <span class="nav-text">阿里GPUShare</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#架构设计"><span class="nav-number">1.1.</span> <span class="nav-text">架构设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#假设条件"><span class="nav-number">1.1.1.</span> <span class="nav-text">假设条件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#设计原则"><span class="nav-number">1.1.2.</span> <span class="nav-text">设计原则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核心组件"><span class="nav-number">1.1.3.</span> <span class="nav-text">核心组件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#具体过程"><span class="nav-number">1.2.</span> <span class="nav-text">具体过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#设备资源报告"><span class="nav-number">1.2.1.</span> <span class="nav-text">设备资源报告</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调度插件扩展"><span class="nav-number">1.2.2.</span> <span class="nav-text">调度插件扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Filter"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Filter</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Bind"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Bind</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kubelet创建Pod"><span class="nav-number">1.2.3.</span> <span class="nav-text">Kubelet创建Pod</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#腾讯GPUManager"><span class="nav-number">2.</span> <span class="nav-text">腾讯GPUManager</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#架构设计-1"><span class="nav-number">2.1.</span> <span class="nav-text">架构设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#设计原则-1"><span class="nav-number">2.1.1.</span> <span class="nav-text">设计原则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核心组件-1"><span class="nav-number">2.1.2.</span> <span class="nav-text">核心组件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#具体过程-1"><span class="nav-number">2.2.</span> <span class="nav-text">具体过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#设备资源上报"><span class="nav-number">2.2.1.</span> <span class="nav-text">设备资源上报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#调度插件扩展-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">调度插件扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#细粒度Quota准入"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">细粒度Quota准入</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#避免GPU碎片化"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">避免GPU碎片化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kubelet创建Pod-1"><span class="nav-number">2.2.3.</span> <span class="nav-text">Kubelet创建Pod</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#vGPU-Manager"><span class="nav-number">2.2.4.</span> <span class="nav-text">vGPU Manager</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vGPU-Library"><span class="nav-number">2.3.</span> <span class="nav-text">vGPU Library</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#资源监控统计"><span class="nav-number">2.4.</span> <span class="nav-text">资源监控统计</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">3.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="Houmin" src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/theme/avatar.png">
  <p class="site-author-name" itemprop="name">Houmin</p>
  <div class="site-description" itemprop="description">丈夫拥书万卷，何假南面百城</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">145</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">206</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SimpCosm" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;SimpCosm" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:weihoumin@gmail.com" title="E-Mail &amp;rarr; mailto:weihoumin@gmail.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="hitokoto">
    <!-- hitokoto -->
    <div id="hito-expression">:D 获取中...</div>

    <script src="https://cdn.jsdelivr.net/npm/bluebird@3/js/browser/bluebird.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/whatwg-fetch@2.0.3/fetch.min.js"></script>
    <script>
      fetch('https://v1.hitokoto.cn')
        .then(function (res){
          return res.json();
        })
        .then(function (data) {
          var hitokoto = document.getElementById('hito-expression');
          hitokoto.innerText = data.hitokoto + '——【' + data.from + '】';
        })
        .catch(function (err) {
          console.error(err);
        })
    </script>
  </div>

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Houmin</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.5m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">45:57</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>












        
      </div>
    </footer>
  </div>



  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.1.0/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>



  




  <script src="/js/local-search.js"></script>








<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '800px'
      });
    });
  }, window.PDFObject);
}
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>



  

  

  


<script>
NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'iEBFuhVyk4tuhVYctQ265uid-gzGzoHsz',
    appKey: 'KGjOktrtgSEWK1v9DYA3T3Az',
    placeholder: "Just go go",
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'zh-cn' || 'zh-cn',
    path: location.pathname,
    recordIP: true,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
