<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Houmin</title>
  
  <subtitle>Yesterday You Said Tomorrow</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://houmin.cc/"/>
  <updated>2020-12-31T15:54:36.389Z</updated>
  <id>http://houmin.cc/</id>
  
  <author>
    <name>Houmin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020 渺小</title>
    <link href="http://houmin.cc/posts/9d4b744a/"/>
    <id>http://houmin.cc/posts/9d4b744a/</id>
    <published>2020-12-31T14:47:19.000Z</published>
    <updated>2020-12-31T15:54:36.389Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>当写下 <a href="https://houmin.cc/posts/2ecc368">2019 未来へ</a> 的时候，我没有意识到2020年将会发生什么。那时的我是轻松惬意的，文字里透露着面向未来的自信，期冀着能够做出自己的改变。一年过去了，咻的一下，很快，生活中的种种已经发生了改变。2020 年发生的种种让我更加意识到到自身的渺小与浅陋，绝大多数时候你改变不了什么，你能确定改变的只能是自己，尽管这并不简单。</p><p>这里是 「岁末围炉」系列的第二篇 <code>2020 渺小</code>，主题曲选择的是田馥甄的渺小，封面图来自冬日的雍和宫。 2020 年结束了，渺小的我依然在构建自己的框架，向着生命，向着未来。</p>    <div id="aplayer-FmHyJOmD" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="27968284" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="世界你好"><a href="#世界你好" class="headerlink" title="世界你好"></a>世界你好</h2><p>有人说，2020 年是这些年来最糟糕的一年，2020年是见证历史的一年，正如时代周刊在年底的一期杂志封面所提到的一样：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-14_time-2020-cover.jpg"></p><ul><li>年初，美军定点清除伊朗名将之花苏莱曼尼，美伊爆发克制性冲突，伊朗导弹误击乌克兰民航客机，176人死亡</li><li>年初，新冠疫情在武汉爆发，李文亮之死引发全国震动，武汉封城，全国驰援下湖北控制住疫情，中国控制住疫情</li><li>然而，疫情继续在全球爆发，截止2020年底全球累计确诊人数到达8000万人，累计死亡人数180万人，东京奥运会延期</li><li>因为新冠疫情的爆发，全球经济衰退，年初美股四次熔断，历史罕见负油价，在美联储持续降息放水下，美股在下半年一路长红</li><li>美国黑人佛洛伊德被警察跪杀，引发美国几十年来规模最大的反种族歧视、反暴力执法抗议，BLM运动下，众多历史雕像被摧毁</li><li>美国2020年大选跌宕起伏，拜登最终赢得大选，但特朗普所代表的群众依然广泛，美国党派冲突、阶层冲突矛盾进一步激化</li><li>中美贸易冲突进一步加剧，美国封杀TikTok、微信等企业，中美互关总领事馆</li><li>互联网行业整治监管趋紧，蚂蚁金服暂停上市，阿里、腾讯、顺丰被罚，巨头入局社区团购引发争议，蛋壳公寓暴雷</li><li>……</li></ul><p>所有的这些事件中，新冠疫情是影响最大的因素。事实证明，与非典不同，我们已经不能指望它在夏天凭空消失，我们所能依靠的只有疫苗。尽管在国内疫情已经得到了控制，但是全球疫情仍然严重，在未来的两年里，它仍将一直围绕着我们，各国之间仍然不能自由通航，以前的那种人员自由流动已经成为一种奢望。伴随着疫情，全球经济持续衰退，有可能造成甚于2008年的全球经济大危机，贫富差距加大，阶层矛盾进一步激化，民粹主义进一步发展。</p><p>显而易见的是，这个世界的情况没有在变好，而在过去十年间进一步恶化，08年以来的次贷危机和欧债危机造成的负面影响至今都还没有完全释放。在没有新的技术革新发生的情况下，未来的环境还会更差，战争的发生不是没有可能。所以，在天灾、疫情、战争带来的威胁下，我们能够做什么呢？我们什么都做不了，在这些面前我们只是渺小的蝼蚁。我们能够做的，就是尝试去理解世界，完善自己。</p><h2 id="行为框架"><a href="#行为框架" class="headerlink" title="行为框架"></a>行为框架</h2><p>在过去的一年，我一直在尝试去更好地理解自己的行为模式，去记录数据，去分析数据，去更新自己的模型。尽管还没有达到我想要的效果，但是我明显地感觉到，<code>I&#39;m on my way</code>。这种记录与分析里最重要的一种表达方式，就是我给自己建立的这个站点，或者也可以说是我的个人博客。虽然除了个人博客，我还想在这个站点记录更多的东西，不过不管怎么样我们可以慢慢来。如你所见，在<a href="https://houmin.cc/about/"> <code>houmin.cc</code></a>，我创建了几个不同的栏目用于定期的去总结、去梳理生活与学习工作中碰到的问题。</p><p>除了一年一度的 <a href="https://houmin.cc/categories/岁末围炉">岁末围炉</a>，另一个重要的专栏就是 <a href="https://houmin.cc/categories/朝花夕拾">朝花夕拾</a> 了。「朝花夕拾」是我给自己安排的每周总结，2020年第一期是 <a href="https://houmin.cc/posts/3e030bdb/">Carpe Diem</a>，最后一期是 <a href="https://houmin.cc/posts/64c2f65e">十字路口</a>。尽管号称每周更新，期间不乏拖更与断更，下面是2020年的「朝花夕拾」发布记录。可以看到，到2020年结束，朝花夕拾总共发布了29期。如果每周按照正常发布的节奏（以及排除年终/生日那周），总计应该有50期。尽管只是刚刚超过预期的一半，令我高兴的是整个「朝花夕拾」的框架已经建立。</p><div id="echarts2220" style="width: 100%;height: 600px;margin: 0 auto"></div><script type="text/javascript" src="https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js"></script><script type="text/javascript" src="http://gallery.echartsjs.com/dep/echarts/map/js/china.js"></script><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('echarts2220'));  // 指定图表的配置项和数据  option = {    title: {        text: '朝花夕拾'    },    tooltip: {        trigger: 'axis',        axisPointer: {            type: 'cross',            crossStyle: {                color: '#999'            }        }    },    toolbox: {        feature: {            dataView: {show: true, readOnly: false},            magicType: {show: true, type: ['line', 'bar']},            restore: {show: true},            saveAsImage: {show: true}        }    },    legend: {        data: ['单月发布', '累计发布']    },    xAxis: [        {            type: 'category',            data:  ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],            axisPointer: {                type: 'shadow'            }        }    ],    yAxis: [        {            type: 'value',            name: '单月发布',            min: 0,            max: 4,            interval: 4,            axisLabel: {                formatter: '{value} 篇'            }        },        {            type: 'value',            name: '累计发布',            min: 0,            max: 50,            interval: 4,            axisLabel: {                formatter: '{value} 篇'            }        }    ],    series: [        {            name: '单月发布',            type: 'bar',            data: [3, 4, 4, 4, 1, 1, 4, 1, 0, 0, 4, 3],        },        {            name: '累计发布',            type: 'line',            yAxisIndex: 1,            data: [3, 7, 11, 15, 16, 17, 21, 22, 22, 22, 26, 29],        }    ]};  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);</script><p>看看那些没能够定期发布的时间段，我依稀还能想到当时发生了什么。五月和六月，因为面临毕业论文答辩，没有花太多的时间在博客上，7月份入职后，在八月、九月和十月基本上都在消化吸收工作中涉及到的知识点，你可以看到那段时间我在 「术业专攻」中发布了很多专业内容的笔记。如果没有十一月的回归正常发布，今年的曲线将会很难看。</p><p>按照我多年来新年计划失效的经验（如果说这次朝花夕拾没能够完全按期发布也算是一次失败经验的话），如果没有定期的回顾与总结，一旦某段时间放松自己，偏离目标，就会因为破窗效应索性放弃了整个一年的计划。一方面是缺少定期的回顾，另外一方面是自己设定的计划完成时间粒度太粗，一旦某段时间放弃则全盘放弃，正如我年初在 <a href="https://houmin.cc/posts/c924112f">Do You Want To Build A Snowman?</a> 这篇博文中提到的一样。</p><p>现在回过头看，非常庆幸自己在2019年底有一段空闲时间给自己建立了一个框架，并在后续的日子里能够坚持实践。也许没有完全达到我期待中的效果，至少框架已经初具模型。事实上，早在16年的时候我就创建过自己的个人网站，也比较早的意识到博客系统可能会对我产生的帮助。然而（按照我现在的理解），正是因为没有建立一个好的可以执行下去的框架，导致当年创建的博客两次中断，在后续的日子里也没有留下多少有价值的记录。我也曾经建立过自己的 wiki 平台，但是因为项目维护和数据的获取更新等问题没有坚持下去。现在的这种框架对于我来说是最好的方式。</p><p>建立框架对于知识体系的构建产生帮助的最为典型的一个例子是，「术业专攻」这个栏目的建立与文章发布。在年初的时候，我在  <a href="https://houmin.cc/posts/c924112f">Do You Want To Build A Snowman?</a> 这篇博文讨论过个人知识管理的话题，当时得到的几个明确结论是：</p><ul><li>Data、Information、Knowledge、Wisdom这几个概念之间有着明显的区别，看到数据并不等于获取信息，阅读博客和书籍并不等同于学会了知识，知道了知识并不等同于掌握了智慧</li><li>提高知识获取效率最好的方法是主动去搜索资料，而不是被动的在信息流中流于表面的学习</li><li>只靠搜索不能内化知识，虽然可能在某个地方能够看到这些知识，但他们永远不是你的</li><li>只靠把知识存储在文件夹或笔记中，而不去花时间整理它们不能内化知识，你需要通过自己的整理去建立索引，方便以后更好的调用这些知识</li><li>图像、思维导图、定期回顾复习、向他人表述能够更好的帮助知识内化</li></ul><p>结合这几个结论，当前我的博客系统就是我最好的知识管理工具，也是我自己的笔记系统。截止2020年底，我在「术业专攻」总共发布了90篇博文，其中大部分是我在专业内容上的笔记总结。目前原创性的内容还比较少，更多的是在做笔记、做梳理、做总结。通过在发布博文的过程中，我会去结合自身的理解去消化吸收这些内容，在我的知识体系中构建自己的索引。在这个过程中，我越发体会到自己的浅陋，除了发布的这些博文，还有很多那些还没来得及被消化和发布的内容存在我的草稿箱中。在过去的一年中，我还剩下 300+ 的博文没有发布，这需要大量的时间去消化吸收。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-19_hexo-drafts.png"></p><p>这是一个动态增长的过程，每每在工作中碰到一些新的知识点，我会去搜索相关的资料并保存于草稿箱中。保存于草稿箱并不等同于我消化吸收了对应的内容，我还需要去按照自己的思维脉络，整理出属于自己的文章。之后再碰到对应的问题后，我就可以很好的在自己的知识体系中定位问题，索引出相关知识点。即使短暂的不记住了，我也可以再来博客系统来回顾。</p><p>除了专业相关的内容，我还根据自己的兴趣点，建立了 <a href="https://houmin.cc/categories/资本不眠">「资本不眠」</a>、<a href="https://houmin.cc/categories/好奇计划">「好奇计划」</a> 等栏目，用于记录专业内容之外的那些有意思的内容，并且可能还会去进一步扩展更多的栏目。记录、消化、总结、思考、时间、提高，是获取知识、创造新知识的最好办法，而 <code>houmin.cc</code> 就是我在这上面最好的框架与平台。</p><p>根据我前面一以贯之的理念，为了更好的成长，我们需要用数据去量化成长的指标，从而进一步反思与优化自己的行为模式。对于我这个系统最基础的框架也是一样，我们也需要数据去衡量这个框架的进步与效果。也就是说，你如何反映出你的框架在实际work，你如何知道自己的产出是否有实际意义，作为一个网站，我给他最明确的指标就是网站搜索与访问量。</p><p>听起来有点像运营微信微博等新媒体账号，对于你运营的网站，你期待的指标就是他的点击量和访问量获得显著提升。但是，与微信微博稍微有点不同的是，我的网站将完全基于搜索获取流量，而不是依赖于社交网络的传播。一方面我不得不承认，我现在网站中的很多内容还很粗浅，还不足以吸引人们在社交网络中传播它；另一方面，与我前面提到的知识管理理念相匹配，应该是人找信息，而不是信息找人，我们才是信息获取的发起方，通过搜索得到的内容将发挥更大的作用。</p><p>下图是 <code>houmin.cc</code> 在过去一年来的搜索数据，尽管目前网站的日均访问量仍然很小很小，但是已经可以看到相比于2019年的日均访问量已经有了一个量级的提高。未来的一年，给自己定下的一个目标是，要进一步修正和完善这个框架，创作更多有价值的内容。当我们实现这个目标后，可以看到的一个效果是，这个站点的日均访问量将进一步提升一个量级。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_site-google-search.png"></p><p>虽然在这里对访问量提出了一定的指标，更关键的是要提高自己网站内容的质量，输出有价值的内容，提高自己的影响力。这里提出这样的目标，源自于之前在网络上搜索问题时，碰见的那些 <a href="https://houmin.cc/links">有意思的站点</a> 。他们的博客记录着他们的思考，记录着那些有价值的内容，是我想要实现的目标。</p><h2 id="好好工作"><a href="#好好工作" class="headerlink" title="好好工作"></a>好好工作</h2><p>2020年7月，我从学校进入职场，开始了正式的职场生活。半年的工作让我对自己未来的期待，对当前的不足有了更加清晰的认识。经过几段实习经历和半年的工作经历，你已经见证了各种各样的前辈，你也看到了作为一个专业的技术工程师应该具备的能力。2021年，继续好好工作，扩展自己的深度和广度，加油！</p><div class="table-container"><table><thead><tr><th>考察指标</th><th>具体指标</th><th>实践改进方式</th></tr></thead><tbody><tr><td>技术实力</td><td>掌握扎实的底层技术原理，包括 kubernetes、linux、网络、Go语言等方面</td><td>「术业专攻」博客更新</td></tr><tr><td>业界动态</td><td>洞悉业界动态，知道大家现在都在做什么，未来的趋势在哪里，我们可以做的事情有哪些</td><td>创建「业界动态」专栏</td></tr><tr><td>开源影响力</td><td>积极参与开源社区贡献，在社区构建自己的影响力</td><td>成为k8s社区Member</td></tr><tr><td>产品思维</td><td>在公司内部积极完成leader下发的任务，并能够从产品和业务的角度去更好的思考问题</td><td>公司影响力提升</td></tr></tbody></table></div><h2 id="阅读光影"><a href="#阅读光影" class="headerlink" title="阅读光影"></a>阅读光影</h2><p>和2019年一样，2020年我并没有阅读多少书籍，豆瓣上标记的只有3本，但是我仍然愿意在这里花上一段篇幅来讨论它。从2014年注册豆瓣以来，下面是我每年在豆瓣读书上标记的阅读数，可以看到，本科时期是我阅读热情的高峰，尤其是大二和大三的时候，基本上每年阅读数能够达到50本。可是，读研之后，阅读的书就越来越少了，甚至在19年毫无所获。</p><div id="echarts2531" style="width: 100%;height: 600px;margin: 0 auto"></div><script type="text/javascript" src="https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js"></script><script type="text/javascript" src="http://gallery.echartsjs.com/dep/echarts/map/js/china.js"></script><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('echarts2531'));  // 指定图表的配置项和数据  option = {    title: {        text: '豆瓣阅读数据'    },    tooltip: {        trigger: 'axis',        axisPointer: {            type: 'cross',            crossStyle: {                color: '#999'            }        }    },    toolbox: {        feature: {            dataView: {show: true, readOnly: false},            magicType: {show: true, type: ['line', 'bar']},            restore: {show: true},            saveAsImage: {show: true}        }    },    legend: {        data: ['阅读统计']    },    xAxis: [        {            type: 'category',            data:  ['2014', '2015', '2016', '2017', '2018', '2019', '2020'],            axisPointer: {                type: 'shadow'            }        }    ],    yAxis: [        {            type: 'value',            name: '阅读统计',            min: 0,            max: 60,            interval: 5,            axisLabel: {                formatter: '{value}'            }        }    ],    series: [        {            name: '阅读统计',            type: 'bar',            data: [25, 55, 49, 6, 6, 0, 3]        }    ]};  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);</script><p>2020年的上半年也是基本如此，没有阅读什么书籍。没有人会否认阅读的重要性，相对于从公众号或者知乎里面获取的零碎知识，阅读书籍是一种更加体系化的知识获取方式。在本科时期，我对于历史和小说比较感兴趣，这些领域的阅读都相对轻松。那个时候我经常会很快地扫读，也很少去写一些阅读笔记和书评，很多书籍看完之后就丢在一边，导致阅读的相对收益并不高。</p><p>双十一之后，我从PT那里拿来了 Kindle，开始构建自己的阅读系统，构建主题阅读的方法论：</p><ul><li>问题意识。为什么要读一本书，是因为你心中对于某个主题包含着困惑（小说等书除外），你是抱着解答这些问题的期待翻开一本书的。我给自己建立了一个<a href="https://www.douban.com/doulist/133636602/" target="_blank" rel="external nofollow noopener noreferrer">2021阅读清单</a>，这是一个很实用主义的书单，涉及到各种经济学的原理与实践、涉及到行为模式的构建、涉及到专业知识的深化，我希望通过阅读这些书去解决这些问题。</li><li>整理书籍的知识框架。一本书的精髓在于作者的框架逻辑，而不在于其遣词造句，读书最重要读的是其知识框架。</li><li>交叉阅读验证知识框架。同一个主题往往不止一本书籍，阅读同一主题中的不同书籍可以交叉验证之前总结的知识框架。</li><li>将书籍的知识框架打入到你的知识体系，将内化的知识表达出来。这个过程中，读书笔记是第一步，通过读书笔记，可以实现对书籍的二次精读，更加关注知识的整体架构，获取的知识才是成体系的，而不是碎片化的信息。</li></ul><p>以此方法论为基础，我将2020年阅读的三本书籍分别写下了阅读笔记，列次如下：</p><ul><li><a href="https://houmin.cc">【读书笔记】如何阅读一本书</a></li><li><a href="https://houmin.cc">【读书笔记】光变</a></li><li><a href="https://houmin.cc">【读书笔记】褚时健传</a></li></ul><p>与阅读相同，希望新的一年观影也能够感受更多的东西。在年初的时候，当时对 2008 年的经济危机十分感兴趣，看了好几部关于次贷危机的纪录片与电影，并以 <a href="https://houmin.cc/posts/787197ce/">The Big Short</a> 的形式总结发布出来。</p><p>新的一年，我也给自己建立了一个 <a href="https://www.douban.com/doulist/133864290/" target="_blank" rel="external nofollow noopener noreferrer">2021观影清单</a>，观影的同时，希望有更多的文章发布。</p><h2 id="资本不眠"><a href="#资本不眠" class="headerlink" title="资本不眠"></a>资本不眠</h2><p>2019 年初，给自己定下的一个目标是股市收益超过20个点，最好能够达到40个点。得益于今年不错的行情，再一次达到了这个目标，最终收益率达到了 <code>35%</code>，稍稍跑赢沪深300。不得不说，在投资方面我仍然没有入门，这是这两年来的股市行情向好，才没有亏钱。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_invest-return.jpg"></p><p>对于股票投资，我现在最大的问题在于，我不知道为什么要买一只股票。对于一只真正好的股票，股票涨了一些的时候我能不能拿得住，股票跌的时候我会不会害怕。这家公司身处什么行业，其主营业务是什么，它的管理层是一些怎样的人，它在过去几年的业绩怎么样，公司和行业的发展怎么样？现在处于经济周期的哪个阶段？股票建仓的策略如何？你对这只股票预期的收益是多少？</p><p>上面的种种问题，我心里都没有底。今年最大的一个遗憾是新能源，看看下面来自我账户的两张图：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_gwm.jpg"></p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_byd.jpg"></p><p>对的，这两只在今年大涨了4倍的股票我在他们起飞之前都已经关注过。这还不算，19年初就已经关注起新能源汽车行业，并且在 2020 年初还专门写了篇 <a href="https://houmin.cc/posts/8082c63c/">聊聊行业：以新能源汽车为例</a>  的文章，作为行业研究的入门笔记。然而，我还是错过了这一切：）</p><p>为什么？想想当初买这两只股票的逻辑：哟，长城汽车最高价到过16块，现在才七八块，绝对便宜啊，而且听说长城哈弗卖的挺不错的，不亏。然后我在长城处于7块到10块震荡的时候一直在做T，到10块后稍微跌了跌就清仓了。嗯，也能够赚点钱，但是 ：）比亚迪也基本是这样很早就卖了。</p><p>说实话，真的很心痛，要是没有卖该多好：）不过说起来，没能拿住，本质上就是自己认知思维没打开，整个投资框架没有建立起来，这笔钱还不属于我。2020年的下半年，我完全没有操作，一方面因为这两只股票的教训已经从认知上更倾向于长期投资，不太喜欢这种短期波动获取的收益；另一方面因为工作原因，没有太多的时间去研究其他的股票，也就没有去操作。</p><p>新的一年，在投资方面还是要去建立自己的投资框架，熟悉了解财经相关的知识，现在对财经方面的兴趣越来越强了（不愧是个俗人：）在2021年，希望至少完成5篇财经方面书籍的阅读，并且整理相关阅读笔记并发布。另外，除了A股投资，也要给自己开一个美股和港股的账户，毕竟那边有很多很好的标的；除了现在的股票投资，明年应该开始指数基金定投计划。</p><p>还是那句话，希望明年整体投资收益超过20%，最好能够到40%，加油。</p><h2 id="旅行影像"><a href="#旅行影像" class="headerlink" title="旅行影像"></a>旅行影像</h2><p>因为疫情，原来的旅行计划被迫放弃。令人开心的是，随着国内疫情的控制，今年还是除了北京，还是走了走杭州和西北，我将所看到的记录在了这些文章里。</p><ul><li><a href="https://houmin.cc/posts/2f653e3b/">西湖印象</a>：初夏到达杭州姐姐家，盛夏回到北京，杭州很美</li><li><a href="https://houmin.cc/posts/2561fdfd/">26：一个人的北京</a>：国庆去了趟大西北，真美</li></ul><p>2020 年我仍然在记录着，除了旅行中的光影，我也记录着身边的影像，你可以在 <a href="https://houmin.cc/tags/摄影">这里</a> 看到。越来越觉得，除了奇美的自然景观，人的影像有时候会更加生动。新的一年，希望自己能够更多的走出去，希望自己的镜头下能够记录更多更美的时刻。</p><h2 id="我与其他"><a href="#我与其他" class="headerlink" title="我与其他"></a>我与其他</h2><ul><li>2020年，我重新捡起了跑步，断断续续着，希望新的一年能够找回大四时跑步的状态，并参加一次马拉松</li><li>2020年，我开始审视自己的睡眠，并开始抓取自己的睡眠数据，希望新的一年能够将这一数据回归到正常</li><li>2020年，卡林巴琴最终没有坚持下来，可是我仍然记得音乐带来的放松，希望以后能够掌握一门乐器，也许吉他</li><li>2020年，「朝花夕拾」开始每周更多的记录，希望新的一年能够开拓数据收集的范围，记录世界纪录自己</li><li>2020年，我开始高密度的听起了播客，喜欢下班路上听着播客的放松状态，希望以后我也能够做一档自己的播客</li><li>2020年，我依旧是单身一个人，工作后能够投入到感情上面的时间越发减少，希望新的一年自己能够更加主动</li><li>2020年结束了，可以确定的是，我们以后会不止一次地回忆起这不平凡的一年，下面是来自「声东击西」记录2020的声音</li></ul><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="1500" height="80" src="//music.163.com/outchain/player?type=3&id=2071218639&auto=1&height=66"></iframe><hr><p>再见，2020</p><p>你好，2021</p><p>新年快乐！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当写下 &lt;a href=&quot;https://houmin.cc/posts/2ecc368&quot;&gt;2019 未来へ&lt;/a&gt; 的时候，我没有意识到2020年将会发生什么。那时的我是轻松惬意的，文字里透露着面向未来的自信，期冀着能够做出自己的改变。一年过去了，咻的一下，很快，生活中的种种已经发生了改变。2020 年发生的种种让我更加意识到到自身的渺小与浅陋，绝大多数时候你改变不了什么，你能确定改变的只能是自己，尽管这并不简单。&lt;/p&gt;
&lt;p&gt;这里是 「岁末围炉」系列的第二篇 &lt;code&gt;2020 渺小&lt;/code&gt;，主题曲选择的是田馥甄的渺小，封面图来自冬日的雍和宫。 2020 年结束了，渺小的我依然在构建自己的框架，向着生命，向着未来。&lt;/p&gt;

    &lt;div id=&quot;aplayer-FmHyJOmD&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;27968284&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-20_temple.png" type="image" />
    
    
      <category term="岁末围炉" scheme="http://houmin.cc/categories/%E5%B2%81%E6%9C%AB%E5%9B%B4%E7%82%89/"/>
    
    
      <category term="年终总结" scheme="http://houmin.cc/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>【深度学习】TensorFlow2</title>
    <link href="http://houmin.cc/posts/fcd0bf09/"/>
    <id>http://houmin.cc/posts/fcd0bf09/</id>
    <published>2020-12-21T06:17:24.000Z</published>
    <updated>2021-01-05T11:52:47.395Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>近年来，深度神经网络技术被大规模地使用在搜索、推荐、广告、翻译、语音、图像和视频等领域。与此同时，深度学习也在推动一些人类最重大的工程挑战，比如自动驾驶技术、医疗诊断和预测、个性化学习、加速科学发展（比如天文发现）、跨语言的自由交流（比如实时翻译），更通用的人工智能系统（比如 AlphaGo）等。</p><p>TensorFlow 是开源的端到端的机器学习平台，提供了丰富的工具链，推动了机器学习的前沿研究，支撑了大规模生产使用，支持多平台灵活部署。2019年10月，谷歌正式发布TensorFlow 2.0，相比于TensorFlow 1.0，TensorFlow 2 重点关注易用性，默认推荐使用 Keras 作为高阶 API，同时兼具可扩展性和高性能，默认为动态图方式执行。本文作为 Tensorflow2 学习笔记，主要参考<a href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days" target="_blank" rel="external nofollow noopener noreferrer">eat_tensorflow2_in_30_days</a>，对照着原教程在Docker环境下对于TensorFlow2进行学习，感谢原作者的贡献。</p><a id="more"></a><h2 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">unicosmos/tensorflow:2.4.0-gpu-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/tf/data</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">data-volume</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data-volume</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/root/data</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">jupyter-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8888</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br></pre></td></tr></table></figure><h2 id="模块与架构"><a href="#模块与架构" class="headerlink" title="模块与架构"></a>模块与架构</h2><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_tensorflow-model.png"></p><h2 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h2><p>TensorFlow最基本的一次计算流程通常是这样的：首先它接受n个固定格式的数据输入，通过特定的函数，将其转化为n个张量（Tensor）格式的输出。</p><p>一般来说，某次计算的输出很可能是下一次计算的（全部或部分）输入。整个计算过程其实是一个个Tensor 数据的流动过程。在这其中，TensorFlow将这一系列的计算流程抽象为了一张数据流图（Data Flow Graph）。简单来说，数据流图，就是在逻辑上描述一次机器学习计算的过程。下面我们以图11-26为例，来说明TensorFlow的几个重要概念。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_tensorflow-dataflow-graph.png"></p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_tensorflow-dataflow.png"></p><p>Tensorflow 变量 <code>Variable</code> 的主要总用是维护特定节点的状态，如深度学习或机器学习的模型参数。<code>tf.Variable</code> 的方法是 <code>Operation</code>，返回值是 <code>Tensor</code>。通过 <code>tf.Varaible</code> 方法创建的变量，与张量一样，可以作为操作的输入和输出。不同之处在于：</p><ul><li>张量的生命周期通常随依赖的计算完成而结束，内存也随之释放</li><li>变量则常驻内存，在每一步训练时不断更新其值，以实现模型参数的更新</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_tensorflow-variable.png"></p><p>下面是在代码中使用 <code>tf.Varaible</code> 的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">w = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将变量作为操作的输入</span></span><br><span class="line">y = tf.matmul(w, ...another variable <span class="keyword">or</span> tensor...)</span><br><span class="line">z = tf.sigmoid(w + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 assign 或 assign_xxx 方法重新给变量赋值</span></span><br><span class="line">w.assign(w + <span class="number">1.0</span>)</span><br><span class="line">w.assign_add(<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p>Tensorflow 用数据流图表示算法模型，数据流图由节点和有向边组成，每个节点均对应一个具体的操作 <code>Operation</code>。因此，<code>Operation</code> 是模型功能的实际载体。数据流图中的节点按照功能不同可以分为3种：</p><ul><li><strong>存储节点</strong>：有状态的变量操作，通常用来存储模型参数</li><li><strong>计算节点</strong>：无状态的计算或控制操作，主要负责算法逻辑表达或流程控制</li><li><strong>数据节点</strong>：数据的占位符操作，用于描述图外输入数据的属性</li></ul><p><strong>操作的输入和输出是张量或者操作（函数式编程）</strong></p><p>Tensorflow 的典型计算和控制操作如下：</p><div class="table-container"><table><thead><tr><th>操作类型</th><th></th></tr></thead><tbody><tr><td>基础算术</td><td>add/multiply/mod/sqrt/sin/trace/fft/argmin</td></tr><tr><td>数组运算</td><td>size/rank/split/reverse/cast/one_hot/quantize</td></tr><tr><td>梯度裁剪</td><td>clip_by_value/clip_by_norm/clip_by_global_norm</td></tr><tr><td>逻辑控制和调试</td><td>identity/logical_and/equal/less/is_finite/is_nan</td></tr><tr><td>数据流控制</td><td>enqueue/dequeue/size/take_grad/apply_grad</td></tr><tr><td>初始化操作</td><td>zeros_initilizer/random_normal_initializer/orthogonal_intializer</td></tr><tr><td>神经网络运算</td><td>convolution/pool/bias_add/softmax/dropout/erision2d</td></tr><tr><td>随机计算</td><td>random_normal/random_shuffle/multinomial/random_gramma</td></tr><tr><td>字符串运算</td><td>string_to_hash_bucket/reduce_join/substr/encode_base 64</td></tr><tr><td>图像处理运算</td><td>encode_png/resize_images/rot90/hsv_to_rgb/adjust_gamma</td></tr></tbody></table></div><p>TensorFlow 使用 <code>占位符操作</code> 表示图外输入的数据，如训练和测试数据。TensorFlow 数据流图描述了算法模型的计算拓扑，其中的各个操作（节点）都是抽象的函数映射或数学表达式。换句话说，数据流图本身是一个具有计算拓扑和内部结构的壳，在用户向数据流图填充数据前，途中并没有真正执行任何运算。</p><h2 id="建模流程"><a href="#建模流程" class="headerlink" title="建模流程"></a>建模流程</h2><p>尽管TensorFlow设计上足够灵活，可以用于进行各种复杂的数值计算。但通常人们使用TensorFlow来实现机器学习模型，尤其常用于实现神经网络模型。从原理上说可以使用张量构建计算图来定义神经网络，并通过自动微分机制训练模型。但为简洁起见，一般推荐使用TensorFlow的高层次keras接口来实现神经网络网模型。使用TensorFlow实现神经网络模型的一般流程包括：</p><ul><li>准备数据</li><li>定义模型</li><li>训练模型</li><li>评估模型</li><li>使用模型</li><li>保存模型</li></ul><p><strong>对新手来说，其中最困难的部分实际上是准备数据过程。</strong></p><p>我们在实践中通常会遇到的数据类型包括结构化数据，图片数据，文本数据，时间序列数据。</p><p>我们将分别以titanic生存预测问题，cifar2图片分类问题，imdb电影评论分类问题，国内新冠疫情结束时间预测问题为例，演示应用tensorflow对这四类数据的建模方法。</p><h3 id="结构化数据建模"><a href="#结构化数据建模" class="headerlink" title="结构化数据建模"></a>结构化数据建模</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>titanic数据集的目标是根据乘客信息预测他们在Titanic号撞击冰山沉没后能否生存。</p><p>结构化数据一般会使用Pandas中的DataFrame进行预处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers</span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">'./data/titanic/train.csv'</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">'./data/titanic/test.csv'</span>)</span><br><span class="line">dftrain_raw.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-1-AUC曲线.jpg"></p><p>字段说明：</p><ul><li>Survived:0代表死亡，1代表存活【y标签】</li><li>Pclass:乘客所持票类，有三种值(1,2,3) 【转换成onehot编码】</li><li>Name:乘客姓名 【舍去】</li><li>Sex:乘客性别 【转换成bool特征】</li><li>Age:乘客年龄(有缺失) 【数值特征，添加“年龄是否缺失”作为辅助特征】</li><li>SibSp:乘客兄弟姐妹/配偶的个数(整数值) 【数值特征】</li><li>Parch:乘客父母/孩子的个数(整数值)【数值特征】</li><li>Ticket:票号(字符串)【舍去】</li><li>Fare:乘客所持票的价格(浮点数，0-500不等) 【数值特征】</li><li>Cabin:乘客所在船舱(有缺失) 【添加“所在船舱是否缺失”作为辅助特征】</li><li>Embarked:乘客登船港口:S、C、Q(有缺失)【转换成onehot编码，四维度 S,C,Q,nan】</li></ul><p>利用Pandas的数据可视化功能我们可以简单地进行探索性数据分析EDA（Exploratory Data Analysis）。</p><p>label分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Survived'</span>].value_counts().plot(kind = <span class="string">'bar'</span>,</span><br><span class="line">     figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>,rot = <span class="number">0</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Counts'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Survived'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/blob/master/data/1-1-Label分布.jpg"></p><p>年龄分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw[<span class="string">'Age'</span>].plot(kind = <span class="string">'hist'</span>,bins = <span class="number">20</span>,color= <span class="string">'purple'</span>,</span><br><span class="line">                    figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">ax.set_ylabel(<span class="string">'Frequency'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-1-年龄分布.jpg"></p><p>年龄和label的相关性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'png'</span></span><br><span class="line">ax = dftrain_raw.query(<span class="string">'Survived == 0'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">dftrain_raw.query(<span class="string">'Survived == 1'</span>)[<span class="string">'Age'</span>].plot(kind = <span class="string">'density'</span>,</span><br><span class="line">                      figsize = (<span class="number">12</span>,<span class="number">8</span>),fontsize=<span class="number">15</span>)</span><br><span class="line">ax.legend([<span class="string">'Survived==0'</span>,<span class="string">'Survived==1'</span>],fontsize = <span class="number">12</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Density'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Age'</span>,fontsize = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-1-年龄相关性.jpg"></p><p>下面为正式的数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(dfdata)</span>:</span></span><br><span class="line"></span><br><span class="line">    dfresult= pd.DataFrame()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Pclass</span></span><br><span class="line">    dfPclass = pd.get_dummies(dfdata[<span class="string">'Pclass'</span>])</span><br><span class="line">    dfPclass.columns = [<span class="string">'Pclass_'</span> +str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfPclass.columns ]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfPclass],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Sex</span></span><br><span class="line">    dfSex = pd.get_dummies(dfdata[<span class="string">'Sex'</span>])</span><br><span class="line">    dfresult = pd.concat([dfresult,dfSex],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Age</span></span><br><span class="line">    dfresult[<span class="string">'Age'</span>] = dfdata[<span class="string">'Age'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">    dfresult[<span class="string">'Age_null'</span>] = pd.isna(dfdata[<span class="string">'Age'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#SibSp,Parch,Fare</span></span><br><span class="line">    dfresult[<span class="string">'SibSp'</span>] = dfdata[<span class="string">'SibSp'</span>]</span><br><span class="line">    dfresult[<span class="string">'Parch'</span>] = dfdata[<span class="string">'Parch'</span>]</span><br><span class="line">    dfresult[<span class="string">'Fare'</span>] = dfdata[<span class="string">'Fare'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Carbin</span></span><br><span class="line">    dfresult[<span class="string">'Cabin_null'</span>] =  pd.isna(dfdata[<span class="string">'Cabin'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Embarked</span></span><br><span class="line">    dfEmbarked = pd.get_dummies(dfdata[<span class="string">'Embarked'</span>],dummy_na=<span class="literal">True</span>)</span><br><span class="line">    dfEmbarked.columns = [<span class="string">'Embarked_'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> dfEmbarked.columns]</span><br><span class="line">    dfresult = pd.concat([dfresult,dfEmbarked],axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>(dfresult)</span><br></pre></td></tr></table></figure><p>运行数据预处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_train = preprocessing(dftrain_raw)</span><br><span class="line">y_train = dftrain_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">x_test = preprocessing(dftest_raw)</span><br><span class="line">y_test = dftest_raw[<span class="string">'Survived'</span>].values</span><br><span class="line"></span><br><span class="line">print(<span class="string">"x_train.shape ="</span>, x_train.shape )</span><br><span class="line">print(<span class="string">"x_test.shape ="</span>, x_test.shape )</span><br></pre></td></tr></table></figure><p>可以看到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train.shape = (<span class="number">712</span>, <span class="number">15</span>)</span><br><span class="line">x_test.shape = (<span class="number">179</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure><h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><p>使用Keras接口有以下3种方式构建模型：</p><ul><li>使用Sequential按层顺序构建模型</li><li>使用函数式API构建任意结构模型</li><li>继承Model基类构建自定义模型。</li></ul><p>此处选择使用最简单的Sequential，按层顺序模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">20</span>,activation = <span class="string">'relu'</span>,input_shape=(<span class="number">15</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,activation = <span class="string">'relu'</span> ))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span> ))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>模型输出</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 20)                320       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                210       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              (None, 1)                 11        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 541</span><br><span class="line">Trainable params: 541</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>训练模型通常有3种方法：</p><ul><li>内置fit方法</li><li>内置train_on_batch方法</li><li>自定义训练循环。</li></ul><p>此处我们选择最常用也最简单的内置fit方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类问题选择二元交叉熵损失函数</span></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'AUC'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train,y_train,</span><br><span class="line">                    batch_size= <span class="number">64</span>,</span><br><span class="line">                    epochs= <span class="number">30</span>,</span><br><span class="line">                    validation_split=<span class="number">0.2</span> <span class="comment">#分割一部分训练数据用于验证</span></span><br><span class="line">                   )</span><br></pre></td></tr></table></figure><p>训练过程如下：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">Train on <span class="number">569</span> samples, validate on <span class="number">143</span> samples</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">1</span>s <span class="number">2</span>ms/sample - loss: <span class="number">3.5841</span> - AUC: <span class="number">0.4079</span> - val_loss: <span class="number">3.4429</span> - val_AUC: <span class="number">0.4129</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">102</span>us/sample - loss: <span class="number">2.6093</span> - AUC: <span class="number">0.3967</span> - val_loss: <span class="number">2.4886</span> - val_AUC: <span class="number">0.4139</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">68</span>us/sample - loss: <span class="number">1.8375</span> - AUC: <span class="number">0.4003</span> - val_loss: <span class="number">1.7383</span> - val_AUC: <span class="number">0.4223</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">83</span>us/sample - loss: <span class="number">1.2545</span> - AUC: <span class="number">0.4390</span> - val_loss: <span class="number">1.1936</span> - val_AUC: <span class="number">0.4765</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - ETA: <span class="number">0</span>s - loss: <span class="number">1.4435</span> - AUC: <span class="number">0.375</span> - <span class="number">0</span>s <span class="number">90</span>us/sample - loss: <span class="number">0.9141</span> - AUC: <span class="number">0.5192</span> - val_loss: <span class="number">0.8274</span> - val_AUC: <span class="number">0.5584</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">110</span>us/sample - loss: <span class="number">0.7052</span> - AUC: <span class="number">0.6290</span> - val_loss: <span class="number">0.6596</span> - val_AUC: <span class="number">0.6880</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">90</span>us/sample - loss: <span class="number">0.6410</span> - AUC: <span class="number">0.7086</span> - val_loss: <span class="number">0.6519</span> - val_AUC: <span class="number">0.6845</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">93</span>us/sample - loss: <span class="number">0.6246</span> - AUC: <span class="number">0.7080</span> - val_loss: <span class="number">0.6480</span> - val_AUC: <span class="number">0.6846</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">73</span>us/sample - loss: <span class="number">0.6088</span> - AUC: <span class="number">0.7113</span> - val_loss: <span class="number">0.6497</span> - val_AUC: <span class="number">0.6838</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">79</span>us/sample - loss: <span class="number">0.6051</span> - AUC: <span class="number">0.7117</span> - val_loss: <span class="number">0.6454</span> - val_AUC: <span class="number">0.6873</span></span><br><span class="line">Epoch <span class="number">11</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">96</span>us/sample - loss: <span class="number">0.5972</span> - AUC: <span class="number">0.7218</span> - val_loss: <span class="number">0.6369</span> - val_AUC: <span class="number">0.6888</span></span><br><span class="line">Epoch <span class="number">12</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">92</span>us/sample - loss: <span class="number">0.5918</span> - AUC: <span class="number">0.7294</span> - val_loss: <span class="number">0.6330</span> - val_AUC: <span class="number">0.6908</span></span><br><span class="line">Epoch <span class="number">13</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">75</span>us/sample - loss: <span class="number">0.5864</span> - AUC: <span class="number">0.7363</span> - val_loss: <span class="number">0.6281</span> - val_AUC: <span class="number">0.6948</span></span><br><span class="line">Epoch <span class="number">14</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">104</span>us/sample - loss: <span class="number">0.5832</span> - AUC: <span class="number">0.7426</span> - val_loss: <span class="number">0.6240</span> - val_AUC: <span class="number">0.7030</span></span><br><span class="line">Epoch <span class="number">15</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">74</span>us/sample - loss: <span class="number">0.5777</span> - AUC: <span class="number">0.7507</span> - val_loss: <span class="number">0.6200</span> - val_AUC: <span class="number">0.7066</span></span><br><span class="line">Epoch <span class="number">16</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">79</span>us/sample - loss: <span class="number">0.5726</span> - AUC: <span class="number">0.7569</span> - val_loss: <span class="number">0.6155</span> - val_AUC: <span class="number">0.7132</span></span><br><span class="line">Epoch <span class="number">17</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">99</span>us/sample - loss: <span class="number">0.5674</span> - AUC: <span class="number">0.7643</span> - val_loss: <span class="number">0.6070</span> - val_AUC: <span class="number">0.7255</span></span><br><span class="line">Epoch <span class="number">18</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">97</span>us/sample - loss: <span class="number">0.5631</span> - AUC: <span class="number">0.7721</span> - val_loss: <span class="number">0.6061</span> - val_AUC: <span class="number">0.7305</span></span><br><span class="line">Epoch <span class="number">19</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">73</span>us/sample - loss: <span class="number">0.5580</span> - AUC: <span class="number">0.7792</span> - val_loss: <span class="number">0.6027</span> - val_AUC: <span class="number">0.7332</span></span><br><span class="line">Epoch <span class="number">20</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">85</span>us/sample - loss: <span class="number">0.5533</span> - AUC: <span class="number">0.7861</span> - val_loss: <span class="number">0.5997</span> - val_AUC: <span class="number">0.7366</span></span><br><span class="line">Epoch <span class="number">21</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">87</span>us/sample - loss: <span class="number">0.5497</span> - AUC: <span class="number">0.7926</span> - val_loss: <span class="number">0.5961</span> - val_AUC: <span class="number">0.7433</span></span><br><span class="line">Epoch <span class="number">22</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">101</span>us/sample - loss: <span class="number">0.5454</span> - AUC: <span class="number">0.7987</span> - val_loss: <span class="number">0.5943</span> - val_AUC: <span class="number">0.7438</span></span><br><span class="line">Epoch <span class="number">23</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">100</span>us/sample - loss: <span class="number">0.5398</span> - AUC: <span class="number">0.8057</span> - val_loss: <span class="number">0.5926</span> - val_AUC: <span class="number">0.7492</span></span><br><span class="line">Epoch <span class="number">24</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">79</span>us/sample - loss: <span class="number">0.5328</span> - AUC: <span class="number">0.8122</span> - val_loss: <span class="number">0.5912</span> - val_AUC: <span class="number">0.7493</span></span><br><span class="line">Epoch <span class="number">25</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">86</span>us/sample - loss: <span class="number">0.5283</span> - AUC: <span class="number">0.8147</span> - val_loss: <span class="number">0.5902</span> - val_AUC: <span class="number">0.7509</span></span><br><span class="line">Epoch <span class="number">26</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">67</span>us/sample - loss: <span class="number">0.5246</span> - AUC: <span class="number">0.8196</span> - val_loss: <span class="number">0.5845</span> - val_AUC: <span class="number">0.7552</span></span><br><span class="line">Epoch <span class="number">27</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">72</span>us/sample - loss: <span class="number">0.5205</span> - AUC: <span class="number">0.8271</span> - val_loss: <span class="number">0.5837</span> - val_AUC: <span class="number">0.7584</span></span><br><span class="line">Epoch <span class="number">28</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">74</span>us/sample - loss: <span class="number">0.5144</span> - AUC: <span class="number">0.8302</span> - val_loss: <span class="number">0.5848</span> - val_AUC: <span class="number">0.7561</span></span><br><span class="line">Epoch <span class="number">29</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">77</span>us/sample - loss: <span class="number">0.5099</span> - AUC: <span class="number">0.8326</span> - val_loss: <span class="number">0.5809</span> - val_AUC: <span class="number">0.7583</span></span><br><span class="line">Epoch <span class="number">30</span>/<span class="number">30</span></span><br><span class="line"><span class="number">569</span>/<span class="number">569</span> [==============================] - <span class="number">0</span>s <span class="number">80</span>us/sample - loss: <span class="number">0.5071</span> - AUC: <span class="number">0.8349</span> - val_loss: <span class="number">0.5816</span> - val_AUC: <span class="number">0.7605</span></span><br></pre></td></tr></table></figure><h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><p>我们首先评估一下模型在训练集和验证集上的效果。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">def plot_metric(<span class="keyword">history</span>, metric):</span><br><span class="line">    train_metrics = <span class="keyword">history</span>.<span class="keyword">history</span>[metric]</span><br><span class="line">    val_metrics = <span class="keyword">history</span>.<span class="keyword">history</span>[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p>查看训练过程中随着迭代次数增加，误差的减少：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">plot_metric</span><span class="params">(history,<span class="string">"loss"</span>)</span></span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-1-Loss%E6%9B%B2%E7%BA%BF.jpg"></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">plot_metric</span><span class="params">(history,<span class="string">"AUC"</span>)</span></span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-1-AUC%E6%9B%B2%E7%BA%BF.jpg"></p><p>我们再看一下模型在测试集上的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(x = x_test,y = y_test)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">6</span>/<span class="number">6</span> [==============================] - <span class="number">0</span>s <span class="number">2</span>ms/step - loss: <span class="number">0.5665</span> - auc: <span class="number">0.7485</span></span><br><span class="line">[<span class="number">0.5664713382720947</span>, <span class="number">0.7484948039054871</span>]</span><br></pre></td></tr></table></figure><h4 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测概率</span></span><br><span class="line">model.predict(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0.26501188</span>],</span><br><span class="line">       [<span class="number">0.40970832</span>],</span><br><span class="line">       [<span class="number">0.44285864</span>],</span><br><span class="line">       [<span class="number">0.78408605</span>],</span><br><span class="line">       [<span class="number">0.47650957</span>],</span><br><span class="line">       [<span class="number">0.43849158</span>],</span><br><span class="line">       [<span class="number">0.27426785</span>],</span><br><span class="line">       [<span class="number">0.5962582</span> ],</span><br><span class="line">       [<span class="number">0.59476686</span>],</span><br><span class="line">       [<span class="number">0.17882936</span>]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测类别</span></span><br><span class="line">model.predict_classes(x_test[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>]], dtype=<span class="built_in">int</span>32)</span><br></pre></td></tr></table></figure><h4 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h4><p>保存模型有两种方式，推荐TensorFlow原生方式进行保存。</p><ul><li>使用Keras方式保存模型，仅仅适合使用Python环境恢复模型</li><li>使用TensorFlow原生方式保存，可以跨平台进行模型部署</li></ul><h5 id="Keras方式保存"><a href="#Keras方式保存" class="headerlink" title="Keras方式保存"></a>Keras方式保存</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构及权重</span></span><br><span class="line">model.save(<span class="string">'./data/keras_model.h5'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除现有模型</span></span><br><span class="line"><span class="keyword">del</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># identical to the previous one</span></span><br><span class="line">model = models.load_model(<span class="string">'./data/keras_model.h5'</span>)</span><br><span class="line">model.evaluate(x_test,y_test)</span><br><span class="line"><span class="comment"># 输出如下</span></span><br><span class="line"><span class="comment"># [0.5191367897907448, 0.8122605]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构</span></span><br><span class="line">json_str = model.to_json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br><span class="line"><span class="comment">#保存模型权重</span></span><br><span class="line">model.save_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复模型结构</span></span><br><span class="line">model_json = models.model_from_json(json_str)</span><br><span class="line">model_json.compile(</span><br><span class="line">        optimizer=<span class="string">'adam'</span>,</span><br><span class="line">        loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">        metrics=[<span class="string">'AUC'</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">model_json.load_weights(<span class="string">'./data/keras_model_weight.h5'</span>)</span><br><span class="line">model_json.evaluate(x_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出如下</span></span><br><span class="line"><span class="comment"># [0.5191367897907448, 0.8122605]</span></span><br></pre></td></tr></table></figure><h5 id="TensorFlow原生方式保存"><a href="#TensorFlow原生方式保存" class="headerlink" title="TensorFlow原生方式保存"></a>TensorFlow原生方式保存</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(x_test,y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出如下</span></span><br><span class="line"><span class="comment"># [0.5191365896656527, 0.8122605]</span></span><br></pre></td></tr></table></figure><h3 id="图片数据建模流程"><a href="#图片数据建模流程" class="headerlink" title="图片数据建模流程"></a>图片数据建模流程</h3><h4 id="准备数据-1"><a href="#准备数据-1" class="headerlink" title="准备数据"></a>准备数据</h4><p>cifar2数据集为cifar10数据集的子集，只包括前两种类别airplane和automobile。训练集有airplane和automobile图片各5000张，测试集有airplane和automobile图片各1000张。cifar2任务的目标是训练一个模型来对飞机airplane和机动车automobile两种图片进行分类。</p><p>我们准备的Cifar2数据集的文件结构如下所示。</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/cifar2.jpg"></p><p>在tensorflow中准备图片数据的常用方案有两种：</p><ul><li>使用 <code>tf.keras</code> 中的 <code>ImageDataGenerator</code> 工具构建图片数据生成器，这种更为简单，其使用范例可以参考以下文章：<a href="https://mp.weixin.qq.com/s?__biz=MzU3OTQzNTU2OA==&amp;mid=2247484795&amp;idx=1&amp;sn=16947726702b87ee535aef0d6ae2db30&amp;chksm=fd676824ca10e1321e77c5fa44339c0a79442cd8d7fbcc58697be166a4b0f990306848724692&amp;mpshare=1&amp;scene=1&amp;srcid=1227ARPw2Ir8nVC4B84CjcIx&amp;sharer_sharetime=1609043128020&amp;sharer_shareid=808295d573831eb57288f1fc0ad3ac69&amp;key=a58ea5adca8c8f06e4a7b7a15ed218f88cbee52ab3ee0fca3f2dd3f0797a36a6de26f8e75bd4787ddf97195c3959d94fe5060be0d3f9f6cd1eba11c0ad1ee37709088084d70034bd03efd43dacc32acd45a231c8359dd84ad73c28b11a9dc50556486b6e1e1ab89ad11da9621e5cdd858fcb53d91037d5116d638d12fced85b3&amp;ascene=0&amp;uin=MTYzMDEzMjAxMg%3D%3D&amp;devicetype=iMac+MacBookAir7%2C2+OSX+OSX+10.14.6+build(18G6032" target="_blank" rel="external nofollow noopener noreferrer">Keras图像数据预处理范例——Cifar2图片分类</a>&amp;version=11020113&amp;lang=zh_CN&amp;exportkey=A8nc9Ve3hcMzsggW3DOY8mU%3D&amp;pass_ticket=JOjUjT6HXslkPfqXrPY1oG3qVEXbIIc1IAKdh8xjlrGyB8OtZ8JjRan45%2Ff%2Bknjb&amp;wx_header=0)</li><li>使用 <code>tf.data.Dataset</code> 搭配 <code>tf.image</code> 中的一些图片处理方法构建数据管道，这种方法是TensorFlow的原生方法，更加灵活，使用得当的话也可以获得更好的性能。</li></ul><p>我们此处介绍第二种方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets,layers,models</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = tf.constant(<span class="number">1</span>,tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*automobile.*"</span>) \</span><br><span class="line">            <span class="keyword">else</span> tf.constant(<span class="number">0</span>,tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment"># 注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)/<span class="number">255.0</span></span><br><span class="line">    <span class="keyword">return</span>(img, label)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用并行化预处理 num_parallel_calls 和预存数据prefetch来提升性能</span></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)  </span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/test/*/*.jpg"</span>) \</span><br><span class="line">           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><p>查看部分样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看部分样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds_train.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-2-图片预览.jpg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape,y.shape)</span><br><span class="line"><span class="comment"># 输出 (100, 32, 32, 3) (100,)</span></span><br></pre></td></tr></table></figure><h4 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h4><p>使用Keras接口有以下3种方式构建模型：</p><ul><li>使用Sequential按层顺序构建模型</li><li>使用函数式API构建任意结构模型</li><li>继承Model基类构建自定义模型</li></ul><p>此处选择使用函数式API构建模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session() <span class="comment">#清空会话</span></span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=(<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>))</span><br><span class="line">x = layers.Conv2D(<span class="number">32</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>))(inputs)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>))(x)</span><br><span class="line">x = layers.MaxPool2D()(x)</span><br><span class="line">x = layers.Dropout(rate=<span class="number">0.1</span>)(x)</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">x = layers.Dense(<span class="number">32</span>,activation=<span class="string">'relu'</span>)(x)</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 32, 32, 3)]       0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d (Conv2D)              (None, 30, 30, 32)        896       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling2d_</span>1 (MaxPooling2 (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dropout (Dropout)            (None, 5, 5, 64)          0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 1600)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 32)                51232     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 1)                 33        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,425</span><br><span class="line">Trainable params: 103,425</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h4><p>训练模型通常有3种方法：</p><ul><li>内置fit方法</li><li>内置train_on_batch方法</li><li>自定义训练循环</li></ul><p>此处我们选择最常用也最简单的内置fit方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.compile(</span><br><span class="line">        optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">        loss=tf.keras.losses.binary_crossentropy,</span><br><span class="line">        metrics=[<span class="string">"accuracy"</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs= <span class="number">10</span>,validation_data=ds_test,</span><br><span class="line">                    callbacks = [tensorboard_callback],workers = <span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">100</span> steps, validate <span class="keyword">for</span> <span class="number">20</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">16</span>s <span class="number">156</span>ms/step - loss: <span class="number">0.4830</span> - accuracy: <span class="number">0.7697</span> - val_loss: <span class="number">0.3396</span> - val_accuracy: <span class="number">0.8475</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">14</span>s <span class="number">142</span>ms/step - loss: <span class="number">0.3437</span> - accuracy: <span class="number">0.8469</span> - val_loss: <span class="number">0.2997</span> - val_accuracy: <span class="number">0.8680</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">13</span>s <span class="number">131</span>ms/step - loss: <span class="number">0.2871</span> - accuracy: <span class="number">0.8777</span> - val_loss: <span class="number">0.2390</span> - val_accuracy: <span class="number">0.9015</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">12</span>s <span class="number">117</span>ms/step - loss: <span class="number">0.2410</span> - accuracy: <span class="number">0.9040</span> - val_loss: <span class="number">0.2005</span> - val_accuracy: <span class="number">0.9195</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">13</span>s <span class="number">130</span>ms/step - loss: <span class="number">0.1992</span> - accuracy: <span class="number">0.9213</span> - val_loss: <span class="number">0.1949</span> - val_accuracy: <span class="number">0.9180</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">14</span>s <span class="number">136</span>ms/step - loss: <span class="number">0.1737</span> - accuracy: <span class="number">0.9323</span> - val_loss: <span class="number">0.1723</span> - val_accuracy: <span class="number">0.9275</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">14</span>s <span class="number">139</span>ms/step - loss: <span class="number">0.1531</span> - accuracy: <span class="number">0.9412</span> - val_loss: <span class="number">0.1670</span> - val_accuracy: <span class="number">0.9310</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">13</span>s <span class="number">134</span>ms/step - loss: <span class="number">0.1299</span> - accuracy: <span class="number">0.9525</span> - val_loss: <span class="number">0.1553</span> - val_accuracy: <span class="number">0.9340</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">14</span>s <span class="number">137</span>ms/step - loss: <span class="number">0.1158</span> - accuracy: <span class="number">0.9556</span> - val_loss: <span class="number">0.1581</span> - val_accuracy: <span class="number">0.9340</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">100</span>/<span class="number">100</span> [==============================] - <span class="number">14</span>s <span class="number">142</span>ms/step - loss: <span class="number">0.1006</span> - accuracy: <span class="number">0.9617</span> - val_loss: <span class="number">0.1614</span> - val_accuracy: <span class="number">0.9345</span></span><br></pre></td></tr></table></figure><h4 id="评估模型-1"><a href="#评估模型-1" class="headerlink" title="评估模型"></a>评估模型</h4><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%load_ext tensorboard</span><br><span class="line"><span class="comment">#%tensorboard --logdir ./data/keras_model</span></span><br><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.<span class="keyword">list</span>() </span><br><span class="line"><span class="comment">#在tensorboard中查看模型</span></span><br><span class="line">notebook.start(<span class="string">"--logdir ./data/keras_model"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-2-tensorboard.jpg"></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import pandas <span class="keyword">as</span> pd </span><br><span class="line">dfhistory = pd.DataFrame(<span class="keyword">history</span>.<span class="keyword">history</span>)</span><br><span class="line">dfhistory.<span class="built_in">index</span> = <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(dfhistory) + <span class="number">1</span>)</span><br><span class="line">dfhistory.<span class="built_in">index</span>.name = <span class="string">'epoch'</span></span><br><span class="line"></span><br><span class="line">dfhistory</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-2-dfhistory.jpg"></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">def plot_metric(<span class="keyword">history</span>, metric):</span><br><span class="line">    train_metrics = <span class="keyword">history</span>.<span class="keyword">history</span>[metric]</span><br><span class="line">    val_metrics = <span class="keyword">history</span>.<span class="keyword">history</span>[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">plot_metric</span><span class="params">(history,<span class="string">"loss"</span>)</span></span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-2-Loss曲线.jpg"></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">plot_metric</span><span class="params">(history,<span class="string">"accuracy"</span>)</span></span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-2-Accuracy曲线.jpg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以使用evaluate对数据进行评估</span></span><br><span class="line">val_loss,val_accuracy = model.evaluate(ds_test,workers=<span class="number">4</span>)</span><br><span class="line">print(val_loss,val_accuracy)</span><br><span class="line"><span class="comment"># 0.16139143370091916 0.9345</span></span><br></pre></td></tr></table></figure><h4 id="使用模型-1"><a href="#使用模型-1" class="headerlink" title="使用模型"></a>使用模型</h4><ul><li>可以使用model.predict(ds_test)进行预测。</li><li>可以使用model.predict_on_batch(x_test)对一个批量进行预测。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">9.9996173e-01</span>],</span><br><span class="line">       [<span class="number">9.5104784e-01</span>],</span><br><span class="line">       [<span class="number">2.8648047e-04</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">1.1484033e-03</span>],</span><br><span class="line">       [<span class="number">3.5589080e-02</span>],</span><br><span class="line">       [<span class="number">9.8537153e-01</span>]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_test.take(<span class="number">1</span>):</span><br><span class="line">    print(model.predict_on_batch(x[<span class="number">0</span>:<span class="number">20</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">8.8849956e-01</span>]</span><br><span class="line"> [<span class="number">5.9769617e-04</span>]</span><br><span class="line"> [<span class="number">9.7809315e-01</span>]</span><br><span class="line"> [<span class="number">9.9903524e-01</span>]</span><br><span class="line"> [<span class="number">9.1890675e-01</span>]</span><br><span class="line"> [<span class="number">1.0246566e-02</span>]</span><br><span class="line"> [<span class="number">3.8106637e-03</span>]</span><br><span class="line"> [<span class="number">8.3489519e-01</span>]</span><br><span class="line"> [<span class="number">4.3317820e-03</span>]</span><br><span class="line"> [<span class="number">9.9998546e-01</span>]</span><br><span class="line"> [<span class="number">2.1413717e-02</span>]</span><br><span class="line"> [<span class="number">2.5603941e-04</span>]</span><br><span class="line"> [<span class="number">1.8045523e-04</span>]</span><br><span class="line"> [<span class="number">9.9734712e-01</span>]</span><br><span class="line"> [<span class="number">2.3647046e-01</span>]</span><br><span class="line"> [<span class="number">8.7527412e-01</span>]</span><br><span class="line"> [<span class="number">6.2612216e-03</span>]</span><br><span class="line"> [<span class="number">1.6598338e-01</span>]</span><br><span class="line"> [<span class="number">5.2631170e-01</span>]</span><br><span class="line"> [<span class="number">9.9995863e-01</span>]]</span><br></pre></td></tr></table></figure><h4 id="保存模型-1"><a href="#保存模型-1" class="headerlink" title="保存模型"></a>保存模型</h4><p>推荐使用TensorFlow原生方式保存模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存权重，该方式仅仅保存权重张量</span></span><br><span class="line">model.save_weights(<span class="string">'./data/tf_model_weights.ckpt'</span>,save_format = <span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.evaluate(ds_test)</span><br><span class="line"><span class="comment"># [0.16139124035835267, 0.9345]</span></span><br></pre></td></tr></table></figure><h3 id="文本数据建模流程"><a href="#文本数据建模流程" class="headerlink" title="文本数据建模流程"></a>文本数据建模流程</h3><h4 id="准备数据-2"><a href="#准备数据-2" class="headerlink" title="准备数据"></a>准备数据</h4><p>imdb数据集的目标是<strong>根据电影评论的文本内容预测评论的情感标签</strong>。训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。</p><p>在tensorflow中完成文本数据预处理的常用方案有两种：</p><ul><li>第一种是利用 <code>tf.keras.preprocessing</code> 中的Tokenizer词典构建工具和 <code>tf.keras.utils.Sequence</code> 构建文本数据生成器管道，这种方法较为复杂，其使用范例可以参考<a href="https://zhuanlan.zhihu.com/p/67697840" target="_blank" rel="external nofollow noopener noreferrer">以下文章</a></li><li>第二种是使用 <code>tf.data.Dataset</code> 搭配 <code>.keras.layers.experimental.preprocessing.TextVectorization</code>预处理层，这种方法为TensorFlow原生方式，相对也更加简单一些。</li></ul><p>我们此处介绍第二种方法。</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/电影评论.jpg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,preprocessing,optimizers,losses,metrics</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="keyword">import</span> TextVectorization</span><br><span class="line"><span class="keyword">import</span> re,string</span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">"./data/imdb/train.csv"</span></span><br><span class="line">test_data_path =  <span class="string">"./data/imdb/test.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># 仅考虑最高频的10000个词</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># 每个样本保留200个词的长度</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    arr = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[<span class="number">0</span>]),tf.int32),axis = <span class="number">0</span>)</span><br><span class="line">    text = tf.expand_dims(arr[<span class="number">1</span>],axis = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (text,label)</span><br><span class="line"></span><br><span class="line">ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \</span><br><span class="line">   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text)</span>:</span></span><br><span class="line">    lowercase = tf.strings.lower(text)</span><br><span class="line">    stripped_html = tf.strings.regex_replace(lowercase, <span class="string">'&lt;br /&gt;'</span>, <span class="string">' '</span>)</span><br><span class="line">    cleaned_punctuation = tf.strings.regex_replace(stripped_html,</span><br><span class="line">         <span class="string">'[%s]'</span> % re.escape(string.punctuation),<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">return</span> cleaned_punctuation</span><br><span class="line"></span><br><span class="line">vectorize_layer = TextVectorization(</span><br><span class="line">    standardize=clean_text,</span><br><span class="line">    split = <span class="string">'whitespace'</span>,</span><br><span class="line">    max_tokens=MAX_WORDS<span class="number">-1</span>, <span class="comment">#有一个留给占位符</span></span><br><span class="line">    output_mode=<span class="string">'int'</span>,</span><br><span class="line">    output_sequence_length=MAX_LEN)</span><br><span class="line"></span><br><span class="line">ds_text = ds_train_raw.map(<span class="keyword">lambda</span> text,label: text)</span><br><span class="line">vectorize_layer.adapt(ds_text)</span><br><span class="line">print(vectorize_layer.get_vocabulary()[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单词编码</span></span><br><span class="line">ds_train = ds_train_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line">ds_test = ds_test_raw.map(<span class="keyword">lambda</span> text,label:(vectorize_layer(text),label)) \</span><br><span class="line">    .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="name">b</span><span class="symbol">'the</span>', b<span class="symbol">'and</span>', b<span class="symbol">'a</span>', b<span class="symbol">'of</span>', b<span class="symbol">'to</span>', b<span class="symbol">'is</span>', b<span class="symbol">'in</span>', b<span class="symbol">'it</span>', b<span class="symbol">'i</span>', b<span class="symbol">'this</span>', b<span class="symbol">'that</span>', b<span class="symbol">'was</span>', b<span class="symbol">'as</span>', b<span class="symbol">'for</span>', b<span class="symbol">'with</span>', b<span class="symbol">'movie</span>', b<span class="symbol">'but</span>', b<span class="symbol">'film</span>', b<span class="symbol">'on</span>', b<span class="symbol">'not</span>', b<span class="symbol">'you</span>', b<span class="symbol">'his</span>', b<span class="symbol">'are</span>', b<span class="symbol">'have</span>', b<span class="symbol">'be</span>', b<span class="symbol">'he</span>', b<span class="symbol">'one</span>', b<span class="symbol">'its</span>', b<span class="symbol">'at</span>', b<span class="symbol">'all</span>', b<span class="symbol">'by</span>', b<span class="symbol">'an</span>', b<span class="symbol">'they</span>', b<span class="symbol">'from</span>', b<span class="symbol">'who</span>', b<span class="symbol">'so</span>', b<span class="symbol">'like</span>', b<span class="symbol">'her</span>', b<span class="symbol">'just</span>', b<span class="symbol">'or</span>', b<span class="symbol">'about</span>', b<span class="symbol">'has</span>', b<span class="symbol">'if</span>', b<span class="symbol">'out</span>', b<span class="symbol">'some</span>', b<span class="symbol">'there</span>', b<span class="symbol">'what</span>', b<span class="symbol">'good</span>', b<span class="symbol">'more</span>', b<span class="symbol">'when</span>', b<span class="symbol">'very</span>', b<span class="symbol">'she</span>', b<span class="symbol">'even</span>', b<span class="symbol">'my</span>', b<span class="symbol">'no</span>', b<span class="symbol">'would</span>', b<span class="symbol">'up</span>', b<span class="symbol">'time</span>', b<span class="symbol">'only</span>', b<span class="symbol">'which</span>', b<span class="symbol">'story</span>', b<span class="symbol">'really</span>', b<span class="symbol">'their</span>', b<span class="symbol">'were</span>', b<span class="symbol">'had</span>', b<span class="symbol">'see</span>', b<span class="symbol">'can</span>', b<span class="symbol">'me</span>', b<span class="symbol">'than</span>', b<span class="symbol">'we</span>', b<span class="symbol">'much</span>', b<span class="symbol">'well</span>', b<span class="symbol">'get</span>', b<span class="symbol">'been</span>', b<span class="symbol">'will</span>', b<span class="symbol">'into</span>', b<span class="symbol">'people</span>', b<span class="symbol">'also</span>', b<span class="symbol">'other</span>', b<span class="symbol">'do</span>', b<span class="symbol">'bad</span>', b<span class="symbol">'because</span>', b<span class="symbol">'great</span>', b<span class="symbol">'first</span>', b<span class="symbol">'how</span>', b<span class="symbol">'him</span>', b<span class="symbol">'most</span>', b<span class="symbol">'dont</span>', b<span class="symbol">'made</span>', b<span class="symbol">'then</span>', b<span class="symbol">'them</span>', b<span class="symbol">'films</span>', b<span class="symbol">'movies</span>', b<span class="symbol">'way</span>', b<span class="symbol">'make</span>', b<span class="symbol">'could</span>', b<span class="symbol">'too</span>', b<span class="symbol">'any</span>', b<span class="symbol">'after</span>', b<span class="symbol">'characters</span>']</span><br></pre></td></tr></table></figure><h4 id="定义模型-2"><a href="#定义模型-2" class="headerlink" title="定义模型"></a>定义模型</h4><p>使用Keras接口有以下3种方式构建模型：</p><ul><li>使用Sequential按层顺序构建模型</li><li>使用函数式API构建任意结构模型</li><li>继承Model基类构建自定义模型</li></ul><p>此处选择使用继承Model基类构建自定义模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 演示自定义模型范例，实际上应该优先使用Sequential或者函数式API</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CnnModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CnnModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN)</span><br><span class="line">        self.conv_1 = layers.Conv1D(<span class="number">16</span>, kernel_size= <span class="number">5</span>,name = <span class="string">"conv_1"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_1 = layers.MaxPool1D(name = <span class="string">"pool_1"</span>)</span><br><span class="line">        self.conv_2 = layers.Conv1D(<span class="number">128</span>, kernel_size=<span class="number">2</span>,name = <span class="string">"conv_2"</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.pool_2 = layers.MaxPool1D(name = <span class="string">"pool_2"</span>)</span><br><span class="line">        self.flatten = layers.Flatten()</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(CnnModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.conv_1(x)</span><br><span class="line">        x = self.pool_1(x)</span><br><span class="line">        x = self.conv_2(x)</span><br><span class="line">        x = self.pool_2(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用于显示Output Shape</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summary</span><span class="params">(self)</span>:</span></span><br><span class="line">        x_input = layers.Input(shape = MAX_LEN)</span><br><span class="line">        output = self.call(x_input)</span><br><span class="line">        model = tf.keras.Model(inputs = x_input,outputs = output)</span><br><span class="line">        model.summary()</span><br><span class="line">    </span><br><span class="line">model = CnnModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,MAX_LEN))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, 200)]             0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">embedding (Embedding)        (None, 200, 7)            70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_1 (Conv1D)              (None, 196, 16)           576       </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_1 (MaxPooling1D)        (None, 98, 16)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv_2 (Conv1D)              (None, 97, 128)           4224      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">pool_2 (MaxPooling1D)        (None, 48, 128)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 6144)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 1)                 6145      </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 80,945</span><br><span class="line">Trainable params: 80,945</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="训练模型-2"><a href="#训练模型-2" class="headerlink" title="训练模型"></a>训练模型</h4><p>训练模型通常有3种方法：</p><ul><li>内置fit方法</li><li>内置train_on_batch方法</li><li>自定义训练循环。</li></ul><p>此处我们通过自定义训练循环训练模型。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 打印时间分割线</span><br><span class="line">@tf<span class="function">.<span class="keyword">function</span></span></span><br><span class="line">def printbar():</span><br><span class="line">    today_ts = <span class="keyword">tf</span>.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">    </span><br><span class="line">    hour = <span class="keyword">tf</span>.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,<span class="keyword">tf</span>.int32)%<span class="keyword">tf</span>.constant(<span class="number">24</span>)</span><br><span class="line">    minite = <span class="keyword">tf</span>.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,<span class="keyword">tf</span>.int32)</span><br><span class="line">    second = <span class="keyword">tf</span>.cast(<span class="keyword">tf</span>.<span class="built_in">floor</span>(today_ts%<span class="number">60</span>),<span class="keyword">tf</span>.int32)</span><br><span class="line">    </span><br><span class="line">    def timeformat(<span class="keyword">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">tf</span>.strings.length(<span class="keyword">tf</span>.strings.format(<span class="string">"&#123;&#125;"</span>,<span class="keyword">m</span>))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(<span class="keyword">tf</span>.strings.format(<span class="string">"0&#123;&#125;"</span>,<span class="keyword">m</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(<span class="keyword">tf</span>.strings.format(<span class="string">"&#123;&#125;"</span>,<span class="keyword">m</span>))</span><br><span class="line">    </span><br><span class="line">    timestring = <span class="keyword">tf</span>.strings.<span class="keyword">join</span>([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    <span class="keyword">tf</span>.<span class="keyword">print</span>(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features,training = <span class="literal">False</span>)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#此处logs模板需要根据metric具体情况修改</span></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,epochs = <span class="number">6</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">08</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">0.442317516</span>,Accuracy:<span class="number">0.7695</span>,Valid Loss:<span class="number">0.323672801</span>,Valid Accuracy:<span class="number">0.8614</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">20</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">0.245737702</span>,Accuracy:<span class="number">0.90215</span>,Valid Loss:<span class="number">0.356488883</span>,Valid Accuracy:<span class="number">0.8554</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">32</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">0.17360799</span>,Accuracy:<span class="number">0.93455</span>,Valid Loss:<span class="number">0.361132562</span>,Valid Accuracy:<span class="number">0.8674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.113476314</span>,Accuracy:<span class="number">0.95975</span>,Valid Loss:<span class="number">0.483677238</span>,Valid Accuracy:<span class="number">0.856</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">54</span>:<span class="number">57</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.0698405355</span>,Accuracy:<span class="number">0.9768</span>,Valid Loss:<span class="number">0.607856631</span>,Valid Accuracy:<span class="number">0.857</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">55</span>:<span class="number">15</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.0366807655</span>,Accuracy:<span class="number">0.98825</span>,Valid Loss:<span class="number">0.745884955</span>,Valid Accuracy:<span class="number">0.854</span></span><br></pre></td></tr></table></figure><h4 id="评估模型-2"><a href="#评估模型-2" class="headerlink" title="评估模型"></a>评估模型</h4><p>通过自定义训练循环训练的模型没有经过编译，无法直接使用model.evaluate(ds_valid)方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_model</span><span class="params">(model,ds_valid)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">         valid_step(model,features,labels)</span><br><span class="line">    logs = <span class="string">'Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span> </span><br><span class="line">    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))</span><br><span class="line">    </span><br><span class="line">    valid_loss.reset_states()</span><br><span class="line">    train_metric.reset_states()</span><br><span class="line">    valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">evaluate_model(model,ds_test)</span><br><span class="line"><span class="comment"># Valid Loss:0.745884418,Valid Accuracy:0.854</span></span><br></pre></td></tr></table></figure><h4 id="使用模型-2"><a href="#使用模型-2" class="headerlink" title="使用模型"></a>使用模型</h4><p>可以使用以下方法:</p><ul><li>model.predict(ds_test)</li><li>model(x_test)</li><li>model.call(x_test)</li><li>model.predict_on_batch(x_test)</li></ul><p>推荐优先使用 <code>model.predict(ds_test)</code> 方法，既可以对Dataset，也可以对Tensor使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(ds_test)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight leaf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for x_test,_ in ds_test.take(1):</span><br><span class="line">    print(model(x_test))</span><br><span class="line">    #以下方法等价：</span><br><span class="line">    <span class="function"><span class="keyword">#</span><span class="title">print</span><span class="params">(<span class="variable">model</span>.<span class="variable">call</span>(<span class="variable">x_test</span>)</span></span>)</span><br><span class="line">    <span class="function"><span class="keyword">#</span><span class="title">print</span><span class="params">(<span class="variable">model</span>.<span class="variable">predict_on_batch</span>(<span class="variable">x_test</span>)</span></span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">7.8648227e-01</span>]</span><br><span class="line"> [<span class="number">9.9999011e-01</span>]</span><br><span class="line"> [<span class="number">9.9944776e-01</span>]</span><br><span class="line"> [<span class="number">3.7153201e-09</span>]</span><br><span class="line"> [<span class="number">9.4462049e-01</span>]</span><br><span class="line"> [<span class="number">2.3522753e-04</span>]</span><br><span class="line"> [<span class="number">1.2044354e-04</span>]</span><br><span class="line"> [<span class="number">9.3752089e-07</span>]</span><br><span class="line"> [<span class="number">9.9996352e-01</span>]</span><br><span class="line"> [<span class="number">9.3435925e-01</span>]</span><br><span class="line"> [<span class="number">9.8746723e-01</span>]</span><br><span class="line"> [<span class="number">9.9908626e-01</span>]</span><br><span class="line"> [<span class="number">4.1563155e-08</span>]</span><br><span class="line"> [<span class="number">4.1808244e-03</span>]</span><br><span class="line"> [<span class="number">8.0184749e-05</span>]</span><br><span class="line"> [<span class="number">8.3910513e-01</span>]</span><br><span class="line"> [<span class="number">3.5167937e-05</span>]</span><br><span class="line"> [<span class="number">7.2113985e-01</span>]</span><br><span class="line"> [<span class="number">4.5228912e-03</span>]</span><br><span class="line"> [<span class="number">9.9942589e-01</span>]], shape=(<span class="number">20</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><h4 id="保存模型-2"><a href="#保存模型-2" class="headerlink" title="保存模型"></a>保存模型</h4><p>推荐使用TensorFlow原生方式保存模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, save_format=<span class="string">"tf"</span>)</span><br><span class="line">print(<span class="string">'export saved model.'</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">'./data/tf_model_savedmodel'</span>)</span><br><span class="line">model_loaded.predict(ds_test)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[<span class="number">0.7864823</span> ],</span><br><span class="line">       [<span class="number">0.9999901</span> ],</span><br><span class="line">       [<span class="number">0.99944776</span>],</span><br><span class="line">       ...,</span><br><span class="line">       [<span class="number">0.8498302</span> ],</span><br><span class="line">       [<span class="number">0.13382755</span>],</span><br><span class="line">       [<span class="number">1.</span>        ]], dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><h3 id="时间序列数据建模流程"><a href="#时间序列数据建模流程" class="headerlink" title="时间序列数据建模流程"></a>时间序列数据建模流程</h3><p>本小节将利用TensorFlow2.0建立时间序列RNN模型，对国内的新冠肺炎疫情结束时间进行预测。</p><h4 id="准备数据-3"><a href="#准备数据-3" class="headerlink" title="准备数据"></a>准备数据</h4><p>本文的数据集取自tushare，数据集在本项目的 data 目录下。</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-4-新增人数.png"></p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">from tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,callbacks </span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"./data/covid-19.csv"</span>,sep = <span class="string">"\t"</span>)</span><br><span class="line">df.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-4-累积曲线.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dfdata = df.set_index(<span class="string">"date"</span>)</span><br><span class="line">dfdiff = dfdata.diff(periods=<span class="number">1</span>).dropna()</span><br><span class="line">dfdiff = dfdiff.reset_index(<span class="string">"date"</span>)</span><br><span class="line"></span><br><span class="line">dfdiff.plot(x = <span class="string">"date"</span>,y = [<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>],figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.xticks(rotation=<span class="number">60</span>)</span><br><span class="line">dfdiff = dfdiff.drop(<span class="string">"date"</span>,axis = <span class="number">1</span>).astype(<span class="string">"float32"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-4-新增曲线.png"></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 用某日前8天窗口数据作为输入预测该日数据</span></span><br><span class="line"><span class="type">WINDOW_SIZE</span> = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="title">def</span> batch_dataset(dataset):</span><br><span class="line">    dataset_batched = dataset.batch(<span class="type">WINDOW_SIZE</span>,drop_remainder=<span class="type">True</span>)</span><br><span class="line">    return dataset_batched</span><br><span class="line"></span><br><span class="line"><span class="title">ds_data</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.from_tensor_slices(<span class="title">tf</span>.<span class="title">constant</span>(<span class="title">dfdiff</span>.<span class="title">values</span>,<span class="title">dtype</span> = <span class="title">tf</span>.<span class="title">float32</span>)) \</span></span><br><span class="line">   .window(<span class="type">WINDOW_SIZE</span>,shift=<span class="number">1</span>).flat_map(batch_dataset)</span><br><span class="line"></span><br><span class="line"><span class="title">ds_label</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.from_tensor_slices(</span></span><br><span class="line"><span class="class">    <span class="title">tf</span>.<span class="title">constant</span>(<span class="title">dfdiff</span>.<span class="title">values</span>[<span class="type">WINDOW_SIZE</span>:],<span class="title">dtype</span> = <span class="title">tf</span>.<span class="title">float32</span>))</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 数据较小，可以将全部训练数据放入到一个batch中，提升性能</span></span><br><span class="line"><span class="title">ds_train</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.zip((<span class="title">ds_data</span>,<span class="title">ds_label</span>)).batch(38).cache()</span></span><br></pre></td></tr></table></figure><h4 id="定义模型-3"><a href="#定义模型-3" class="headerlink" title="定义模型"></a>定义模型</h4><p>使用Keras接口有以下3种方式构建模型：</p><ul><li>使用Sequential按层顺序构建模型</li><li>使用函数式API构建任意结构模型</li><li>继承Model基类构建自定义模型</li></ul><p>此处选择使用函数式API构建任意结构模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(Block, self).__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x_input,x)</span>:</span></span><br><span class="line">        x_out = tf.maximum((<span class="number">1</span>+x)*x_input[:,<span class="number">-1</span>,:],<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Block, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">x_input = layers.Input(shape = (<span class="literal">None</span>,<span class="number">3</span>),dtype = tf.float32)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x_input)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,return_sequences = <span class="literal">True</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.LSTM(<span class="number">3</span>,input_shape=(<span class="literal">None</span>,<span class="number">3</span>))(x)</span><br><span class="line">x = layers.Dense(<span class="number">3</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 考虑到新增确诊，新增治愈，新增死亡人数数据不可能小于0，设计如下结构</span></span><br><span class="line"><span class="comment"># x = tf.maximum((1+x)*x_input[:,-1,:],0.0)</span></span><br><span class="line">x = Block()(x_input,x)</span><br><span class="line">model = models.Model(inputs = [x_input],outputs = [x])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         [(None, None, 3)]         0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm (LSTM)                  (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_1 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_2 (LSTM)                (None, None, 3)           84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">lstm_3 (LSTM)                (None, 3)                 84        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 3)                 12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">block (Block)                (None, 3)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 348</span><br><span class="line">Trainable params: 348</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="训练模型-3"><a href="#训练模型-3" class="headerlink" title="训练模型"></a>训练模型</h4><p>训练模型通常有3种方法：</p><ul><li>内置fit方法</li><li>内置train_on_batch方法</li><li>自定义训练循环</li></ul><p>此处我们选择最常用也最简单的内置fit方法。</p><p>注：循环神经网络调试较为困难，需要设置多个不同的学习率多次尝试，以取得较好的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义损失函数，考虑平方差和预测目标的比值</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSPE</span><span class="params">(losses.Loss)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        err_percent = (y_true - y_pred)**<span class="number">2</span>/(tf.maximum(y_true**<span class="number">2</span>,<span class="number">1e-7</span>))</span><br><span class="line">        mean_err_percent = tf.reduce_mean(err_percent)</span><br><span class="line">        <span class="keyword">return</span> mean_err_percent</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></span><br><span class="line">        config = super(MSPE, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">model.compile(optimizer=optimizer,loss=MSPE(name = <span class="string">"MSPE"</span>))</span><br><span class="line"></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 如果loss在100个epoch后没有提升，学习率减半。</span></span><br><span class="line">lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=<span class="string">"loss"</span>,factor = <span class="number">0.5</span>, patience = <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 当loss在200个epoch后没有提升，则提前终止训练。</span></span><br><span class="line">stop_callback = tf.keras.callbacks.EarlyStopping(monitor = <span class="string">"loss"</span>, patience= <span class="number">200</span>)</span><br><span class="line">callbacks_list = [tb_callback,lr_callback,stop_callback]</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs=<span class="number">500</span>,callbacks = callbacks_list)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.4135</span> - lr: <span class="number">0.0100</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - ETA: <span class="number">0</span>s - loss: <span class="number">3.0553</span>WARNING:tensorflow:Method (on_train_batch_end) <span class="keyword">is</span> slow compared to the batch update (<span class="number">0.271260</span>). Check your callbacks.</span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">14</span>ms/step - loss: <span class="number">3.0553</span> - lr: <span class="number">0.0100</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">10</span>ms/step - loss: <span class="number">2.7220</span> - lr: <span class="number">0.0100</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">500</span></span><br><span class="line"><span class="number">1</span>/<span class="number">1</span> [==============================] - <span class="number">0</span>s <span class="number">10</span>ms/step - loss: <span class="number">2.3993</span> - lr: <span class="number">0.0100</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="评估模型-3"><a href="#评估模型-3" class="headerlink" title="评估模型"></a>评估模型</h4><p>评估模型一般要设置验证集或者测试集，由于此例数据较少，我们仅仅可视化损失函数在训练集上的迭代情况。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">def plot_metric(<span class="keyword">history</span>, metric):</span><br><span class="line">    train_metrics = <span class="keyword">history</span>.<span class="keyword">history</span>[metric]</span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric])</span><br><span class="line">    plt.show()</span><br><span class="line">plot_metric(<span class="keyword">history</span>,<span class="string">"loss"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/1-4-损失函数曲线.png"></p><h4 id="使用模型-3"><a href="#使用模型-3" class="headerlink" title="使用模型"></a>使用模型</h4><p>此处我们使用模型预测疫情结束时间，即 新增确诊病例为0 的时间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用dfresult记录现有数据以及此后预测的疫情数据</span></span><br><span class="line">dfresult = dfdiff[[<span class="string">"confirmed_num"</span>,<span class="string">"cured_num"</span>,<span class="string">"dead_num"</span>]].copy()</span><br><span class="line">dfresult.tail()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预测此后100天的新增走势,将其结果添加到dfresult中</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[<span class="number">-38</span>:,:],axis = <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),</span><br><span class="line">                columns = dfresult.columns)</span><br><span class="line">    dfresult = dfresult.append(dfpredict,ignore_index=<span class="literal">True</span>)</span><br><span class="line">dfresult.query(<span class="string">"confirmed_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第55天开始新增确诊降为0，第45天对应3月10日，也就是10天后，即预计3月20日新增确诊降为0</span></span><br><span class="line"><span class="comment"># 注：该预测偏乐观</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dfresult.query(<span class="string">"cured_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第164天开始新增治愈降为0，第45天对应3月10日，也就是大概4个月后，即7月10日左右全部治愈。</span></span><br><span class="line"><span class="comment"># 注: 该预测偏悲观，并且存在问题，如果将每天新增治愈人数加起来，将超过累计确诊人数。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dfresult.query(<span class="string">"dead_num==0"</span>).head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第60天开始，新增死亡降为0，第45天对应3月10日，也就是大概15天后，即20200325</span></span><br><span class="line"><span class="comment"># 该预测较为合理</span></span><br></pre></td></tr></table></figure><h4 id="保存模型-3"><a href="#保存模型-3" class="headerlink" title="保存模型"></a>保存模型</h4><p>推荐使用TensorFlow原生方式保存模型。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">'./data/tf_model_savedmodel'</span>, <span class="attribute">save_format</span>=<span class="string">"tf"</span>)</span><br><span class="line"><span class="builtin-name">print</span>(<span class="string">'export saved model.'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_loaded = tf.keras.models.load<span class="constructor">_model('.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">tf_model_savedmodel</span>',<span class="params">compile</span>=False)</span></span><br><span class="line">optimizer = tf.keras.optimizers.<span class="constructor">Adam(<span class="params">learning_rate</span>=0.001)</span></span><br><span class="line">model_loaded.compile(optimizer=optimizer,loss=<span class="constructor">MSPE(<span class="params">name</span> = <span class="string">"MSPE"</span>)</span>)</span><br><span class="line">model_loaded.predict(ds_train)</span><br></pre></td></tr></table></figure><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p>程序 = 数据结构+算法。<br>TensorFlow程序 = 张量数据结构 + 计算图算法语言</p><p><strong>张量</strong> 和 <strong>计算图</strong> 是 TensorFlow 的核心概念。</p><h3 id="张量数据结构"><a href="#张量数据结构" class="headerlink" title="张量数据结构"></a>张量数据结构</h3><p>Tensorflow的基本数据结构是张量Tensor。张量即多维数组，Tensorflow的张量和numpy中的array很类似。</p><p>从行为特性来看，有两种类型的张量：</p><ul><li>常量constant，常量的值在计算图中不可以被重新赋值</li><li>变量Variable，变量可以在计算图中用assign等算子重新赋值</li></ul><h4 id="常量张量"><a href="#常量张量" class="headerlink" title="常量张量"></a>常量张量</h4><p>张量的数据类型和 <code>numpy.array</code> 基本一一对应。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">i = tf.constant(<span class="number">1</span>) <span class="comment"># tf.int32 类型常量</span></span><br><span class="line">l = tf.constant(<span class="number">1</span>,dtype = tf.int64) <span class="comment"># tf.int64 类型常量</span></span><br><span class="line">f = tf.constant(<span class="number">1.23</span>) <span class="comment">#tf.float32 类型常量</span></span><br><span class="line">d = tf.constant(<span class="number">3.14</span>,dtype = tf.double) <span class="comment"># tf.double 类型常量</span></span><br><span class="line">s = tf.constant(<span class="string">"hello world"</span>) <span class="comment"># tf.string类型常量</span></span><br><span class="line">b = tf.constant(<span class="literal">True</span>) <span class="comment">#tf.bool类型常量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(tf.int64 == np.int64) </span><br><span class="line">print(tf.bool == np.bool)</span><br><span class="line">print(tf.double == np.float64)</span><br><span class="line">print(tf.string == np.unicode) <span class="comment"># tf.string类型和np.unicode类型不等价</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p>不同类型的数据可以用不同维度(rank)的张量来表示：</p><ul><li>标量为0维张量，向量为1维张量，矩阵为2维张量</li><li>彩色图像有rgb三个通道，可以表示为3维张量</li><li>视频还有时间维，可以表示为4维张量<br>可以简单地总结为：有几层中括号，就是多少维的张量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar = tf.constant(<span class="literal">True</span>)  <span class="comment">#标量，0维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(scalar))</span><br><span class="line">print(scalar.numpy().ndim)  <span class="comment"># tf.rank的作用和numpy的ndim方法相同</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>32)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">4.0</span>]) <span class="comment">#向量，1维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(vector))</span><br><span class="line">print(np.ndim(vector.numpy()))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=<span class="built_in">int</span>32)</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">matrix = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]]) <span class="comment">#矩阵, 2维张量</span></span><br><span class="line"></span><br><span class="line">print(tf.rank(matrix).numpy())</span><br><span class="line">print(np.ndim(matrix))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor3 = tf.constant([[[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],[[<span class="number">5.0</span>,<span class="number">6.0</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]]])  <span class="comment"># 3维张量</span></span><br><span class="line">print(tensor3)</span><br><span class="line">print(tf.rank(tensor3))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[[<span class="number">1.</span> <span class="number">2.</span>]</span><br><span class="line">  [<span class="number">3.</span> <span class="number">4.</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">5.</span> <span class="number">6.</span>]</span><br><span class="line">  [<span class="number">7.</span> <span class="number">8.</span>]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32)</span><br><span class="line">tf.Tensor(<span class="number">3</span>, shape=(), dtype=<span class="built_in">int</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor4 = tf.constant([[[[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>]],[[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">4.0</span>,<span class="number">4.0</span>]]],</span><br><span class="line">                        [[[<span class="number">5.0</span>,<span class="number">5.0</span>],[<span class="number">6.0</span>,<span class="number">6.0</span>]],[[<span class="number">7.0</span>,<span class="number">7.0</span>],[<span class="number">8.0</span>,<span class="number">8.0</span>]]]])  <span class="comment"># 4维张量</span></span><br><span class="line">print(tensor4)</span><br><span class="line">print(tf.rank(tensor4))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[[[<span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line">   [<span class="number">2.</span> <span class="number">2.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">3.</span> <span class="number">3.</span>]</span><br><span class="line">   [<span class="number">4.</span> <span class="number">4.</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> [[[<span class="number">5.</span> <span class="number">5.</span>]</span><br><span class="line">   [<span class="number">6.</span> <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">7.</span> <span class="number">7.</span>]</span><br><span class="line">   [<span class="number">8.</span> <span class="number">8.</span>]]]], shape=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32)</span><br><span class="line">tf.Tensor(<span class="number">4</span>, shape=(), dtype=<span class="built_in">int</span>32)</span><br></pre></td></tr></table></figure><ul><li>可以用<code>tf.cast</code>改变张量的数据类型</li><li>可以用<code>numpy</code>方法将tensorflow中的张量转化成numpy中的张量</li><li>可以用<code>shape</code>方法查看张量的尺寸</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h = tf.constant([<span class="number">123</span>,<span class="number">456</span>],dtype = tf.int32)</span><br><span class="line">f = tf.cast(h,tf.float32)</span><br><span class="line">print(h.dtype, f.dtype)</span><br></pre></td></tr></table></figure><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="string">dtype:</span> <span class="string">'int32'</span>&gt; &lt;<span class="string">dtype:</span> <span class="string">'float32'</span>&gt;</span><br></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = tf.constant(<span class="string">[[1.0,2.0],[3.0,4.0]]</span>)</span><br><span class="line"><span class="built_in">print</span>(y.numpy()) #转换成np.array</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="name">1.</span> <span class="number">2</span>.]</span><br><span class="line"> [<span class="name">3.</span> <span class="number">4</span>.]]</span><br><span class="line">(<span class="name">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">u = tf.constant(<span class="string">u"你好 世界"</span>)</span><br><span class="line">print(u.numpy())  </span><br><span class="line">print(u.numpy().decode(<span class="string">"utf-8"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b'<span class="symbol">\x</span>e4<span class="symbol">\x</span>bd<span class="symbol">\x</span>a0<span class="symbol">\x</span>e5<span class="symbol">\x</span>a5<span class="symbol">\x</span>bd <span class="symbol">\x</span>e4<span class="symbol">\x</span>b8<span class="symbol">\x</span>96<span class="symbol">\x</span>e7<span class="symbol">\x</span>95<span class="symbol">\x</span>8c'</span><br><span class="line">你好 世界</span><br></pre></td></tr></table></figure><h4 id="变量张量"><a href="#变量张量" class="headerlink" title="变量张量"></a>变量张量</h4><p>模型中需要被训练的参数一般被设置成变量Variable</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常量值不可以改变，常量的重新赋值相当于创造新的内存空间</span></span><br><span class="line">c = tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</span><br><span class="line">print(c)</span><br><span class="line">print(id(c))</span><br><span class="line">c = c + tf.constant([<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line">print(c)</span><br><span class="line">print(id(c))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([<span class="number">1.</span> <span class="number">2.</span>], shape=(<span class="number">2</span>,), dtype=<span class="built_in">float</span>32)</span><br><span class="line"><span class="number">5276289568</span></span><br><span class="line">tf.Tensor([<span class="number">2.</span> <span class="number">3.</span>], shape=(<span class="number">2</span>,), dtype=<span class="built_in">float</span>32)</span><br><span class="line"><span class="number">5276290240</span></span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值</span></span><br><span class="line">v = tf.Variable([1.0,2.0],name = <span class="string">"v"</span>)</span><br><span class="line"><span class="builtin-name">print</span>(v)</span><br><span class="line"><span class="builtin-name">print</span>(id(v))</span><br><span class="line">v.assign_add([1.0,1.0])</span><br><span class="line"><span class="builtin-name">print</span>(v)</span><br><span class="line"><span class="builtin-name">print</span>(id(v))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Variable <span class="string">'v:0'</span> shape=(<span class="number">2</span>,) dtype=<span class="built_in">float</span>32, numpy=<span class="built_in">array</span>([<span class="number">1.</span>, <span class="number">2.</span>], dtype=<span class="built_in">float</span>32)&gt;</span><br><span class="line"><span class="number">5276259888</span></span><br><span class="line">&lt;tf.Variable <span class="string">'v:0'</span> shape=(<span class="number">2</span>,) dtype=<span class="built_in">float</span>32, numpy=<span class="built_in">array</span>([<span class="number">2.</span>, <span class="number">3.</span>], dtype=<span class="built_in">float</span>32)&gt;</span><br><span class="line"><span class="number">5276259888</span></span><br></pre></td></tr></table></figure><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>Tensorflow 有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。</p><ul><li>在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。</li><li>在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。<ul><li>使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。</li><li>使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。</li></ul></li><li>如果需要在TensorFlow2.0中使用静态图，可以使用<code>@tf.function</code> 装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.0中用Session执行代码。使用<code>tf.function</code>构建静态图的方式叫做 <code>Autograph</code></li></ul><h4 id="计算图简介"><a href="#计算图简介" class="headerlink" title="计算图简介"></a>计算图简介</h4><p>计算图由节点(nodes)和线(edges)组成。</p><ul><li>节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。</li><li>实线表示有数据传递依赖，传递的数据即张量。</li><li>虚线通常可以表示控制依赖，即执行先后顺序。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/strjoin_graph.png"></p><h4 id="静态计算图"><a href="#静态计算图" class="headerlink" title="静态计算图"></a>静态计算图</h4><p>在TensorFlow1.0中，使用静态计算图分两步：</p><ul><li>定义计算图</li><li>在会话中执行计算图</li></ul><h5 id="TensorFlow-1-0静态计算图范例"><a href="#TensorFlow-1-0静态计算图范例" class="headerlink" title="TensorFlow 1.0静态计算图范例"></a>TensorFlow 1.0静态计算图范例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment">#placeholder为占位符，执行会话时候指定填充对象</span></span><br><span class="line">    x = tf.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)  </span><br><span class="line">    y = tf.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.string_join([x,y],name = <span class="string">'join'</span>,separator=<span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;))</span><br></pre></td></tr></table></figure><h5 id="TensorFlow2-0-怀旧版静态计算图"><a href="#TensorFlow2-0-怀旧版静态计算图" class="headerlink" title="TensorFlow2.0 怀旧版静态计算图"></a>TensorFlow2.0 怀旧版静态计算图</h5><p>TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。</p><p>可称之为怀旧版静态计算图，已经不推荐使用了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.compat.v1.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    x = tf.compat.v1.placeholder(name=<span class="string">'x'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    y = tf.compat.v1.placeholder(name=<span class="string">'y'</span>, shape=[], dtype=tf.string)</span><br><span class="line">    z = tf.strings.join([x,y],name = <span class="string">"join"</span>,separator = <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session(graph = g) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。</span></span><br><span class="line">    result = sess.run(fetches = z,feed_dict = &#123;x:<span class="string">"hello"</span>,y:<span class="string">"world"</span>&#125;)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b'hello world'</span><br></pre></td></tr></table></figure><h4 id="动态计算图"><a href="#动态计算图" class="headerlink" title="动态计算图"></a>动态计算图</h4><p>在TensorFlow2.0中，使用的是动态计算图和Autograph。与TensorFlow1.0中使用静态计算图分两步不同，动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为 <code>Eager Excution</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态计算图在每个算子处都进行构建，构建后立即执行</span></span><br><span class="line"></span><br><span class="line">x = tf.constant(<span class="string">"hello"</span>)</span><br><span class="line">y = tf.constant(<span class="string">"world"</span>)</span><br><span class="line">z = tf.strings.join([x,y],separator=<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hello world</span></span><br></pre></td></tr></table></figure><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 可以将动态计算图代码的输入和输出关系封装成函数</span><br><span class="line"></span><br><span class="line">def strjo<span class="meta">in(</span><span class="meta">x</span>,y):</span><br><span class="line">    z =  tf.strings.jo<span class="meta">in(</span>[<span class="meta">x</span>,y],separator = <span class="string">" "</span>)</span><br><span class="line">    tf.p<span class="meta">rint(</span>z)</span><br><span class="line">    <span class="meta">return</span> z</span><br><span class="line"></span><br><span class="line">result = strjo<span class="meta">in(</span>tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br><span class="line">p<span class="meta">rint(</span>result)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> <span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><h4 id="Autograph"><a href="#Autograph" class="headerlink" title="Autograph"></a>Autograph</h4><p>动态计算图运行效率相对较低。</p><p>可以用 <code>@tf.function</code> 装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。</p><p>在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。</p><p>在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了 <strong>定义函数</strong>，第二步执行计算图变成了<strong>调用函数</strong>。</p><p>不需要使用会话了，一些都像原始的Python语法一样自然。</p><p>实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。</p><p>当然，<code>@tf.function</code>的使用需要遵循一定的规范，我们后面章节将重点介绍。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></span><br><span class="line"></span><br><span class="line"># 使用autograph构建静态图</span><br><span class="line"></span><br><span class="line">@tf<span class="function">.<span class="keyword">function</span></span></span><br><span class="line">def strjoin(<span class="keyword">x</span>,<span class="keyword">y</span>):</span><br><span class="line">    <span class="keyword">z</span> =  <span class="keyword">tf</span>.strings.<span class="keyword">join</span>([<span class="keyword">x</span>,<span class="keyword">y</span>],separator = <span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">tf</span>.<span class="keyword">print</span>(<span class="keyword">z</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">z</span></span><br><span class="line"></span><br><span class="line">result = strjoin(<span class="keyword">tf</span>.constant(<span class="string">"hello"</span>),<span class="keyword">tf</span>.constant(<span class="string">"world"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span>(result)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello world</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> <span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">import os</span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = os.path.join(<span class="string">'data'</span>, <span class="string">'autograph'</span>, stamp)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 在 Python3 下建议使用 pathlib 修正各操作系统的路径</span></span><br><span class="line"><span class="comment"># from pathlib import Path</span></span><br><span class="line"><span class="comment"># stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span></span><br><span class="line"><span class="comment"># logdir = str(Path('./data/autograph/' + stamp))</span></span><br><span class="line"></span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(<span class="attribute">graph</span>=<span class="literal">True</span>, <span class="attribute">profiler</span>=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行autograph</span></span><br><span class="line">result = strjoin(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line">with writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        <span class="attribute">name</span>=<span class="string">"autograph"</span>,</span><br><span class="line">        <span class="attribute">step</span>=0,</span><br><span class="line">        <span class="attribute">profiler_outdir</span>=logdir)</span><br></pre></td></tr></table></figure><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#启动 tensorboard在jupyter中的魔法命令</span><br><span class="line"><span class="tag">%<span class="selector-tag">load_ext</span></span> tensorboard</span><br><span class="line">#启动tensorboard</span><br><span class="line"><span class="tag">%<span class="selector-tag">tensorboard</span></span> --logdir ./data/autograph/</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/2-2-tensorboard计算图.jpg"></p><h3 id="自动微分机制"><a href="#自动微分机制" class="headerlink" title="自动微分机制"></a>自动微分机制</h3><p>神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情，而深度学习框架可以帮助我们自动地完成这种求梯度运算。</p><p>Tensorflow一般使用梯度磁带 <code>tf.GradientTape</code> 来记录正向运算过程，然后反播磁带自动得到梯度值。</p><p>这种利用<code>tf.GradientTape</code> 求微分的方法叫做Tensorflow的 <strong>自动微分机制</strong></p><h4 id="利用梯度磁带求导数"><a href="#利用梯度磁带求导数" class="headerlink" title="利用梯度磁带求导数"></a>利用梯度磁带求导数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    </span><br><span class="line">dy_dx = tape.gradient(y,x)</span><br><span class="line">print(dy_dx)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(<span class="number">-2.0</span>, shape=(), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 对常量张量也可以求导，需要增加watch</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() as tape:</span><br><span class="line">    tape.watch([a,b,c])</span><br><span class="line">    <span class="symbol">y</span> = a*tf.pow(<span class="symbol">x</span>,<span class="number">2</span>) + b*<span class="symbol">x</span> + c</span><br><span class="line">    </span><br><span class="line">dy_dx,dy_da,dy_db,dy_dc = tape.gradient(<span class="symbol">y</span>,[<span class="symbol">x</span>,a,b,c])</span><br><span class="line">print(dy_da)</span><br><span class="line">print(dy_dc)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(<span class="number">0.0</span>, shape=(), dtype=<span class="built_in">float</span>32)</span><br><span class="line">tf.Tensor(<span class="number">1.0</span>, shape=(), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 可以求二阶导数</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() as tape2:</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() as tape1:   </span><br><span class="line">        <span class="symbol">y</span> = a*tf.pow(<span class="symbol">x</span>,<span class="number">2</span>) + b*<span class="symbol">x</span> + c</span><br><span class="line">    dy_dx = tape1.gradient(<span class="symbol">y</span>,<span class="symbol">x</span>)   </span><br><span class="line">dy2_dx2 = tape2.gradient(dy_dx,<span class="symbol">x</span>)</span><br><span class="line"></span><br><span class="line">print(dy2_dx2)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(<span class="number">2.0</span>, shape=(), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以在autograph中使用</span></span><br><span class="line"></span><br><span class="line">@tf<span class="function">.<span class="keyword">function</span></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">f</span><span class="params">(x)</span>:   </span></span><br><span class="line"><span class="function">    <span class="title">a</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">1.0</span>)</span></span></span><br><span class="line"><span class="function">    <span class="title">b</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">-2.0</span>)</span></span></span><br><span class="line"><span class="function">    <span class="title">c</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">1.0</span>)</span></span></span><br><span class="line"><span class="function">    </span></span><br><span class="line"><span class="function">    # 自变量转换成<span class="title">tf</span>.<span class="title">float32</span></span></span><br><span class="line"><span class="function">    <span class="title">x</span> = <span class="title">tf</span>.<span class="title">cast</span><span class="params">(x,tf.float32)</span></span></span><br><span class="line"><span class="function">    <span class="title">with</span> <span class="title">tf</span>.<span class="title">GradientTape</span><span class="params">()</span> <span class="title">as</span> <span class="title">tape</span>:</span></span><br><span class="line"><span class="function">        <span class="title">tape</span>.<span class="title">watch</span><span class="params">(x)</span></span></span><br><span class="line"><span class="function">        <span class="title">y</span> = <span class="title">a</span>*<span class="title">tf</span>.<span class="title">pow</span><span class="params">(x,<span class="number">2</span>)</span>+<span class="title">b</span>*<span class="title">x</span>+<span class="title">c</span></span></span><br><span class="line"><span class="function">    <span class="title">dy_dx</span> = <span class="title">tape</span>.<span class="title">gradient</span><span class="params">(y,x)</span> </span></span><br><span class="line"><span class="function">    </span></span><br><span class="line"><span class="function">    <span class="title">return</span><span class="params">(<span class="params">(dy_dx,y)</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">tf</span>.<span class="title">print</span><span class="params">(f<span class="params">(tf.constant<span class="params">(<span class="number">0.0</span>)</span>)</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">tf</span>.<span class="title">print</span><span class="params">(f<span class="params">(tf.constant<span class="params">(<span class="number">1.0</span>)</span>)</span>)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">-2</span>, <span class="number">1</span>)</span><br><span class="line">(<span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="利用梯度磁带和优化器求最小值"><a href="#利用梯度磁带和优化器求最小值" class="headerlink" title="利用梯度磁带和优化器求最小值"></a>利用梯度磁带和优化器求最小值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"><span class="comment"># 使用optimizer.apply_gradients</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">-2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = a*tf.pow(x,<span class="number">2</span>) + b*x + c</span><br><span class="line">    dy_dx = tape.gradient(y,x)</span><br><span class="line">    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])</span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,y,<span class="string">"; x ="</span>,x)</span><br></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">y</span> = <span class="number">0</span> <span class="comment">; x = 0.999998569</span></span><br></pre></td></tr></table></figure><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 求f(<span class="keyword">x</span>) = a*<span class="keyword">x</span>**<span class="number">2</span> + b*<span class="keyword">x</span> + <span class="keyword">c</span>的最小值</span><br><span class="line"># 使用optimizer.minimize</span><br><span class="line"># optimizer.minimize相当于先用tape求gradient,再apply_gradient</span><br><span class="line"></span><br><span class="line"><span class="keyword">x</span> = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">#注意f()无参数</span><br><span class="line">def f():   </span><br><span class="line">    a = tf.<span class="keyword">constant</span>(<span class="number">1.0</span>)</span><br><span class="line">    b = tf.<span class="keyword">constant</span>(<span class="number">-2.0</span>)</span><br><span class="line">    <span class="keyword">c</span> = tf.<span class="keyword">constant</span>(<span class="number">1.0</span>)</span><br><span class="line">    y = a*tf.pow(<span class="keyword">x</span>,<span class="number">2</span>)+b*<span class="keyword">x</span>+<span class="keyword">c</span></span><br><span class="line">    return(y)</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line">for _ in range(<span class="number">1000</span>):</span><br><span class="line">    optimizer.minimize(f,[<span class="keyword">x</span>])   </span><br><span class="line">    </span><br><span class="line">tf.print(<span class="string">"y ="</span>,f(),<span class="string">"; x ="</span>,<span class="keyword">x</span>)</span><br></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">y</span> = <span class="number">0</span> <span class="comment">; x = 0.999998569</span></span><br></pre></td></tr></table></figure><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 在autograph中完成最小值求解</span><br><span class="line"># 使用optimizer.apply_gradients</span><br><span class="line"></span><br><span class="line"><span class="keyword">x</span> = <span class="keyword">tf</span>.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = <span class="keyword">tf</span>.float32)</span><br><span class="line">optimizer = <span class="keyword">tf</span>.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">@tf<span class="function">.<span class="keyword">function</span></span></span><br><span class="line">def minimizef():</span><br><span class="line">    <span class="keyword">a</span> = <span class="keyword">tf</span>.constant(<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">b</span> = <span class="keyword">tf</span>.constant(-<span class="number">2.0</span>)</span><br><span class="line">    <span class="keyword">c</span> = <span class="keyword">tf</span>.constant(<span class="number">1.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ in <span class="keyword">tf</span>.<span class="built_in">range</span>(<span class="number">1000</span>): #注意autograph时使用<span class="keyword">tf</span>.<span class="built_in">range</span>(<span class="number">1000</span>)而不是<span class="built_in">range</span>(<span class="number">1000</span>)</span><br><span class="line">        with <span class="keyword">tf</span>.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">y</span> = <span class="keyword">a</span>*<span class="keyword">tf</span>.<span class="built_in">pow</span>(<span class="keyword">x</span>,<span class="number">2</span>) + <span class="keyword">b</span>*<span class="keyword">x</span> + <span class="keyword">c</span></span><br><span class="line">        dy_dx = tape.gradient(<span class="keyword">y</span>,<span class="keyword">x</span>)</span><br><span class="line">        optimizer.apply_gradients(grads_and_vars=[(dy_dx,<span class="keyword">x</span>)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">y</span> = <span class="keyword">a</span>*<span class="keyword">tf</span>.<span class="built_in">pow</span>(<span class="keyword">x</span>,<span class="number">2</span>) + <span class="keyword">b</span>*<span class="keyword">x</span> + <span class="keyword">c</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">y</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">tf</span>.<span class="keyword">print</span>(minimizef())</span><br><span class="line"><span class="keyword">tf</span>.<span class="keyword">print</span>(<span class="keyword">x</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">0.999998569</span></span><br></pre></td></tr></table></figure><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在autograph中完成最小值求解</span></span><br><span class="line"><span class="comment"># 使用optimizer.minimize</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">0.0</span>,name = <span class="string">"x"</span>,dtype = tf.float32)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>)   </span><br><span class="line"></span><br><span class="line">@tf<span class="function">.<span class="keyword">function</span></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">f</span><span class="params">()</span>:   </span></span><br><span class="line"><span class="function">    <span class="title">a</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">1.0</span>)</span></span></span><br><span class="line"><span class="function">    <span class="title">b</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">-2.0</span>)</span></span></span><br><span class="line"><span class="function">    <span class="title">c</span> = <span class="title">tf</span>.<span class="title">constant</span><span class="params">(<span class="number">1.0</span>)</span></span></span><br><span class="line"><span class="function">    <span class="title">y</span> = <span class="title">a</span>*<span class="title">tf</span>.<span class="title">pow</span><span class="params">(x,<span class="number">2</span>)</span>+<span class="title">b</span>*<span class="title">x</span>+<span class="title">c</span></span></span><br><span class="line"><span class="function">    <span class="title">return</span><span class="params">(y)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">@<span class="title">tf</span>.<span class="title">function</span></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">train</span><span class="params">(epoch)</span>:  </span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">_</span> <span class="title">in</span> <span class="title">tf</span>.<span class="title">range</span><span class="params">(epoch)</span>:  </span></span><br><span class="line"><span class="function">        <span class="title">optimizer</span>.<span class="title">minimize</span><span class="params">(f,[x])</span></span></span><br><span class="line"><span class="function">    <span class="title">r</span>结构<span class="title">eturn</span><span class="params">(f<span class="params">()</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">tf</span>.<span class="title">print</span><span class="params">(train<span class="params">(<span class="number">1000</span>)</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">tf</span>.<span class="title">print</span><span class="params">(x)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">0.999998569</span></span><br></pre></td></tr></table></figure><h2 id="结构层次"><a href="#结构层次" class="headerlink" title="结构层次"></a>结构层次</h2><p>本章我们介绍TensorFlow中5个不同的层次结构：即硬件层，内核层，低阶API，中阶API，高阶API。并以线性回归和DNN二分类模型为例，直观对比展示在不同层级实现模型的特点。</p><p>TensorFlow的层次结构从低到高可以分成如下五层。</p><ul><li>最底层为硬件层，TensorFlow支持CPU、GPU或TPU加入计算资源池。</li><li>第二层为C++实现的内核，kernel可以跨平台分布运行。</li><li>第三层为Python实现的操作符，提供了封装C++内核的低级API指令，主要包括各种张量操作算子、计算图、自动微分. 如<code>tf.Variable</code>, <code>tf.constant</code>, <code>tf.function</code>, <code>tf.GradientTape</code>, <code>tf.nn.softmax</code> … 如果把模型比作一个房子，那么第三层API就是【模型之砖】。</li><li>第四层为Python实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。 如<code>tf.keras.layers</code>, <code>tf.keras.losses</code>, <code>tf.keras.metrics</code>, <code>tf.keras.optimizers</code>, <code>tf.data.DataSet</code>, <code>tf.feature_column</code>… 如果把模型比作一个房子，那么第四层API就是【模型之墙】。</li><li>第五层为Python实现的模型成品，一般为按照OOP方式封装的高级API，主要为tf.keras.models提供的模型的类接口。 如果把模型比作一个房子，那么第五层API就是模型本身，即【模型之屋】。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/tensorflow_structure.jpg"></p><h3 id="低阶API示范"><a href="#低阶API示范" class="headerlink" title="低阶API示范"></a>低阶API示范</h3><p>下面的范例使用TensorFlow的低阶API实现线性回归模型和DNN二分类模型。</p><p>低阶API主要包括张量操作，计算图和自动微分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><h4 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h4><h5 id="准备数据-4"><a href="#准备数据-4" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.<span class="attr">figure_format</span> = 'svg'</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="attr">figsize</span> = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line"><span class="attr">ax1</span> = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], <span class="attr">c</span> = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,<span class="attr">rotation</span> = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="attr">ax2</span> = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], <span class="attr">c</span> = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,<span class="attr">rotation</span> = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-1-01-回归数据可视化.png"></p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 构建数据管道迭代器</span><br><span class="line">def data_iter(<span class="built_in">features</span>, <span class="built_in">labels</span>, batch_size=<span class="number">8</span>):</span><br><span class="line">    num_examples = len(<span class="built_in">features</span>)</span><br><span class="line">    <span class="built_in">indices</span> = list(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    <span class="built_in">np</span>.<span class="built_in">random</span>.shuffle(<span class="built_in">indices</span>)  #样本的读取顺序是随机的</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = <span class="built_in">indices</span>[i: <span class="built_in">min</span>(i + batch_size, num_examples)]</span><br><span class="line">        yield tf.gather(<span class="built_in">features</span>,indexs), tf.gather(<span class="built_in">labels</span>,indexs)</span><br><span class="line">        </span><br><span class="line"># 测试数据管道效果   </span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">(<span class="built_in">features</span>,<span class="built_in">labels</span>) = next(data_iter(X,Y,batch_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">features</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">labels</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">2.6161194</span>   <span class="number">0.11071014</span>]</span><br><span class="line"> [ <span class="number">9.79207</span>    <span class="number">-0.70180416</span>]</span><br><span class="line"> [ <span class="number">9.792343</span>    <span class="number">6.9149055</span> ]</span><br><span class="line"> [<span class="number">-2.4186516</span>  <span class="number">-9.375019</span>  ]</span><br><span class="line"> [ <span class="number">9.83749</span>    <span class="number">-3.4637213</span> ]</span><br><span class="line"> [ <span class="number">7.3953056</span>   <span class="number">4.374569</span>  ]</span><br><span class="line"> [<span class="number">-0.14686584</span> <span class="number">-0.28063297</span>]</span><br><span class="line"> [ <span class="number">0.49001217</span> <span class="number">-9.739792</span>  ]], shape=(<span class="number">8</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">9.334667</span> ]</span><br><span class="line"> [<span class="number">22.058844</span> ]</span><br><span class="line"> [ <span class="number">3.0695205</span>]</span><br><span class="line"> [<span class="number">26.736238</span> ]</span><br><span class="line"> [<span class="number">35.292133</span> ]</span><br><span class="line"> [ <span class="number">4.2943544</span>]</span><br><span class="line"> [ <span class="number">1.6713585</span>]</span><br><span class="line"> [<span class="number">34.826904</span> ]], shape=(<span class="number">8</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><h5 id="定义模型-4"><a href="#定义模型-4" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable(tf.random.normal(w0.shape))</span><br><span class="line">b = tf.Variable(tf.zeros_like(b0,dtype = tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span>     </span><br><span class="line">    <span class="comment">#正向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> x@w + b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="keyword">return</span> tf.reduce_mean((y_true - y_pred)**<span class="number">2</span>/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure><h5 id="训练模型-4"><a href="#训练模型-4" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用动态图调试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 测试train_step效果</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data<span class="constructor">_iter(X,Y,<span class="params">batch_size</span>)</span>)</span><br><span class="line">train<span class="constructor">_step(<span class="params">model</span>,<span class="params">features</span>,<span class="params">labels</span>)</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">211.09982</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">35</span>:<span class="number">56</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">1.78806472</span></span><br><span class="line">w = <span class="string">[[1.97554708]</span></span><br><span class="line"><span class="string"> [-2.97719598]]</span></span><br><span class="line">b = <span class="string">[[2.60692883]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">00</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">2.64588404</span></span><br><span class="line">w = <span class="string">[[1.97319281]</span></span><br><span class="line"><span class="string"> [-2.97810626]]</span></span><br><span class="line">b = <span class="string">[[2.95525956]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">1.42576694</span></span><br><span class="line">w = <span class="string">[[1.96466208]</span></span><br><span class="line"><span class="string"> [-2.98337793]]</span></span><br><span class="line">b = <span class="string">[[3.00264144]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">08</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.68992615</span></span><br><span class="line">w = <span class="string">[[1.97718477]</span></span><br><span class="line"><span class="string"> [-2.983814]]</span></span><br><span class="line">b = <span class="string">[[3.01013041]]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions)</span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    dloss_dw,dloss_db = tape.gradient(loss,[w,b])</span><br><span class="line">    <span class="comment"># 梯度下降法更新参数</span></span><br><span class="line">    w.assign(w - <span class="number">0.001</span>*dloss_dw)</span><br><span class="line">    b.assign(b - <span class="number">0.001</span>*dloss_db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">10</span>):</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,w)</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,b)</span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.894210339</span></span><br><span class="line">w = <span class="string">[[1.96927285]</span></span><br><span class="line"><span class="string"> [-2.98914337]]</span></span><br><span class="line">b = <span class="string">[[3.00987792]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">1.58621466</span></span><br><span class="line">w = <span class="string">[[1.97566223]</span></span><br><span class="line"><span class="string"> [-2.98550248]]</span></span><br><span class="line">b = <span class="string">[[3.00998402]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">37</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">2.2695992</span></span><br><span class="line">w = <span class="string">[[1.96664226]</span></span><br><span class="line"><span class="string"> [-2.99248481]]</span></span><br><span class="line">b = <span class="string">[[3.01028705]]</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">36</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">1.90848124</span></span><br><span class="line">w = <span class="string">[[1.98000824]</span></span><br><span class="line"><span class="string"> [-2.98888135]]</span></span><br><span class="line">b = <span class="string">[[3.01085401]]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-1-2-回归结果可视化.png"></p><h4 id="DNN二分类模型"><a href="#DNN二分类模型" class="headerlink" title="DNN二分类模型"></a>DNN二分类模型</h4><h5 id="准备数据-5"><a href="#准备数据-5" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-1-03-分类数据可视化.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据管道迭代器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(features, labels, batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    np.random.shuffle(indices)  <span class="comment">#样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        indexs = indices[i: min(i + batch_size, num_examples)]</span><br><span class="line">        <span class="keyword">yield</span> tf.gather(features,indexs), tf.gather(labels,indexs)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试数据管道效果   </span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(features,labels) = next(data_iter(X,Y,batch_size))</span><br><span class="line">print(features)</span><br><span class="line">print(labels)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[ <span class="number">0.03732629</span>  <span class="number">3.5783494</span> ]</span><br><span class="line"> [ <span class="number">0.542919</span>    <span class="number">5.035079</span>  ]</span><br><span class="line"> [ <span class="number">5.860281</span>   <span class="number">-2.4476354</span> ]</span><br><span class="line"> [ <span class="number">0.63657564</span>  <span class="number">3.194231</span>  ]</span><br><span class="line"> [<span class="number">-3.5072308</span>   <span class="number">2.5578873</span> ]</span><br><span class="line"> [<span class="number">-2.4109735</span>  <span class="number">-3.6621518</span> ]</span><br><span class="line"> [ <span class="number">4.0975413</span>  <span class="number">-2.4172943</span> ]</span><br><span class="line"> [ <span class="number">1.9393908</span>  <span class="number">-6.782317</span>  ]</span><br><span class="line"> [<span class="number">-4.7453732</span>  <span class="number">-0.5176727</span> ]</span><br><span class="line"> [<span class="number">-1.4057113</span>  <span class="number">-7.9775257</span> ]], shape=(<span class="number">10</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]</span><br><span class="line"> [<span class="number">0.</span>]], shape=(<span class="number">10</span>, <span class="number">1</span>), dtype=<span class="built_in">float</span>32)</span><br></pre></td></tr></table></figure><h5 id="定义模型-5"><a href="#定义模型-5" class="headerlink" title="定义模型"></a>定义模型</h5><p>此处范例我们利用tf.Module来组织模型变量，关于tf.Module的较详细介绍参考本书第四章最后一节: Autograph和tf.Module。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.w1 = tf.Variable(tf.random.truncated_normal([<span class="number">2</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">4</span>]),dtype = tf.float32)</span><br><span class="line">        self.w2 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">8</span>]),dtype = tf.float32)</span><br><span class="line">        self.w3 = tf.Variable(tf.random.truncated_normal([<span class="number">8</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line">        self.b3 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>]),dtype = tf.float32)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = tf.nn.relu(x@self.w1 + self.b1)</span><br><span class="line">        x = tf.nn.relu(x@self.w2 + self.b2)</span><br><span class="line">        y = tf.nn.sigmoid(x@self.w3 + self.b3)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失函数(二元交叉熵)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)])  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(self,y_true,y_pred)</span>:</span>  </span><br><span class="line">        <span class="comment">#将预测值限制在 1e-7 以上, 1 - 1e-7 以下，避免log(0)错误</span></span><br><span class="line">        eps = <span class="number">1e-7</span></span><br><span class="line">        y_pred = tf.clip_by_value(y_pred,eps,<span class="number">1.0</span>-eps)</span><br><span class="line">        bce = - y_true*tf.math.log(y_pred) - (<span class="number">1</span>-y_true)*tf.math.log(<span class="number">1</span>-y_pred)</span><br><span class="line">        <span class="keyword">return</span>  tf.reduce_mean(bce)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 评估指标(准确率)</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,1], dtype = tf.float32),</span></span><br><span class="line">                              tf.TensorSpec(shape = [<span class="literal">None</span>,<span class="number">1</span>], dtype = tf.float32)]) </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">metric_func</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_pred = tf.where(y_pred&gt;<span class="number">0.5</span>,tf.ones_like(y_pred,dtype = tf.float32),</span><br><span class="line">                          tf.zeros_like(y_pred,dtype = tf.float32))</span><br><span class="line">        acc = tf.reduce_mean(<span class="number">1</span>-tf.abs(y_true-y_pred))</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 测试模型结构</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">(<span class="built_in">features</span>,<span class="built_in">labels</span>) = next(data_iter(X,Y,batch_size))</span><br><span class="line"></span><br><span class="line">predictions = model(<span class="built_in">features</span>)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(<span class="built_in">labels</span>,predictions)</span><br><span class="line">metric = model.metric_func(<span class="built_in">labels</span>,predictions)</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">print</span>(<span class="string">"init loss:"</span>,loss)</span><br><span class="line">tf.<span class="built_in">print</span>(<span class="string">"init metric"</span>,metric)</span><br><span class="line">init loss: <span class="number">1.76568353</span></span><br><span class="line">init metric <span class="number">0.6</span></span><br><span class="line"><span class="built_in">print</span>(len(model.trainable_variables))</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure><h5 id="训练模型-5"><a href="#训练模型-5" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 正向传播求损失</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(labels, predictions) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 反向传播求梯度</span></span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 执行梯度下降</span></span><br><span class="line">    <span class="keyword">for</span> p, dloss_dp <span class="keyword">in</span> zip(model.trainable_variables,grads):</span><br><span class="line">        p.assign(p - <span class="number">0.001</span>*dloss_dp)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算评估指标</span></span><br><span class="line">    metric = model.metric_func(labels,predictions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, metric</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter(X,Y,<span class="number">100</span>):</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>, metric)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">train_model(model,epochs = <span class="number">600</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">35</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">0.567795336</span> accuracy =  <span class="number">0.71</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">39</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">0.50955683</span> accuracy =  <span class="number">0.77</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">300</span> loss =  <span class="number">0.421476126</span> accuracy =  <span class="number">0.84</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">47</span></span><br><span class="line">epoch = <span class="number">400</span> loss =  <span class="number">0.330618203</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">500</span> loss =  <span class="number">0.308296859</span> accuracy =  <span class="number">0.89</span></span><br><span class="line">================================================================================<span class="number">16</span>:<span class="number">47</span>:<span class="number">55</span></span><br><span class="line">epoch = <span class="number">600</span> loss =  <span class="number">0.279367268</span> accuracy =  <span class="number">0.96</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>],Xp[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>],Xn[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>],Xp_pred[:,<span class="number">1</span>],c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>],Xn_pred[:,<span class="number">1</span>],c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-1-04-分类结果可视化.png"></p><h3 id="中阶API示范"><a href="#中阶API示范" class="headerlink" title="中阶API示范"></a>中阶API示范</h3><p>下面的范例使用TensorFlow的中阶API实现线性回归模型和和DNN二分类模型。</p><p>TensorFlow的中阶API主要包括各种模型层，损失函数，优化器，数据管道，特征列等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><h4 id="线性回归模型-1"><a href="#线性回归模型-1" class="headerlink" title="线性回归模型"></a>线性回归模型</h4><h5 id="准备数据-6"><a href="#准备数据-6" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-2-01-回归数据可视化.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">100</span>).batch(<span class="number">10</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><h5 id="定义模型-6"><a href="#定义模型-6" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = layers.Dense(units = <span class="number">1</span>) </span><br><span class="line">model.build(input_shape = (<span class="number">2</span>,)) <span class="comment">#用build方法创建variables</span></span><br><span class="line">model.loss_func = losses.mean_squared_error</span><br><span class="line">model.optimizer = optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><h5 id="训练模型-6"><a href="#训练模型-6" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.variables))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss = tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss)</span><br><span class="line">            tf.print(<span class="string">"w ="</span>,model.variables[<span class="number">0</span>])</span><br><span class="line">            tf.print(<span class="string">"b ="</span>,model.variables[<span class="number">1</span>])</span><br><span class="line">train_model(model,epochs = <span class="number">200</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">48</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">2.56481647</span></span><br><span class="line">w = [[<span class="number">1.99355531</span>]</span><br><span class="line"> [<span class="number">-2.99061537</span>]]</span><br><span class="line">b = [<span class="number">3.09484935</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">51</span></span><br><span class="line">epoch = <span class="number">100</span> loss =  <span class="number">5.96198225</span></span><br><span class="line">w = [[<span class="number">1.98028314</span>]</span><br><span class="line"> [<span class="number">-2.96975136</span>]]</span><br><span class="line">b = [<span class="number">3.09501529</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">54</span></span><br><span class="line">epoch = <span class="number">150</span> loss =  <span class="number">4.79625702</span></span><br><span class="line">w = [[<span class="number">2.00056171</span>]</span><br><span class="line"> [<span class="number">-2.98774862</span>]]</span><br><span class="line">b = [<span class="number">3.09567738</span>]</span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">01</span>:<span class="number">58</span></span><br><span class="line">epoch = <span class="number">200</span> loss =  <span class="number">8.26704407</span></span><br><span class="line">w = [[<span class="number">2.00282311</span>]</span><br><span class="line"> [<span class="number">-2.99300027</span>]]</span><br><span class="line">b = [<span class="number">3.09406662</span>]</span><br></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.<span class="attr">figure_format</span> = 'svg'</span><br><span class="line"></span><br><span class="line">w,<span class="attr">b</span> = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="attr">figsize</span> = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line"><span class="attr">ax1</span> = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], <span class="attr">c</span> = <span class="string">"b"</span>,<span class="attr">label</span> = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,<span class="attr">linewidth</span> = <span class="number">5.0</span>,<span class="attr">label</span> = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,<span class="attr">rotation</span> = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">ax2</span> = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], <span class="attr">c</span> = <span class="string">"g"</span>,<span class="attr">label</span> = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,<span class="attr">linewidth</span> = <span class="number">5.0</span>,<span class="attr">label</span> = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,<span class="attr">rotation</span> = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-2-02-回归结果可视化.png"></p><h4 id="DNN二分类模型-1"><a href="#DNN二分类模型-1" class="headerlink" title="DNN二分类模型"></a>DNN二分类模型</h4><h5 id="准备数据-7"><a href="#准备数据-7" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-2-03-分类数据可视化.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建输入数据管道</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((X,Y)) \</span><br><span class="line">     .shuffle(buffer_size = <span class="number">4000</span>).batch(<span class="number">100</span>) \</span><br><span class="line">     .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><h5 id="定义模型-7"><a href="#定义模型-7" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name = None)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__(name=name)</span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">model = DNNModel()</span><br><span class="line">model.loss_func = losses.binary_crossentropy</span><br><span class="line">model.metric_func = metrics.binary_accuracy</span><br><span class="line">model.optimizer = optimizers.Adam(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试模型结构</span></span><br><span class="line">(features,labels) = next(ds.as_numpy_iterator())</span><br><span class="line"></span><br><span class="line">predictions = model(features)</span><br><span class="line"></span><br><span class="line">loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]),tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"init loss:"</span>,loss)</span><br><span class="line">tf.print(<span class="string">"init metric"</span>,metric)</span><br><span class="line">init loss: <span class="number">1.13653195</span></span><br><span class="line">init metric <span class="number">0.5</span></span><br></pre></td></tr></table></figure><h5 id="训练模型-7"><a href="#训练模型-7" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用autograph机制转换成静态图加速</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = model.loss_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    grads = tape.gradient(loss,model.trainable_variables)</span><br><span class="line">    model.optimizer.apply_gradients(zip(grads,model.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    metric = model.metric_func(tf.reshape(labels,[<span class="number">-1</span>]), tf.reshape(predictions,[<span class="number">-1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss,metric</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试train_step效果</span></span><br><span class="line">features,labels = next(ds.as_numpy_iterator())</span><br><span class="line">train_step(model,features,labels)</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">1.2033114</span>&gt;</span>,</span><br><span class="line"> <span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">0.47</span>&gt;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        loss, metric = tf.constant(<span class="number">0.0</span>),tf.constant(<span class="number">0.0</span>)</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds:</span><br><span class="line">            loss,metric = train_step(model,features,labels)</span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">"loss = "</span>,loss, <span class="string">"accuracy = "</span>,metric)</span><br><span class="line">train_model(model,epochs = <span class="number">60</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">36</span></span><br><span class="line">epoch = <span class="number">10</span> loss =  <span class="number">0.556449413</span> accuracy =  <span class="number">0.79</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">38</span></span><br><span class="line">epoch = <span class="number">20</span> loss =  <span class="number">0.439187407</span> accuracy =  <span class="number">0.86</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">40</span></span><br><span class="line">epoch = <span class="number">30</span> loss =  <span class="number">0.259921253</span> accuracy =  <span class="number">0.95</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">42</span></span><br><span class="line">epoch = <span class="number">40</span> loss =  <span class="number">0.244920313</span> accuracy =  <span class="number">0.9</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">43</span></span><br><span class="line">epoch = <span class="number">50</span> loss =  <span class="number">0.19839409</span> accuracy =  <span class="number">0.92</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">07</span>:<span class="number">45</span></span><br><span class="line">epoch = <span class="number">60</span> loss =  <span class="number">0.126151696</span> accuracy =  <span class="number">0.95</span></span><br></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(<span class="attr">nrows=1,ncols=2,figsize</span> = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),<span class="attr">c</span> = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),<span class="attr">c</span> = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line"><span class="attr">Xp_pred</span> = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),<span class="attr">axis</span> = <span class="number">0</span>)</span><br><span class="line"><span class="attr">Xn_pred</span> = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),<span class="attr">axis</span> = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),<span class="attr">c</span> = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),<span class="attr">c</span> = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-2-04-分类结果可视化.png"></p><h3 id="高阶API示范"><a href="#高阶API示范" class="headerlink" title="高阶API示范"></a>高阶API示范</h3><p>下面的范例使用TensorFlow的高阶API实现线性回归模型和DNN二分类模型。</p><p>TensorFlow的高阶API主要为tf.keras.models提供的模型的类接口。</p><p>使用Keras接口有以下3种方式构建模型：使用Sequential按层顺序构建模型，使用函数式API构建任意结构模型，继承Model基类构建自定义模型。</p><p>此处分别演示使用Sequential按层顺序构建模型以及继承Model基类构建自定义模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><h4 id="线性回归模型-2"><a href="#线性回归模型-2" class="headerlink" title="线性回归模型"></a>线性回归模型</h4><p>此范例我们使用Sequential按层顺序构建模型，并使用内置model.fit方法训练模型【面向新手】。</p><h5 id="准备数据-8"><a href="#准备数据-8" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本数量</span></span><br><span class="line">n = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-3.0</span>]])</span><br><span class="line">b0 = tf.constant([[<span class="number">3.0</span>]])</span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-3-01-回归数据可视化.png"></p><h5 id="定义模型-8"><a href="#定义模型-8" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,input_shape =(<span class="number">2</span>,)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br></pre></td></tr></table></figure><h5 id="训练模型-8"><a href="#训练模型-8" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 使用fit方法进行训练</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">"adam"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">model.fit(X,Y,batch_size = <span class="number">10</span>,epochs = <span class="number">200</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,model.layers[<span class="number">0</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,model.layers[<span class="number">0</span>].bias)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">197</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">190</span>us/sample - loss: <span class="number">4.3977</span> - mae: <span class="number">1.7129</span></span><br><span class="line">Epoch <span class="number">198</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">172</span>us/sample - loss: <span class="number">4.3918</span> - mae: <span class="number">1.7117</span></span><br><span class="line">Epoch <span class="number">199</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">134</span>us/sample - loss: <span class="number">4.3861</span> - mae: <span class="number">1.7106</span></span><br><span class="line">Epoch <span class="number">200</span>/<span class="number">200</span></span><br><span class="line"><span class="number">400</span>/<span class="number">400</span> [==============================] - <span class="number">0</span>s <span class="number">166</span>us/sample - loss: <span class="number">4.3786</span> - mae: <span class="number">1.7092</span></span><br><span class="line">w =  [[<span class="number">1.99339032</span>]</span><br><span class="line"> [<span class="number">-3.00866461</span>]]</span><br><span class="line">b =  [<span class="number">2.67018795</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line">w,b = model.variables</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.scatter(X[:,<span class="number">0</span>],Y[:,<span class="number">0</span>], c = <span class="string">"b"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax1.plot(X[:,<span class="number">0</span>],w[<span class="number">0</span>]*X[:,<span class="number">0</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax1.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">ax2.scatter(X[:,<span class="number">1</span>],Y[:,<span class="number">0</span>], c = <span class="string">"g"</span>,label = <span class="string">"samples"</span>)</span><br><span class="line">ax2.plot(X[:,<span class="number">1</span>],w[<span class="number">1</span>]*X[:,<span class="number">1</span>]+b[<span class="number">0</span>],<span class="string">"-r"</span>,linewidth = <span class="number">5.0</span>,label = <span class="string">"model"</span>)</span><br><span class="line">ax2.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>,rotation = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-3-02-回归结果可视化.png"></p><h4 id="DNN二分类模型-2"><a href="#DNN二分类模型-2" class="headerlink" title="DNN二分类模型"></a>DNN二分类模型</h4><p>此范例我们使用继承Model基类构建自定义模型，并构建自定义训练循环【面向专家】</p><h5 id="准备数据-9"><a href="#准备数据-9" class="headerlink" title="准备数据"></a>准备数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,losses,metrics,optimizers</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#正负样本数量</span></span><br><span class="line">n_positive,n_negative = <span class="number">2000</span>,<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成正样本, 小圆环分布</span></span><br><span class="line">r_p = <span class="number">5.0</span> + tf.random.truncated_normal([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_p = tf.random.uniform([n_positive,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xp = tf.concat([r_p*tf.cos(theta_p),r_p*tf.sin(theta_p)],axis = <span class="number">1</span>)</span><br><span class="line">Yp = tf.ones_like(r_p)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成负样本, 大圆环分布</span></span><br><span class="line">r_n = <span class="number">8.0</span> + tf.random.truncated_normal([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>)</span><br><span class="line">theta_n = tf.random.uniform([n_negative,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">2</span>*np.pi) </span><br><span class="line">Xn = tf.concat([r_n*tf.cos(theta_n),r_n*tf.sin(theta_n)],axis = <span class="number">1</span>)</span><br><span class="line">Yn = tf.zeros_like(r_n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#汇总样本</span></span><br><span class="line">X = tf.concat([Xp,Xn],axis = <span class="number">0</span>)</span><br><span class="line">Y = tf.concat([Yp,Yn],axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#样本洗牌</span></span><br><span class="line">data = tf.concat([X,Y],axis = <span class="number">1</span>)</span><br><span class="line">data = tf.random.shuffle(data)</span><br><span class="line">X = data[:,:<span class="number">2</span>]</span><br><span class="line">Y = data[:,<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#可视化</span></span><br><span class="line">plt.figure(figsize = (<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">plt.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">plt.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-3-03-分类数据可视化.png"></p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">ds_train</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.from_tensor_slices((<span class="type">X</span>[0:<span class="title">n</span>*3//4,:],<span class="type">Y</span>[0:<span class="title">n</span>*3//4,:])) \</span></span><br><span class="line">     .shuffle(buffer_size = <span class="number">1000</span>).batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.<span class="class"><span class="keyword">data</span>.experimental.<span class="type">AUTOTUNE</span>) \</span></span><br><span class="line">     .cache()</span><br><span class="line"></span><br><span class="line"><span class="title">ds_valid</span> = tf.<span class="class"><span class="keyword">data</span>.<span class="type">Dataset</span>.from_tensor_slices((<span class="type">X</span>[<span class="title">n</span>*3//4:,:],<span class="type">Y</span>[<span class="title">n</span>*3//4:,:])) \</span></span><br><span class="line">     .batch(<span class="number">20</span>) \</span><br><span class="line">     .prefetch(tf.<span class="class"><span class="keyword">data</span>.experimental.<span class="type">AUTOTUNE</span>) \</span></span><br><span class="line">     .cache()</span><br></pre></td></tr></table></figure><h5 id="定义模型-9"><a href="#定义模型-9" class="headerlink" title="定义模型"></a>定义模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DNNModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(DNNModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">4</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense1"</span>) </span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">8</span>,activation = <span class="string">"relu"</span>,name = <span class="string">"dense2"</span>)</span><br><span class="line">        self.dense3 = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>,name = <span class="string">"dense3"</span>)</span><br><span class="line">        super(DNNModel,self).build(input_shape)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [None,2], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        x = self.dense2(x)</span><br><span class="line">        y = self.dense3(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">model = DNNModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Model: "dnn_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense1 (Dense)               multiple                  12        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense2 (Dense)               multiple                  40        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense3 (Dense)               multiple                  9         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 61</span><br><span class="line">Trainable params: 61</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h5 id="训练模型-9"><a href="#训练模型-9" class="headerlink" title="训练模型"></a>训练模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 自定义训练循环</span></span><br><span class="line"></span><br><span class="line">optimizer = optimizers.Adam(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">loss_func = tf.keras.losses.BinaryCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = tf.keras.metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = tf.keras.metrics.BinaryAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>  epoch%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">        </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_valid,<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">02</span></span><br><span class="line">Epoch=<span class="number">100</span>,Loss:<span class="number">0.194088802</span>,Accuracy:<span class="number">0.923064</span>,Valid Loss:<span class="number">0.215538561</span>,Valid Accuracy:<span class="number">0.904368</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">22</span></span><br><span class="line">Epoch=<span class="number">200</span>,Loss:<span class="number">0.151239693</span>,Accuracy:<span class="number">0.93768847</span>,Valid Loss:<span class="number">0.181166962</span>,Valid Accuracy:<span class="number">0.920664132</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">35</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">300</span>,Loss:<span class="number">0.134556711</span>,Accuracy:<span class="number">0.944247484</span>,Valid Loss:<span class="number">0.171530813</span>,Valid Accuracy:<span class="number">0.926396072</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">04</span></span><br><span class="line">Epoch=<span class="number">400</span>,Loss:<span class="number">0.125722557</span>,Accuracy:<span class="number">0.949172914</span>,Valid Loss:<span class="number">0.16731061</span>,Valid Accuracy:<span class="number">0.929318547</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">24</span></span><br><span class="line">Epoch=<span class="number">500</span>,Loss:<span class="number">0.120216407</span>,Accuracy:<span class="number">0.952525079</span>,Valid Loss:<span class="number">0.164817035</span>,Valid Accuracy:<span class="number">0.931044817</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">36</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">600</span>,Loss:<span class="number">0.116434008</span>,Accuracy:<span class="number">0.954830289</span>,Valid Loss:<span class="number">0.163089141</span>,Valid Accuracy:<span class="number">0.932202339</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">700</span>,Loss:<span class="number">0.113658346</span>,Accuracy:<span class="number">0.956433</span>,Valid Loss:<span class="number">0.161804497</span>,Valid Accuracy:<span class="number">0.933092058</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">25</span></span><br><span class="line">Epoch=<span class="number">800</span>,Loss:<span class="number">0.111522928</span>,Accuracy:<span class="number">0.957467675</span>,Valid Loss:<span class="number">0.160796657</span>,Valid Accuracy:<span class="number">0.93379426</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">46</span></span><br><span class="line">Epoch=<span class="number">900</span>,Loss:<span class="number">0.109816991</span>,Accuracy:<span class="number">0.958205402</span>,Valid Loss:<span class="number">0.159987748</span>,Valid Accuracy:<span class="number">0.934343576</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">38</span>:<span class="number">06</span></span><br><span class="line">Epoch=<span class="number">1000</span>,Loss:<span class="number">0.10841465</span>,Accuracy:<span class="number">0.958805501</span>,Valid Loss:<span class="number">0.159325734</span>,Valid Accuracy:<span class="number">0.934785843</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 结果可视化</span></span><br><span class="line">fig, (ax1,ax2) = plt.subplots(nrows=<span class="number">1</span>,ncols=<span class="number">2</span>,figsize = (<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line">ax1.scatter(Xp[:,<span class="number">0</span>].numpy(),Xp[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax1.scatter(Xn[:,<span class="number">0</span>].numpy(),Xn[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax1.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax1.set_title(<span class="string">"y_true"</span>);</span><br><span class="line"></span><br><span class="line">Xp_pred = tf.boolean_mask(X,tf.squeeze(model(X)&gt;=<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line">Xn_pred = tf.boolean_mask(X,tf.squeeze(model(X)&lt;<span class="number">0.5</span>),axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ax2.scatter(Xp_pred[:,<span class="number">0</span>].numpy(),Xp_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"r"</span>)</span><br><span class="line">ax2.scatter(Xn_pred[:,<span class="number">0</span>].numpy(),Xn_pred[:,<span class="number">1</span>].numpy(),c = <span class="string">"g"</span>)</span><br><span class="line">ax2.legend([<span class="string">"positive"</span>,<span class="string">"negative"</span>]);</span><br><span class="line">ax2.set_title(<span class="string">"y_pred"</span>);</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/3-3-04-分类结果可视化.png"></p><h2 id="低阶API"><a href="#低阶API" class="headerlink" title="低阶API"></a>低阶API</h2><p>TensorFlow的低阶API主要包括张量操作，计算图和自动微分。如果把模型比作一个房子，那么低阶API就是【模型之砖】。在低阶API层次上，可以把TensorFlow当做一个增强版的numpy来使用。TensorFlow提供的方法比numpy更全面，运算速度更快，如果需要的话，还可以使用GPU进行加速。前面几章我们对低阶API已经有了一个整体的认识，本章我们将重点详细介绍张量操作和Autograph计算图。</p><h3 id="张量的结构操作"><a href="#张量的结构操作" class="headerlink" title="张量的结构操作"></a>张量的结构操作</h3><p>张量的操作主要包括：</p><ul><li>张量结构操作：张量创建，索引切片，维度变换，合并分割。</li><li>张量数学运算：标量运算，向量运算，矩阵运算。</li></ul><p>另外我们会介绍张量运算的广播机制。</p><h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><p>张量创建的许多方法和numpy中创建array的方法很像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype = tf.float32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = tf.range(<span class="number">1</span>,<span class="number">10</span>,delta = <span class="number">2</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">3</span> <span class="number">5</span> <span class="number">7</span> <span class="number">9</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = tf.linspace(<span class="number">0.0</span>,<span class="number">2</span>*<span class="number">3.14</span>,<span class="number">100</span>)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span> <span class="number">0.0634343475</span> <span class="number">0.126868695</span> ... <span class="number">6.15313148</span> <span class="number">6.21656609</span> <span class="number">6.28</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = tf.zeros([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">tf.print(d)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.zeros_like(a,dtype= tf.float32)</span><br><span class="line">tf.print(a)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = tf.fill([<span class="number">3</span>,<span class="number">2</span>],<span class="number">5</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">5</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均匀分布随机</span></span><br><span class="line">tf.random.set_seed(<span class="number">1.0</span>)</span><br><span class="line">a = tf.random.uniform([<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1.65130854</span> <span class="number">9.01481247</span> <span class="number">6.30974197</span> <span class="number">4.34546089</span> <span class="number">2.9193902</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正态分布随机</span></span><br><span class="line">b = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</span><br><span class="line">tf.print(b)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.403087884</span> <span class="number">-1.0880208</span> <span class="number">-0.0630953535</span>]</span><br><span class="line"> [<span class="number">1.33655667</span> <span class="number">0.711760104</span> <span class="number">-0.489286453</span>]</span><br><span class="line"> [<span class="number">-0.764221311</span> <span class="number">-1.03724861</span> <span class="number">-1.25193381</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正态分布随机，剔除2倍方差以外数据重新生成</span></span><br><span class="line">c = tf.random.truncated_normal((<span class="number">5</span>,<span class="number">5</span>), mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">-0.457012236</span> <span class="number">-0.406867266</span> <span class="number">0.728577733</span> <span class="number">-0.892977774</span> <span class="number">-0.369404584</span>]</span><br><span class="line"> [<span class="number">0.323488563</span> <span class="number">1.19383323</span> <span class="number">0.888299048</span> <span class="number">1.25985599</span> <span class="number">-1.95951891</span>]</span><br><span class="line"> [<span class="number">-0.202244401</span> <span class="number">0.294496894</span> <span class="number">-0.468728036</span> <span class="number">1.29494202</span> <span class="number">1.48142183</span>]</span><br><span class="line"> [<span class="number">0.0810953453</span> <span class="number">1.63843894</span> <span class="number">0.556645</span> <span class="number">0.977199793</span> <span class="number">-1.17777884</span>]</span><br><span class="line"> [<span class="number">1.67368948</span> <span class="number">0.0647980496</span> <span class="number">-0.705142677</span> <span class="number">-0.281972528</span> <span class="number">0.126546144</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特殊矩阵</span></span><br><span class="line">I = tf.eye(<span class="number">3</span>,<span class="number">3</span>) <span class="comment">#单位矩阵</span></span><br><span class="line">tf.print(I)</span><br><span class="line">tf.print(<span class="string">" "</span>)</span><br><span class="line">t = tf.linalg.diag([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment">#对角阵</span></span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line"> </span><br><span class="line">[[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure><h4 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h4><p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。</p><p>对于tf.Variable,可以通过索引和切片对部分元素进行修改。</p><p>对于提取张量的连续子区域，也可以使用tf.slice.</p><p>此外，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p><p>tf.boolean_mask功能最为强大，它可以实现tf.gather,tf.gather_nd的功能，并且tf.boolean_mask还可以实现布尔索引。</p><p>如果要通过修改张量的某些元素得到新的张量，可以使用tf.where，tf.scatter_nd。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">3</span>)</span><br><span class="line">t = tf.random.uniform([<span class="number">5</span>,<span class="number">5</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(t)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">4</span> <span class="number">7</span> <span class="number">4</span> <span class="number">2</span> <span class="number">9</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第0行</span></span><br><span class="line">tf.print(t[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">4</span> <span class="number">7</span> <span class="number">4</span> <span class="number">2</span> <span class="number">9</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#倒数第一行</span></span><br><span class="line">tf.print(t[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">3</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行第3列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">tf.print(t[<span class="number">1</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行至第3行</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:])</span><br><span class="line">tf.print(tf.slice(t,[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">5</span>])) <span class="comment">#tf.slice(input,begin_vector,size_vector)</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line">[[<span class="number">9</span> <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">2</span> <span class="number">7</span> <span class="number">4</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">6</span> <span class="number">9</span> <span class="number">7</span> <span class="number">2</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第1行至最后一行，第0列到最后一列每隔两列取一列</span></span><br><span class="line">tf.print(t[<span class="number">1</span>:<span class="number">4</span>,:<span class="number">4</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">9</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">7</span> <span class="number">7</span>]</span><br><span class="line"> [<span class="number">9</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#对变量来说，还可以使用索引和切片修改部分元素</span><br><span class="line">x = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.<span class="built_in">float</span>32)</span><br><span class="line">x[<span class="number">1</span>,:].assign(tf.constant([<span class="number">0.0</span>,<span class="number">0.0</span>]))</span><br><span class="line">tf.print(x)</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],minval=<span class="number">0</span>,maxval=<span class="number">10</span>,dtype=tf.int32)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">7</span> <span class="number">3</span> <span class="number">9</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">0</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">9</span> <span class="number">6</span> <span class="number">7</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">1</span> <span class="number">3</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line">  [<span class="number">3</span> <span class="number">1</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">4</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">2</span> <span class="number">2</span>]</span><br><span class="line">  [<span class="number">7</span> <span class="number">9</span> <span class="number">5</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#省略号可以表示多个冒号</span></span><br><span class="line">tf.<span class="builtin-name">print</span>(a[<span class="built_in">..</span>.,1])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">3</span> <span class="number">0</span> <span class="number">6</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">8</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">2</span> <span class="number">9</span>]]</span><br></pre></td></tr></table></figure><p>以上切片方式相对规则，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。</p><p>考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = tf.random.uniform((<span class="number">4</span>,<span class="number">10</span>,<span class="number">7</span>),minval=<span class="number">0</span>,maxval=<span class="number">100</span>,dtype=tf.<span class="built_in">int</span>32)</span><br><span class="line">tf.print(scores)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">8</span> <span class="number">36</span> <span class="number">94</span> ... <span class="number">13</span> <span class="number">78</span> <span class="number">41</span>]</span><br><span class="line">  [<span class="number">77</span> <span class="number">53</span> <span class="number">51</span> ... <span class="number">22</span> <span class="number">91</span> <span class="number">56</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">11</span> <span class="number">19</span> <span class="number">26</span> ... <span class="number">89</span> <span class="number">86</span> <span class="number">68</span>]</span><br><span class="line">  [<span class="number">60</span> <span class="number">72</span> <span class="number">0</span> ... <span class="number">11</span> <span class="number">26</span> <span class="number">15</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">83</span> <span class="number">36</span> <span class="number">31</span> ... <span class="number">75</span> <span class="number">38</span> <span class="number">85</span>]</span><br><span class="line">  [<span class="number">54</span> <span class="number">26</span> <span class="number">67</span> ... <span class="number">60</span> <span class="number">68</span> <span class="number">98</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">20</span> <span class="number">5</span> <span class="number">18</span> ... <span class="number">32</span> <span class="number">45</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">72</span> <span class="number">52</span> <span class="number">81</span> ... <span class="number">88</span> <span class="number">41</span> <span class="number">20</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">54</span> ... <span class="number">43</span> <span class="number">98</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">66</span> <span class="number">53</span> ... <span class="number">97</span> <span class="number">75</span> <span class="number">77</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">6</span> <span class="number">74</span> <span class="number">3</span> ... <span class="number">53</span> <span class="number">65</span> <span class="number">43</span>]</span><br><span class="line">  [<span class="number">98</span> <span class="number">36</span> <span class="number">72</span> ... <span class="number">33</span> <span class="number">36</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">35</span> <span class="number">8</span> <span class="number">82</span> ... <span class="number">11</span> <span class="number">59</span> <span class="number">97</span>]</span><br><span class="line">  [<span class="number">44</span> <span class="number">6</span> <span class="number">99</span> ... <span class="number">81</span> <span class="number">60</span> <span class="number">27</span>]</span><br><span class="line">  ...</span><br><span class="line">  [<span class="number">76</span> <span class="number">26</span> <span class="number">35</span> ... <span class="number">51</span> <span class="number">8</span> <span class="number">17</span>]</span><br><span class="line">  [<span class="number">33</span> <span class="number">52</span> <span class="number">53</span> ... <span class="number">78</span> <span class="number">37</span> <span class="number">31</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#抽取每个班级第<span class="number">0</span>个学生，第<span class="number">5</span>个学生，第<span class="number">9</span>个学生的全部成绩</span><br><span class="line">p = tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>)</span><br><span class="line">tf.print(p)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#抽取每个班级第<span class="number">0</span>个学生，第<span class="number">5</span>个学生，第<span class="number">9</span>个学生的第<span class="number">1</span>门课程，第<span class="number">3</span>门课程，第<span class="number">6</span>门课程成绩</span><br><span class="line">q = tf.gather(tf.gather(scores,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>],axis=<span class="number">1</span>),[<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>],axis=<span class="number">2</span>)</span><br><span class="line">tf.print(q)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">82</span> <span class="number">55</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">80</span> <span class="number">46</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">99</span> <span class="number">58</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">73</span> <span class="number">48</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">10</span> <span class="number">38</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">21</span> <span class="number">86</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">80</span> <span class="number">57</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">12</span> <span class="number">34</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">78</span> <span class="number">71</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">57</span> <span class="number">75</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">41</span> <span class="number">47</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">27</span> <span class="number">96</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 抽取第<span class="number">0</span>个班级第<span class="number">0</span>个学生，第<span class="number">2</span>个班级的第<span class="number">4</span>个学生，第<span class="number">3</span>个班级的第<span class="number">6</span>个学生的全部成绩</span><br><span class="line">#indices的长度为采样样本的个数，每个元素为采样位置的坐标</span><br><span class="line">s = tf.gather_nd(scores,indices = [(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">6</span>)])</span><br><span class="line">s</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">7</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">52</span>, <span class="number">82</span>, <span class="number">66</span>, <span class="number">55</span>, <span class="number">17</span>, <span class="number">86</span>, <span class="number">14</span>],</span><br><span class="line">       [<span class="number">99</span>, <span class="number">94</span>, <span class="number">46</span>, <span class="number">70</span>,  <span class="number">1</span>, <span class="number">63</span>, <span class="number">41</span>],</span><br><span class="line">       [<span class="number">46</span>, <span class="number">83</span>, <span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>, <span class="number">85</span>, <span class="number">17</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><p>以上tf.gather和tf.gather_nd的功能也可以用tf.boolean_mask来实现。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</span></span><br><span class="line">p = tf.boolean_mask(scores,[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,</span><br><span class="line">                            <span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],<span class="attribute">axis</span>=1)</span><br><span class="line">tf.<span class="builtin-name">print</span>(p)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">80</span> <span class="number">70</span> ... <span class="number">72</span> <span class="number">63</span> <span class="number">96</span>]</span><br><span class="line">  [<span class="number">24</span> <span class="number">99</span> <span class="number">38</span> ... <span class="number">97</span> <span class="number">44</span> <span class="number">74</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">79</span> <span class="number">73</span> <span class="number">73</span> ... <span class="number">35</span> <span class="number">3</span> <span class="number">81</span>]</span><br><span class="line">  [<span class="number">46</span> <span class="number">10</span> <span class="number">94</span> ... <span class="number">23</span> <span class="number">18</span> <span class="number">92</span>]</span><br><span class="line">  [<span class="number">0</span> <span class="number">21</span> <span class="number">89</span> ... <span class="number">53</span> <span class="number">10</span> <span class="number">90</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">52</span> <span class="number">80</span> <span class="number">22</span> ... <span class="number">29</span> <span class="number">25</span> <span class="number">60</span>]</span><br><span class="line">  [<span class="number">19</span> <span class="number">12</span> <span class="number">23</span> ... <span class="number">87</span> <span class="number">86</span> <span class="number">25</span>]</span><br><span class="line">  [<span class="number">61</span> <span class="number">78</span> <span class="number">70</span> ... <span class="number">7</span> <span class="number">59</span> <span class="number">21</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">56</span> <span class="number">57</span> <span class="number">45</span> ... <span class="number">23</span> <span class="number">15</span> <span class="number">3</span>]</span><br><span class="line">  [<span class="number">6</span> <span class="number">41</span> <span class="number">79</span> ... <span class="number">97</span> <span class="number">43</span> <span class="number">13</span>]</span><br><span class="line">  [<span class="number">71</span> <span class="number">27</span> <span class="number">44</span> ... <span class="number">0</span> <span class="number">52</span> <span class="number">16</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩</span></span><br><span class="line">s = tf.boolean_mask(scores,</span><br><span class="line">    [[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>],</span><br><span class="line">     [<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.<span class="builtin-name">print</span>(s)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">52</span> <span class="number">82</span> <span class="number">66</span> ... <span class="number">17</span> <span class="number">86</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">99</span> <span class="number">94</span> <span class="number">46</span> ... <span class="number">1</span> <span class="number">63</span> <span class="number">41</span>]</span><br><span class="line"> [<span class="number">46</span> <span class="number">83</span> <span class="number">70</span> ... <span class="number">90</span> <span class="number">85</span> <span class="number">17</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#利用tf.<span class="built_in">bool</span>ean_mask可以实现布尔索引</span><br><span class="line"></span><br><span class="line">#找到矩阵中小于<span class="number">0</span>的元素</span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.<span class="built_in">float</span>32)</span><br><span class="line">tf.print(c,<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">tf.print(tf.<span class="built_in">bool</span>ean_mask(c,c&lt;<span class="number">0</span>),<span class="string">"\n"</span>) </span><br><span class="line">tf.print(c[c&lt;<span class="number">0</span>]) #布尔索引，为<span class="built_in">bool</span>ean_mask的语法糖形式</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">-1</span> <span class="number">1</span> <span class="number">-1</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">2</span> <span class="number">-2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">-3</span> <span class="number">3</span>]] </span><br><span class="line"></span><br><span class="line">[<span class="number">-1</span> <span class="number">-1</span> <span class="number">-2</span> <span class="number">-3</span>] </span><br><span class="line"></span><br><span class="line">[<span class="number">-1</span> <span class="number">-1</span> <span class="number">-2</span> <span class="number">-3</span>]</span><br></pre></td></tr></table></figure><p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。</p><p>如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。</p><p>tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。</p><p>tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，</p><p>而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#找到张量中小于<span class="number">0</span>的元素,将其换成np.nan得到新的张量</span><br><span class="line">#tf.where和np.where作用类似，可以理解为<span class="keyword">if</span>的张量版本</span><br><span class="line"></span><br><span class="line">c = tf.constant([[<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">-2</span>],[<span class="number">3</span>,<span class="number">-3</span>,<span class="number">3</span>]],dtype=tf.<span class="built_in">float</span>32)</span><br><span class="line">d = tf.where(c&lt;<span class="number">0</span>,tf.fill(c.shape,np.nan),c) </span><br><span class="line">d</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[nan,  <span class="number">1.</span>, nan],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, nan],</span><br><span class="line">       [ <span class="number">3.</span>, nan,  <span class="number">3.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#如果<span class="keyword">where</span>只有一个参数，将返回所有满足条件的位置坐标</span><br><span class="line"><span class="built_in">indices</span> = tf.<span class="keyword">where</span>(<span class="built_in">c</span>&lt;<span class="number">0</span>)</span><br><span class="line"><span class="built_in">indices</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">4</span>, <span class="number">2</span>), dtype=<span class="built_in">int</span>64, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">1</span>]])&gt;</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#将张量的第[<span class="number">0</span>,<span class="number">0</span>]和[<span class="number">2</span>,<span class="number">1</span>]两个位置元素替换为<span class="number">0</span>得到新的张量</span><br><span class="line">d = c - tf.scatter_nd([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>]],[c[<span class="number">0</span>,<span class="number">0</span>],c[<span class="number">2</span>,<span class="number">1</span>]],c.shape)</span><br><span class="line">d</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">0.</span>,  <span class="number">3.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#scatter_nd的作用和gather_nd有些相反</span><br><span class="line">#可以将某些值插入到一个给定shape的全<span class="number">0</span>的张量的指定位置处。</span><br><span class="line">indices = tf.where(c&lt;<span class="number">0</span>)</span><br><span class="line">tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)</span><br><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">-1.</span>,  <span class="number">0.</span>, <span class="number">-1.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">-2.</span>],</span><br><span class="line">       [ <span class="number">0.</span>, <span class="number">-3.</span>,  <span class="number">0.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><h4 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h4><p>维度变换相关函数主要有 tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.</p><p>tf.reshape 可以改变张量的形状。</p><p>tf.squeeze 可以减少维度。</p><p>tf.expand_dims 可以增加维度。</p><p>tf.transpose 可以交换维度。</p><p>tf.reshape可以改变张量的形状，但是其本质上不会改变张量元素的存储顺序，所以，该操作实际上非常迅速，并且是可逆的。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform(shape=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>],</span><br><span class="line">                      minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.<span class="built_in">int</span>32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line">tf.print(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改成 （3,6）形状的张量</span></span><br><span class="line">b = tf.reshape(a,[3,6])</span><br><span class="line">tf.<span class="builtin-name">print</span>(b.shape)</span><br><span class="line">tf.<span class="builtin-name">print</span>(b)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">[[<span class="number">135</span> <span class="number">178</span> <span class="number">26</span> <span class="number">116</span> <span class="number">29</span> <span class="number">224</span>]</span><br><span class="line"> [<span class="number">179</span> <span class="number">219</span> <span class="number">153</span> <span class="number">209</span> <span class="number">111</span> <span class="number">215</span>]</span><br><span class="line"> [<span class="number">39</span> <span class="number">7</span> <span class="number">138</span> <span class="number">129</span> <span class="number">59</span> <span class="number">205</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 改回成 [<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>] 形状的张量</span><br><span class="line">c = tf.reshape(b,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line">tf.print(c)</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">   [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">   [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">   [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">   [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">   [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">   [<span class="number">59</span> <span class="number">205</span>]]]]</span><br></pre></td></tr></table></figure><p>如果张量在某个维度上只有一个元素，利用tf.squeeze可以消除这个维度。</p><p>和tf.reshape相似，它本质上不会改变张量元素的存储顺序。</p><p>张量的各个元素在内存中是线性存储的，其一般规律是，同一层级中的相邻元素的物理地址也相邻。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="keyword">tf</span>.squeeze(<span class="keyword">a</span>)</span><br><span class="line"><span class="keyword">tf</span>.<span class="keyword">print</span>(s.shape)</span><br><span class="line"><span class="keyword">tf</span>.<span class="keyword">print</span>(s)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">[[[<span class="number">135</span> <span class="number">178</span>]</span><br><span class="line">  [<span class="number">26</span> <span class="number">116</span>]</span><br><span class="line">  [<span class="number">29</span> <span class="number">224</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">179</span> <span class="number">219</span>]</span><br><span class="line">  [<span class="number">153</span> <span class="number">209</span>]</span><br><span class="line">  [<span class="number">111</span> <span class="number">215</span>]]</span><br><span class="line"></span><br><span class="line"> [[<span class="number">39</span> <span class="number">7</span>]</span><br><span class="line">  [<span class="number">138</span> <span class="number">129</span>]</span><br><span class="line">  [<span class="number">59</span> <span class="number">205</span>]]]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = tf.expand_dims(s,axis=<span class="number">0</span>) #在第<span class="number">0</span>维插入长度为<span class="number">1</span>的一个维度</span><br><span class="line">d</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[[[<span class="number">135</span>, <span class="number">178</span>],</span><br><span class="line">         [ <span class="number">26</span>, <span class="number">116</span>],</span><br><span class="line">         [ <span class="number">29</span>, <span class="number">224</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">179</span>, <span class="number">219</span>],</span><br><span class="line">         [<span class="number">153</span>, <span class="number">209</span>],</span><br><span class="line">         [<span class="number">111</span>, <span class="number">215</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">39</span>,   <span class="number">7</span>],</span><br><span class="line">         [<span class="number">138</span>, <span class="number">129</span>],</span><br><span class="line">         [ <span class="number">59</span>, <span class="number">205</span>]]]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><p>tf.transpose可以交换张量的维度，与tf.reshape不同，它会改变张量元素的存储顺序。</p><p>tf.transpose常用于图片存储格式的变换上。</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Batch,Height,Width,Channel</span><br><span class="line">a = tf.random.uniform(shape=[<span class="number">100</span>,<span class="number">600</span>,<span class="number">600</span>,<span class="number">4</span>],minval=<span class="number">0</span>,maxval=<span class="number">255</span>,dtype=tf.<span class="built_in">int</span>32)</span><br><span class="line">tf.print(a.shape)</span><br><span class="line"></span><br><span class="line"># 转换成 Channel,Height,Width,Batch</span><br><span class="line">s= tf.transpose(a,perm=[<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>])</span><br><span class="line">tf.print(s.shape)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TensorShape([<span class="number">100</span>, <span class="number">600</span>, <span class="number">600</span>, <span class="number">4</span>])</span><br><span class="line">TensorShape([<span class="number">4</span>, <span class="number">600</span>, <span class="number">600</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><h4 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h4><p>和numpy类似，可以用tf.concat和tf.stack方法对多个张量进行合并，可以用tf.split方法把一个张量分割成多个张量。</p><p>tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="string">[[1.0,2.0],[3.0,4.0]]</span>)</span><br><span class="line">b = tf.constant(<span class="string">[[5.0,6.0],[7.0,8.0]]</span>)</span><br><span class="line">c = tf.constant(<span class="string">[[9.0,10.0],[11.0,12.0]]</span>)</span><br><span class="line"></span><br><span class="line">tf.<span class="built_in">concat</span>([a,b,c],axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">6</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([<span class="selector-tag">a</span>,<span class="selector-tag">b</span>,c],axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">6</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>, <span class="number">11.</span>, <span class="number">12.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">tf</span><span class="selector-class">.stack</span>(<span class="selector-attr">[a,b,c]</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">3.</span>,  <span class="number">4.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack<span class="comment">([a,b,c],axis=1)</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[[ <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">10.</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="string">[[1.0,2.0],[3.0,4.0]]</span>)</span><br><span class="line">b = tf.constant(<span class="string">[[5.0,6.0],[7.0,8.0]]</span>)</span><br><span class="line">c = tf.constant(<span class="string">[[9.0,10.0],[11.0,12.0]]</span>)</span><br><span class="line"></span><br><span class="line">c = tf.<span class="built_in">concat</span>([a,b,c],axis = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf.split(value,num_or_size_splits,axis)</span></span><br><span class="line">tf.split<span class="params">(c,3,<span class="attr">axis</span> = 0)</span>  <span class="comment">#指定分割份数，平均分割</span></span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.split(c,[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],axis = <span class="number">0</span>) #指定每份的记录数量</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">7.</span>, <span class="number">8.</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">11.</span>, <span class="number">12.</span>]], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure><h3 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h3><p>张量的操作主要包括张量的结构操作和张量的数学运算。</p><p>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</p><p>张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。</p><p>本篇我们介绍张量的数学运算。</p><h4 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h4><p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p><p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。</p><p>标量运算符的特点是对张量实施逐元素运算。</p><p>有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p><p>许多标量运算符都在 tf.math模块下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-3</span>,<span class="number">4.0</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">5.0</span>,<span class="number">6</span>],[<span class="number">7.0</span>,<span class="number">8.0</span>]])</span><br><span class="line">a+b  <span class="comment">#运算符重载</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">4.</span>, <span class="number">12.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a-b</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">-4.</span>,  <span class="number">-4.</span>],</span><br><span class="line">       [<span class="number">-10.</span>,  <span class="number">-4.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a*b</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[  <span class="number">5.</span>,  <span class="number">12.</span>],</span><br><span class="line">       [<span class="number">-21.</span>,  <span class="number">32.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a/b</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">0.2</span>       ,  <span class="number">0.33333334</span>],</span><br><span class="line">       [<span class="number">-0.42857143</span>,  <span class="number">0.5</span>       ]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a**<span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">1.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">9.</span>, <span class="number">16.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a**(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a%<span class="number">3</span> <span class="comment">#mod的运算符重载，等价于m = tf.math.mod(a,3)</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>32, numpy=<span class="built_in">array</span>([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a//<span class="number">3</span>  <span class="comment">#地板除法</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">-1.</span>,  <span class="number">1.</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a&gt;=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="type">bool</span>, numpy=</span><br><span class="line"><span class="keyword">array</span>([[<span class="keyword">False</span>,  <span class="keyword">True</span>],</span><br><span class="line">       [<span class="keyword">False</span>,  <span class="keyword">True</span>]])&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a&gt;=<span class="number">2</span>)&amp;(a&lt;=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="type">bool</span>, numpy=</span><br><span class="line"><span class="keyword">array</span>([[<span class="keyword">False</span>,  <span class="keyword">True</span>],</span><br><span class="line">       [<span class="keyword">False</span>, <span class="keyword">False</span>]])&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a&gt;=<span class="number">2</span>)|(a&lt;=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&lt;tf.Tensor:</span> <span class="string">shape=(2,</span> <span class="number">2</span><span class="string">),</span> <span class="string">dtype=bool,</span> <span class="string">numpy=</span></span><br><span class="line"><span class="string">array([[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">],</span></span><br><span class="line">       <span class="string">[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">]])&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a==<span class="number">5</span> <span class="comment">#tf.equal(a,5)</span></span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=bool, <span class="attribute">numpy</span>=array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>])&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.sqrt(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">1.</span>       , <span class="number">1.4142135</span>],</span><br><span class="line">       [      nan, <span class="number">2.</span>       ]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.0</span>,<span class="number">8.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">5.0</span>,<span class="number">6.0</span>])</span><br><span class="line">c = tf.constant([<span class="number">6.0</span>,<span class="number">7.0</span>])</span><br><span class="line">tf.add_n([a,b,c])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>,), dtype=<span class="built_in">float</span>32, numpy=<span class="built_in">array</span>([<span class="number">12.</span>, <span class="number">21.</span>], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.print(tf.maximum(a,b))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">5</span> <span class="number">8</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.print(tf.minimum(a,b))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">6</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">2.6</span>,<span class="number">-2.7</span>])</span><br><span class="line"></span><br><span class="line">tf.print(tf.math.round(x)) <span class="comment">#保留整数部分，四舍五入</span></span><br><span class="line">tf.print(tf.math.floor(x)) <span class="comment">#保留整数部分，向下归整</span></span><br><span class="line">tf.print(tf.math.ceil(x))  <span class="comment">#保留整数部分，向上归整</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">3</span> <span class="number">-3</span>]</span><br><span class="line">[<span class="number">2</span> <span class="number">-3</span>]</span><br><span class="line">[<span class="number">3</span> <span class="number">-2</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 幅值裁剪</span></span><br><span class="line">x = tf.constant([<span class="number">0.9</span>,<span class="number">-0.8</span>,<span class="number">100.0</span>,<span class="number">-20.0</span>,<span class="number">0.7</span>])</span><br><span class="line">y = tf.clip_by_value(x,clip_value_min=<span class="number">-1</span>,clip_value_max=<span class="number">1</span>)</span><br><span class="line">z = tf.clip_by_norm(x,clip_norm = <span class="number">3</span>)</span><br><span class="line">tf.print(y)</span><br><span class="line">tf.print(z)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0.9</span> <span class="number">-0.8</span> <span class="number">1</span> <span class="number">-1</span> <span class="number">0.7</span>]</span><br><span class="line">[<span class="number">0.0264732055</span> <span class="number">-0.0235317405</span> <span class="number">2.94146752</span> <span class="number">-0.588293493</span> <span class="number">0.0205902718</span>]</span><br></pre></td></tr></table></figure><h4 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h4><p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。<br>许多向量运算符都以reduce开头。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#向量reduce</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.reduce_sum(a))</span><br><span class="line">tf.print(tf.reduce_mean(a))</span><br><span class="line">tf.print(tf.reduce_max(a))</span><br><span class="line">tf.print(tf.reduce_min(a))</span><br><span class="line">tf.print(tf.reduce_prod(a))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">45</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">362880</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#张量指定维度进行reduce</span></span><br><span class="line">b = tf.reshape(a,(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br><span class="line">tf.print(tf.reduce_sum(b, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">6</span>]</span><br><span class="line"> [<span class="number">15</span>]</span><br><span class="line"> [<span class="number">24</span>]]</span><br><span class="line">[[<span class="number">12</span> <span class="number">15</span> <span class="number">18</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#bool类型的reduce</span></span><br><span class="line">p = tf.constant([<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>])</span><br><span class="line">q = tf.constant([<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>])</span><br><span class="line">tf.print(tf.reduce_all(p))</span><br><span class="line">tf.print(tf.reduce_any(q))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用tf.foldr实现tf.reduce_sum</span></span><br><span class="line">s = tf.foldr(<span class="keyword">lambda</span> a,b:a+b,tf.range(<span class="number">10</span>)) </span><br><span class="line">tf.print(s)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">45</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#cum扫描累积</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.math.cumsum(a))</span><br><span class="line">tf.print(tf.math.cumprod(a))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span> <span class="number">3</span> <span class="number">6</span> ... <span class="number">28</span> <span class="number">36</span> <span class="number">45</span>]</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">6</span> ... <span class="number">5040</span> <span class="number">40320</span> <span class="number">362880</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#arg最大最小值索引</span></span><br><span class="line">a = tf.range(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">tf.print(tf.argmax(a))</span><br><span class="line">tf.print(tf.argmin(a))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf.math.top_k可以用于对张量排序</span></span><br><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">values,indices = tf.math.top_k(a,<span class="number">3</span>,sorted=<span class="literal">True</span>)</span><br><span class="line">tf.print(values)</span><br><span class="line">tf.print(indices)</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用tf.math.top_k可以在TensorFlow中实现KNN算法</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">8</span> <span class="number">7</span> <span class="number">5</span>]</span><br><span class="line">[<span class="number">5</span> <span class="number">2</span> <span class="number">3</span>]</span><br></pre></td></tr></table></figure><h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><p>矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。</p><p>矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p><p>除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵乘法</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">2</span>]])</span><br><span class="line">a@b  <span class="comment">#等价于tf.matmul(a,b)</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">8</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵转置</span></span><br><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.transpose(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">4</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵逆，必须为tf.float32或tf.double类型</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.inv(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=<span class="built_in">float</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">-2.0000002</span> ,  <span class="number">1.0000001</span> ],</span><br><span class="line">       [ <span class="number">1.5000001</span> , <span class="number">-0.50000006</span>]], dtype=<span class="built_in">float</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵求trace</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]],dtype = tf.float32)</span><br><span class="line">tf.linalg.trace(a)</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">5.0</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵求范数</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.norm(a)</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">5.477226</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵行列式</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.det(a)</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">-2.0</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵特征值</span></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2</span>],[<span class="number">-5</span>,<span class="number">4</span>]])</span><br><span class="line">tf.linalg.eigvals(a)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>,), dtype=complex64, numpy=<span class="built_in">array</span>([<span class="number">2.4999995</span>+<span class="number">2.7838817</span>j, <span class="number">2.5</span>      <span class="number">-2.783882</span>j ], dtype=complex64)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r</span></span><br><span class="line"><span class="comment">#QR分解实际上是对矩阵a实施Schmidt正交化得到q</span></span><br><span class="line"></span><br><span class="line">a = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>]],dtype = tf.float32)</span><br><span class="line">q,r = tf.linalg.qr(a)</span><br><span class="line">tf.print(q)</span><br><span class="line">tf.print(r)</span><br><span class="line">tf.print(q@r)</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">-0.316227794</span> <span class="number">-0.948683321</span>]</span><br><span class="line"> [<span class="number">-0.948683321</span> <span class="number">0.316227734</span>]]</span><br><span class="line">[[<span class="number">-3.1622777</span> <span class="number">-4.4271884</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">-0.632455349</span>]]</span><br><span class="line">[[<span class="number">1.00000012</span> <span class="number">1.99999976</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵svd分解</span></span><br><span class="line"><span class="comment">#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积</span></span><br><span class="line"><span class="comment">#svd常用于矩阵压缩和降维</span></span><br><span class="line"></span><br><span class="line">a  = tf.constant([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">3.0</span>,<span class="number">4.0</span>],[<span class="number">5.0</span>,<span class="number">6.0</span>]], dtype = tf.float32)</span><br><span class="line">s,u,v = tf.linalg.svd(a)</span><br><span class="line">tf.print(u,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(s,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(v,<span class="string">"\n"</span>)</span><br><span class="line">tf.print(u@tf.linalg.diag(s)@tf.transpose(v))</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用svd分解可以在TensorFlow中实现主成分分析降维</span></span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.229847744</span> <span class="number">-0.88346082</span>]</span><br><span class="line"> [<span class="number">0.524744868</span> <span class="number">-0.240782902</span>]</span><br><span class="line"> [<span class="number">0.819642067</span> <span class="number">0.401896209</span>]] </span><br><span class="line"></span><br><span class="line">[<span class="number">9.52551842</span> <span class="number">0.51429987</span>] </span><br><span class="line"></span><br><span class="line">[[<span class="number">0.619629562</span> <span class="number">0.784894466</span>]</span><br><span class="line"> [<span class="number">0.784894466</span> <span class="number">-0.619629562</span>]] </span><br><span class="line"></span><br><span class="line">[[<span class="number">1.00000119</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3.00000095</span> <span class="number">4.00000048</span>]</span><br><span class="line"> [<span class="number">5.00000143</span> <span class="number">6.00000095</span>]]</span><br></pre></td></tr></table></figure><h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p>TensorFlow的广播规则和numpy是一样的:</p><ul><li>1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li><li>2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li><li>3、如果两个张量在所有维度上都是相容的，它们就能使用广播。</li><li>4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li><li>5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li></ul><p>tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.constant([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]])</span><br><span class="line">b + a  <span class="comment">#等价于 b + tf.broadcast_to(a,b.shape)</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.broadcast_to(a,b.shape)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，静态形状，TensorShape类型参数</span></span><br><span class="line">tf.broadcast_static_shape(a.shape,b.shape)</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">TensorShape</span><span class="params">([<span class="number">3</span>, <span class="number">3</span>])</span></span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算广播后计算结果的形状，动态形状，Tensor类型参数</span></span><br><span class="line">c = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">d = tf.constant([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line">tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">2</span>,), dtype=<span class="built_in">int</span>32, numpy=<span class="built_in">array</span>([<span class="number">3</span>, <span class="number">3</span>], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#广播效果</span></span><br><span class="line">c+d <span class="comment">#等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=<span class="built_in">int</span>32, numpy=</span><br><span class="line"><span class="built_in">array</span>([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><h3 id="AutoGraph的使用规范"><a href="#AutoGraph的使用规范" class="headerlink" title="AutoGraph的使用规范"></a>AutoGraph的使用规范</h3><p>有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。</p><p>TensorFlow 2.0主要使用的是动态计算图和Autograph。</p><p>动态计算图易于调试，编码效率较高，但执行效率偏低。</p><p>静态计算图执行效率很高，但较难调试。</p><p>而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。</p><p>当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。</p><p>我们将着重介绍Autograph的编码规范和Autograph转换成静态图的原理。</p><p>并介绍使用tf.Module来更好地构建Autograph。</p><p>本篇我们介绍使用Autograph的编码规范。</p><h4 id="Autograph编码规范总结"><a href="#Autograph编码规范总结" class="headerlink" title="Autograph编码规范总结"></a>Autograph编码规范总结</h4><ul><li><p>被 <code>@tf.function</code> 修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用 <code>tf.print</code>而不是print，使用 <code>tf.range</code> 而不是range，使用 <code>tf.constant(True)</code> 而不是True.</p></li><li><p>避免在 <code>@tf.function</code> 修饰的函数内部定义 <code>tf.Variable</code>.</p></li><li><p>被 <code>@tf.function</code>修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</p></li></ul><h4 id="Autograph编码规范解析"><a href="#Autograph编码规范解析" class="headerlink" title="Autograph编码规范解析"></a>Autograph编码规范解析</h4><p> <strong>被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">np_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = np.random.randn(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">    tf.print(a)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf_random</span><span class="params">()</span>:</span></span><br><span class="line">    a = tf.random.normal((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">    tf.print(a)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#np_random每次执行都是一样的结果。</span></span><br><span class="line">np_random()</span><br><span class="line">np_random()</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">array</span>([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br><span class="line"><span class="built_in">array</span>([[ <span class="number">0.22619201</span>, <span class="number">-0.4550123</span> , <span class="number">-0.42587565</span>],</span><br><span class="line">       [ <span class="number">0.05429906</span>,  <span class="number">0.2312667</span> , <span class="number">-1.44819738</span>],</span><br><span class="line">       [ <span class="number">0.36571796</span>,  <span class="number">1.45578986</span>, <span class="number">-1.05348983</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tf_random每次执行都会有重新生成随机数。</span></span><br><span class="line">tf_random()</span><br><span class="line">tf_random()</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">-1.38956189</span> <span class="number">-0.394843668</span> <span class="number">0.420657277</span>]</span><br><span class="line"> [<span class="number">2.87235498</span> <span class="number">-1.33740318</span> <span class="number">-0.533843279</span>]</span><br><span class="line"> [<span class="number">0.918233037</span> <span class="number">0.118598573</span> <span class="number">-0.399486482</span>]]</span><br><span class="line">[[<span class="number">-0.858178258</span> <span class="number">1.67509317</span> <span class="number">0.511889517</span>]</span><br><span class="line"> [<span class="number">-0.545829177</span> <span class="number">-2.20118237</span> <span class="number">-0.968222201</span>]</span><br><span class="line"> [<span class="number">0.733958483</span> <span class="number">-0.61904633</span> <span class="number">0.77440238</span>]]</span><br></pre></td></tr></table></figure><p><strong>避免在@tf.function修饰的函数内部定义tf.Variable.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 避免在@tf.function修饰的函数内部定义tf.Variable.</span></span><br><span class="line"></span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outer_var</span><span class="params">()</span>:</span></span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line">outer_var()</span><br><span class="line">outer_var()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inner_var</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.Variable(<span class="number">1.0</span>,dtype = tf.float32)</span><br><span class="line">    x.assign_add(<span class="number">1.0</span>)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行将报错</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br><span class="line"><span class="comment">#inner_var()</span></span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">ValueError                                Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-<span class="number">12</span>-c95a7c3c1ddd&gt; <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">      <span class="number">7</span></span><br><span class="line">      <span class="number">8</span> #执行将报错</span><br><span class="line">----&gt; <span class="number">9</span> inner<span class="constructor">_var()</span></span><br><span class="line">     <span class="number">10</span> inner<span class="constructor">_var()</span></span><br><span class="line"></span><br><span class="line">~/anaconda3/lib/python3.<span class="number">7</span>/site-packages/tensorflow_core/python/eager/def_function.py <span class="keyword">in</span> <span class="constructor">__call__(<span class="params">self</span>, <span class="operator">*</span><span class="params">args</span>, <span class="operator">**</span><span class="params">kwds</span>)</span></span><br><span class="line">    <span class="number">566</span>         xla_context.<span class="constructor">Exit()</span></span><br><span class="line">    <span class="number">567</span>     <span class="keyword">else</span>:</span><br><span class="line">--&gt; <span class="number">568</span>       result = self.<span class="constructor">_call(<span class="operator">*</span><span class="params">args</span>, <span class="operator">**</span><span class="params">kwds</span>)</span></span><br><span class="line">    <span class="number">569</span></span><br><span class="line">    <span class="number">570</span>     <span class="keyword">if</span> tracing_count<span class="operator"> == </span>self.<span class="constructor">_get_tracing_count()</span>:</span><br><span class="line">......</span><br><span class="line">ValueError: tf.<span class="keyword">function</span>-decorated <span class="keyword">function</span> tried <span class="keyword">to</span> create variables on non-first call.</span><br></pre></td></tr></table></figure><p><strong>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等结构类型变量。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment">#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.0&gt;]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor_list = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function #加上这一行切换成Autograph结果将不符合预期！！！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_tensor</span><span class="params">(x)</span>:</span></span><br><span class="line">    tensor_list.append(x)</span><br><span class="line">    <span class="keyword">return</span> tensor_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">append_tensor(tf.constant(<span class="number">5.0</span>))</span><br><span class="line">append_tensor(tf.constant(<span class="number">6.0</span>))</span><br><span class="line">print(tensor_list)</span><br></pre></td></tr></table></figure><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[&lt;tf.Tensor 'x:0' shape=() dtype=float32&gt;]</span></span><br></pre></td></tr></table></figure><h3 id="AutoGraph的机制原理"><a href="#AutoGraph的机制原理" class="headerlink" title="AutoGraph的机制原理"></a>AutoGraph的机制原理</h3><p>有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。</p><p>TensorFlow 2.0主要使用的是动态计算图和Autograph。</p><p>动态计算图易于调试，编码效率较高，但执行效率偏低。</p><p>静态计算图执行效率很高，但较难调试。</p><p>而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。</p><p>当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。</p><p>我们会介绍Autograph的编码规范和Autograph转换成静态图的原理。</p><p>并介绍使用tf.Module来更好地构建Autograph。</p><p>上篇我们介绍了Autograph的编码规范，本篇我们介绍Autograph的机制原理。</p><h4 id="Autograph的机制原理"><a href="#Autograph的机制原理" class="headerlink" title="Autograph的机制原理"></a>Autograph的机制原理</h4><p><strong>当我们使用@tf.function装饰一个函数的时候，后面到底发生了什么呢？</strong></p><p>例如我们写下如下代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(autograph=True)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myadd</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">        tf.print(i)</span><br><span class="line">    c = a+b</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><p>后面什么都没有发生。仅仅是在Python堆栈中记录了这样一个函数的签名。</p><p><strong>当我们第一次调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p><p>例如我们写下如下代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"hello"</span>),tf.constant(<span class="string">"world"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>发生了2件事情，</p><p>第一件事情是创建计算图。</p><p>即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。<br>在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。<br>主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加<br>tf.control_dependencies指定执行顺序依赖关系。</p><p>相当于在 tensorflow1.0执行了类似下面的语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    a = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    b = tf.placeholder(shape=[],dtype=tf.string)</span><br><span class="line">    cond = <span class="keyword">lambda</span> i: i&lt;tf.constant(<span class="number">3</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i)</span>:</span></span><br><span class="line">        tf.print(i)</span><br><span class="line">        <span class="keyword">return</span>(i+<span class="number">1</span>)</span><br><span class="line">    loop = tf.while_loop(cond,body,loop_vars=[<span class="number">0</span>])</span><br><span class="line">    loop</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies(loop):</span><br><span class="line">        c = tf.strings.join([a,b])</span><br><span class="line">    print(<span class="string">"tracing"</span>)</span><br></pre></td></tr></table></figure><p>第二件事情是执行计算图。</p><p>相当于在 tensorflow1.0中执行了下面的语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=g) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(c,feed_dict=&#123;a:tf.constant(<span class="string">"hello"</span>),b:tf.constant(<span class="string">"world"</span>)&#125;)</span><br></pre></td></tr></table></figure><p>因此我们先看到的是第一个步骤的结果：即Python调用标准输出流打印”tracing”语句。</p><p>然后看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p><p><strong>当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p><p>例如我们写下如下代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="string">"good"</span>),tf.constant(<span class="string">"morning"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>只会发生一件事情，那就是上面步骤的第二步，执行计算图。</p><p>所以这一次我们没有看到打印”tracing”的结果。</p><p><strong>当我们再次用不同的的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？</strong></p><p>例如我们写下如下代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myadd(tf.constant(<span class="number">1</span>),tf.constant(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。</p><p>需要重新做2件事情：创建新的计算图、执行计算图。</p><p>所以我们又会先看到的是第一个步骤的结果：即Python调用标准输出流打印”tracing”语句。</p><p>然后再看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。</p><p><strong>需要注意的是，如果调用被@tf.function装饰的函数时输入的参数不是Tensor类型，则每次都会重新创建计算图。</strong></p><p>例如我们写下如下代码。两次都会重新创建计算图。因此，一般建议调用@tf.function时应传入Tensor类型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">myadd(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line">myadd(<span class="string">"good"</span>,<span class="string">"morning"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">tracing</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="重新理解Autograph的编码规范"><a href="#重新理解Autograph的编码规范" class="headerlink" title="重新理解Autograph的编码规范"></a>重新理解Autograph的编码规范</h4><p>了解了以上Autograph的机制原理，我们也就能够理解Autograph编码规范的3条建议了。</p><ol><li>被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print.</li></ol><p>解释：Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用，普通Python函数是无法嵌入到静态计算图中的，所以<br>在计算图构建好之后再次调用的时候，这些Python函数并没有被计算，而TensorFlow中的函数则可以嵌入到计算图中。使用普通的Python函数会导致<br>被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。</p><ol><li>避免在@tf.function修饰的函数内部定义tf.Variable. </li></ol><p>解释：如果函数内部定义了tf.Variable,那么在【eager执行】时，这种创建tf.Variable的行为在每次函数调用时候都会发生。但是在【静态图执行】时，这种创建tf.Variable的行为只会发生在第一步跟踪Python代码逻辑创建计算图时，这会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。实际上，TensorFlow在这种情况下一般会报错。</p><ol><li>被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。</li></ol><p>解释：静态计算图是被编译成C++代码在TensorFlow内核中执行的。Python中的列表和字典等数据结构变量是无法嵌入到计算图中，它们仅仅能够在创建计算图时被读取，在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的。</p><h3 id="AutoGraph和tf-Module"><a href="#AutoGraph和tf-Module" class="headerlink" title="AutoGraph和tf.Module"></a>AutoGraph和tf.Module</h3><p>有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。</p><p>TensorFlow 2.0主要使用的是动态计算图和Autograph。</p><p>动态计算图易于调试，编码效率较高，但执行效率偏低。</p><p>静态计算图执行效率很高，但较难调试。</p><p>而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。</p><p>当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。</p><p>前面我们介绍了Autograph的编码规范和Autograph转换成静态图的原理。</p><p>本篇我们介绍使用tf.Module来更好地构建Autograph。</p><h4 id="Autograph和tf-Module概述"><a href="#Autograph和tf-Module概述" class="headerlink" title="Autograph和tf.Module概述"></a>Autograph和tf.Module概述</h4><p>前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在@tf.function修饰的函数内部定义tf.Variable. </p><p>但是如果在函数外部定义tf.Variable的话，又会显得这个函数有外部变量依赖，封装不够完美。</p><p>一种简单的思路是定义一个类，并将相关的tf.Variable创建放在类的初始化方法中。而将函数的逻辑放在其他方法中。</p><p>这样一顿猛如虎的操作之后，我们会觉得一切都如同人法地地法天天法道道法自然般的自然。</p><p>惊喜的是，TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。</p><p>实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。</p><p><strong>因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用。</strong></p><h4 id="应用tf-Module封装Autograph"><a href="#应用tf-Module封装Autograph" class="headerlink" title="应用tf.Module封装Autograph"></a>应用tf.Module封装Autograph</h4><p>定义一个简单的function。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line">x = tf.Variable(<span class="number">1.0</span>,dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在tf.function中用input_signature限定输入张量的签名类型：shape和dtype</span></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_print</span><span class="params">(a)</span>:</span></span><br><span class="line">    x.assign_add(a)</span><br><span class="line">    tf.print(x)</span><br><span class="line">    <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add_print(tf.constant(<span class="number">3.0</span>))</span><br><span class="line"><span class="comment">#add_print(tf.constant(3)) #输入不符合张量签名的参数将报错</span></span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><p>下面利用tf.Module的子类化将其封装一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoModule</span><span class="params">(tf.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,init_value = tf.constant<span class="params">(<span class="number">0.0</span>)</span>,name=None)</span>:</span></span><br><span class="line">        super(DemoModule, self).__init__(name=name)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope:  <span class="comment">#相当于with tf.name_scope("demo_module")</span></span><br><span class="line">            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"><span class="meta">    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(self,a)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope:</span><br><span class="line">            self.x.assign_add(a)</span><br><span class="line">            tf.print(self.x)</span><br><span class="line">            <span class="keyword">return</span>(self.x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#执行</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">1.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部变量和全部可训练变量</span></span><br><span class="line">print(demo.variables)</span><br><span class="line">print(demo.trainable_variables)</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br><span class="line">(&lt;tf.Variable <span class="string">'demo_module/Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=6.0&gt;,)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模块中的全部子模块</span></span><br><span class="line">demo.submodules</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型，并指定需要跨平台部署的方法</span></span><br><span class="line">tf.saved_model.save(demo,<span class="string">"./data/demo/1"</span>,signatures = &#123;<span class="string">"serving_default"</span>:demo.addprint&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">demo2 = tf.saved_model.load(<span class="string">"./data/demo/1"</span>)</span><br><span class="line">demo2.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">11</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息，红框标出来的输出信息在模型部署和跨平台使用时有可能会用到</span></span><br><span class="line">!saved_model_cli show --dir ./data/demo/<span class="number">1</span> --all</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/查看模型文件信息.jpg"></p><p>在tensorboard中查看计算图，模块会被添加模块名demo_module,方便层次化呈现计算图结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建日志</span></span><br><span class="line">stamp = datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">logdir = <span class="string">'./data/demomodule/%s'</span> % stamp</span><br><span class="line">writer = tf.summary.create_file_writer(logdir)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开启autograph跟踪</span></span><br><span class="line">tf.summary.trace_on(graph=<span class="literal">True</span>, profiler=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#执行autograph</span></span><br><span class="line">demo = DemoModule(init_value = tf.constant(<span class="number">0.0</span>))</span><br><span class="line">result = demo.addprint(tf.constant(<span class="number">5.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将计算图信息写入日志</span></span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    tf.summary.trace_export(</span><br><span class="line">        name=<span class="string">"demomodule"</span>,</span><br><span class="line">        step=<span class="number">0</span>,</span><br><span class="line">        profiler_outdir=logdir)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动 tensorboard在jupyter中的魔法命令</span></span><br><span class="line">%reload_ext tensorboard</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboard <span class="keyword">import</span> notebook</span><br><span class="line">notebook.list()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">notebook.start(<span class="string">"--logdir ./data/demomodule/"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/demomodule的计算图结构.jpg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>除了利用tf.Module的子类化实现封装，我们也可以通过给tf.Module添加属性的方法进行封装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mymodule = tf.Module()</span><br><span class="line">mymodule.x = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addprint</span><span class="params">(a)</span>:</span></span><br><span class="line">    mymodule.x.assign_add(a)</span><br><span class="line">    tf.print(mymodule.x)</span><br><span class="line">    <span class="keyword">return</span> (mymodule.x)</span><br><span class="line"></span><br><span class="line">mymodule.addprint = addprint</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mymodule.addprint(tf.constant(<span class="number">1.0</span>)).numpy()</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(mymodule.variables)</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&lt;tf.Variable <span class="string">'Variable:0'</span> shape=() <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=0.0&gt;,)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用tf.saved_model 保存模型</span></span><br><span class="line">tf.saved_model.save(mymodule,<span class="string">"./data/mymodule"</span>,</span><br><span class="line">    signatures = &#123;<span class="string">"serving_default"</span>:mymodule.addprint&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载模型</span></span><br><span class="line">mymodule2 = tf.saved_model.load(<span class="string">"./data/mymodule"</span>)</span><br><span class="line">mymodule2.addprint(tf.constant(<span class="number">5.0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Assets written <span class="string">to:</span> .<span class="regexp">/data/</span>mymodule/assets</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><h4 id="tf-Module和tf-keras-Model，tf-keras-layers-Layer"><a href="#tf-Module和tf-keras-Model，tf-keras-layers-Layer" class="headerlink" title="tf.Module和tf.keras.Model，tf.keras.layers.Layer"></a>tf.Module和tf.keras.Model，tf.keras.layers.Layer</h4><p>tf.keras中的模型和层都是继承tf.Module实现的，也具有变量管理和子模块管理功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,losses,metrics</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(issubclass(tf.keras.Model,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.layers.Layer,tf.Module))</span><br><span class="line">print(issubclass(tf.keras.Model,tf.keras.layers.Layer))</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session() </span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>,input_shape = (<span class="number">10</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 4)                 44        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 2)                 10        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 57</span><br><span class="line">Trainable params: 57</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.variables</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable 'dense/kernel:<span class="number">0</span>' shape=(<span class="number">10</span>, <span class="number">4</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.06741005</span>,  <span class="number">0.45534766</span>,  <span class="number">0.5190817</span> , <span class="number">-0.01806331</span>],</span><br><span class="line">        [<span class="number">-0.14258742</span>, <span class="number">-0.49711505</span>,  <span class="number">0.26030976</span>,  <span class="number">0.18607801</span>],</span><br><span class="line">        [<span class="number">-0.62806034</span>,  <span class="number">0.5327399</span> ,  <span class="number">0.42206633</span>,  <span class="number">0.29201728</span>],</span><br><span class="line">        [<span class="number">-0.16602087</span>, <span class="number">-0.18901917</span>,  <span class="number">0.55159235</span>, <span class="number">-0.01091868</span>],</span><br><span class="line">        [ <span class="number">0.04533798</span>,  <span class="number">0.326845</span>  , <span class="number">-0.582667</span>  ,  <span class="number">0.19431782</span>],</span><br><span class="line">        [ <span class="number">0.6494713</span> , <span class="number">-0.16174704</span>,  <span class="number">0.4062966</span> ,  <span class="number">0.48760796</span>],</span><br><span class="line">        [ <span class="number">0.58400524</span>, <span class="number">-0.6280886</span> , <span class="number">-0.11265379</span>, <span class="number">-0.6438277</span> ],</span><br><span class="line">        [ <span class="number">0.26642334</span>,  <span class="number">0.49275804</span>,  <span class="number">0.20793378</span>, <span class="number">-0.43889117</span>],</span><br><span class="line">        [ <span class="number">0.4092741</span> ,  <span class="number">0.09871006</span>, <span class="number">-0.2073121</span> ,  <span class="number">0.26047975</span>],</span><br><span class="line">        [ <span class="number">0.43910992</span>,  <span class="number">0.00199282</span>, <span class="number">-0.07711256</span>, <span class="number">-0.27966842</span>]],</span><br><span class="line">       dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense/bias:<span class="number">0</span>' shape=(<span class="number">4</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_1/kernel:<span class="number">0</span>' shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_1/bias:<span class="number">0</span>' shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_2/kernel:<span class="number">0</span>' shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_2/bias:<span class="number">0</span>' shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.layers[<span class="number">0</span>].trainable = <span class="literal">False</span> <span class="comment">#冻结第0层的变量,使其不可训练</span></span><br><span class="line">model.trainable_variables</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tf.Variable 'dense_1/kernel:<span class="number">0</span>' shape=(<span class="number">4</span>, <span class="number">2</span>) dtype=float32, numpy=</span><br><span class="line"> array([[ <span class="number">0.5022683</span> , <span class="number">-0.0507431</span> ],</span><br><span class="line">        [<span class="number">-0.61540484</span>,  <span class="number">0.9369011</span> ],</span><br><span class="line">        [<span class="number">-0.14412141</span>, <span class="number">-0.54607415</span>],</span><br><span class="line">        [ <span class="number">0.2027781</span> , <span class="number">-0.4651153</span> ]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_1/bias:<span class="number">0</span>' shape=(<span class="number">2</span>,) dtype=float32, numpy=array([<span class="number">0.</span>, <span class="number">0.</span>], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_2/kernel:<span class="number">0</span>' shape=(<span class="number">2</span>, <span class="number">1</span>) dtype=float32, numpy=</span><br><span class="line"> array([[<span class="number">-0.244825</span> ],</span><br><span class="line">        [<span class="number">-1.2101456</span>]], dtype=float32)&gt;,</span><br><span class="line"> &lt;tf.Variable 'dense_2/bias:<span class="number">0</span>' shape=(<span class="number">1</span>,) dtype=float32, numpy=array([<span class="number">0.</span>], dtype=float32)&gt;]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.submodules</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(&lt;<span class="selector-tag">tensorflow</span><span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.engine</span><span class="selector-class">.input_layer</span><span class="selector-class">.InputLayer</span> <span class="selector-tag">at</span> 0<span class="selector-tag">x144d8c080</span>&gt;,</span><br><span class="line"> &lt;<span class="selector-tag">tensorflow</span><span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> <span class="selector-tag">at</span> 0<span class="selector-tag">x144daada0</span>&gt;,</span><br><span class="line"> &lt;<span class="selector-tag">tensorflow</span><span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> <span class="selector-tag">at</span> 0<span class="selector-tag">x144d8c5c0</span>&gt;,</span><br><span class="line"> &lt;<span class="selector-tag">tensorflow</span><span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> <span class="selector-tag">at</span> 0<span class="selector-tag">x144d7aa20</span>&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.layers</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&lt;tensorflow<span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> at <span class="number">0</span>x144daada0&gt;,</span><br><span class="line"> &lt;tensorflow<span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> at <span class="number">0</span>x144d8c5c0&gt;,</span><br><span class="line"> &lt;tensorflow<span class="selector-class">.python</span><span class="selector-class">.keras</span><span class="selector-class">.layers</span><span class="selector-class">.core</span><span class="selector-class">.Dense</span> at <span class="number">0</span>x144d7aa20&gt;]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(model.name)</span><br><span class="line">print(model.name_scope())</span><br></pre></td></tr></table></figure><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sequential</span></span><br><span class="line"><span class="keyword">sequential</span></span><br></pre></td></tr></table></figure><h2 id="中阶API"><a href="#中阶API" class="headerlink" title="中阶API"></a>中阶API</h2><h3 id="数据管道Dataset"><a href="#数据管道Dataset" class="headerlink" title="数据管道Dataset"></a>数据管道Dataset</h3><p>如果需要训练的数据大小不大，例如不到1G，那么可以直接全部读入内存中进行训练，这样一般效率最高。</p><p>但如果需要训练的数据很大，例如超过10G，无法一次载入内存，那么通常需要在训练的过程中分批逐渐读入。</p><p>使用 <code>tf.data</code> API 可以构建数据输入管道，轻松处理大量的数据，不同的数据格式，以及不同的数据转换。</p><h4 id="构建数据管道"><a href="#构建数据管道" class="headerlink" title="构建数据管道"></a>构建数据管道</h4><p>可以从 Numpy array, Pandas DataFrame, Python generator, csv文件, 文本文件, 文件路径, tfrecords文件等方式构建数据管道。</p><p>其中通过Numpy array, Pandas DataFrame, 文件路径构建数据管道是最常用的方法。</p><p>通过 tfrecords文件方式构建数据管道较为复杂，需要对样本构建tf.Example后压缩成字符串写到tfrecords文件，读取后再解析成tf.Example。</p><p>但tfrecords文件的优点是压缩后文件较小，便于网络传播，加载速度较快。</p><h5 id="从Numpy-array构建数据管道"><a href="#从Numpy-array构建数据管道" class="headerlink" title="从Numpy array构建数据管道"></a>从Numpy array构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Numpy array构建数据管道</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.from_tensor_slices((iris[<span class="string">"data"</span>],iris[<span class="string">"target"</span>]))</span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds1.take(<span class="number">5</span>):</span><br><span class="line">    print(features,label)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([<span class="number">5.1</span> <span class="number">3.5</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">float</span>64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">4.9</span> <span class="number">3.</span>  <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">float</span>64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">4.7</span> <span class="number">3.2</span> <span class="number">1.3</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">float</span>64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">4.6</span> <span class="number">3.1</span> <span class="number">1.5</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">float</span>64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">5.</span>  <span class="number">3.6</span> <span class="number">1.4</span> <span class="number">0.2</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">float</span>64) tf.Tensor(<span class="number">0</span>, shape=(), dtype=<span class="built_in">int</span>64)</span><br></pre></td></tr></table></figure><h5 id="从-Pandas-DataFrame构建数据管道"><a href="#从-Pandas-DataFrame构建数据管道" class="headerlink" title="从 Pandas DataFrame构建数据管道"></a>从 Pandas DataFrame构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从 Pandas DataFrame构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">dfiris = pd.DataFrame(iris[<span class="string">"data"</span>],columns = iris.feature_names)</span><br><span class="line">ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(<span class="string">"list"</span>),iris[<span class="string">"target"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features,label <span class="keyword">in</span> ds2.take(<span class="number">3</span>):</span><br><span class="line">    print(features,label)</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=5.1&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=3.5&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=1.4&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=0.2&gt;&#125; tf.Tensor(0, shape=(), <span class="attribute">dtype</span>=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=4.9&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=3.0&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=1.4&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=0.2&gt;&#125; tf.Tensor(0, shape=(), <span class="attribute">dtype</span>=int64)</span><br><span class="line">&#123;<span class="string">'sepal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=4.7&gt;, <span class="string">'sepal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=3.2&gt;, <span class="string">'petal length (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=1.3&gt;, <span class="string">'petal width (cm)'</span>: &lt;tf.Tensor: shape=(), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=0.2&gt;&#125; tf.Tensor(0, shape=(), <span class="attribute">dtype</span>=int64)</span><br></pre></td></tr></table></figure><h5 id="从Python-generator构建数据管道"><a href="#从Python-generator构建数据管道" class="headerlink" title="从Python generator构建数据管道"></a>从Python generator构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Python generator构建数据管道</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个从文件中读取图片的generator</span></span><br><span class="line">image_generator = ImageDataGenerator(rescale=<span class="number">1.0</span>/<span class="number">255</span>).flow_from_directory(</span><br><span class="line">                    <span class="string">"./data/cifar2/test/"</span>,</span><br><span class="line">                    target_size=(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                    batch_size=<span class="number">20</span>,</span><br><span class="line">                    class_mode=<span class="string">'binary'</span>)</span><br><span class="line"></span><br><span class="line">classdict = image_generator.class_indices</span><br><span class="line">print(classdict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> features,label <span class="keyword">in</span> image_generator:</span><br><span class="line">        <span class="keyword">yield</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds3.unbatch().take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/5-1-cifar2预览.jpg"></p><h5 id="从csv文件构建数据管道"><a href="#从csv文件构建数据管道" class="headerlink" title="从csv文件构建数据管道"></a>从csv文件构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从csv文件构建数据管道</span></span><br><span class="line">ds4 = tf.data.experimental.make_csv_dataset(</span><br><span class="line">      file_pattern = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>],</span><br><span class="line">      batch_size=<span class="number">3</span>, </span><br><span class="line">      label_name=<span class="string">"Survived"</span>,</span><br><span class="line">      na_value=<span class="string">""</span>,</span><br><span class="line">      num_epochs=<span class="number">1</span>,</span><br><span class="line">      ignore_errors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data,label <span class="keyword">in</span> ds4.take(<span class="number">2</span>):</span><br><span class="line">    print(data,label)</span><br></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([540,  58, 764], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([1, 3, 1], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, numpy=</span><br><span class="line">array([b<span class="string">'Frolicher, Miss. Hedwig Margaritha'</span>, b<span class="string">'Novel, Mr. Mansouer'</span>,</span><br><span class="line">       b<span class="string">'Carter, Mrs. William Ernest (Lucile Polk)'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'female'</span>, b<span class="string">'male'</span>, b<span class="string">'female'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=array([22. , 28.5, 36. ], <span class="attribute">dtype</span>=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([0, 0, 1], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([2, 0, 2], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'13568'</span>, b<span class="string">'2697'</span>, b<span class="string">'113760'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=array([ 49.5   ,   7.2292, 120.    ], <span class="attribute">dtype</span>=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'B39'</span>, b<span class="string">''</span>, b<span class="string">'B96 B98'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'C'</span>, b<span class="string">'C'</span>, b<span class="string">'S'</span>], <span class="attribute">dtype</span>=object)&gt;)]) tf.Tensor([1 0 1], shape=(3,), <span class="attribute">dtype</span>=int32)</span><br><span class="line">OrderedDict([(<span class="string">'PassengerId'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([845,  66, 390], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Pclass'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([3, 3, 2], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Name'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, numpy=</span><br><span class="line">array([b<span class="string">'Culumovic, Mr. Jeso'</span>, b<span class="string">'Moubarek, Master. Gerios'</span>,</span><br><span class="line">       b<span class="string">'Lehmann, Miss. Bertha'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Sex'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'male'</span>, b<span class="string">'male'</span>, b<span class="string">'female'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Age'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=array([17.,  0., 17.], <span class="attribute">dtype</span>=float32)&gt;), (<span class="string">'SibSp'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([0, 1, 0], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Parch'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=int32, <span class="attribute">numpy</span>=array([0, 1, 0], <span class="attribute">dtype</span>=int32)&gt;), (<span class="string">'Ticket'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'315090'</span>, b<span class="string">'2661'</span>, b<span class="string">'SC 1748'</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Fare'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=float32, <span class="attribute">numpy</span>=array([ 8.6625, 15.2458, 12.    ], <span class="attribute">dtype</span>=float32)&gt;), (<span class="string">'Cabin'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">''</span>, b<span class="string">''</span>, b<span class="string">''</span>], <span class="attribute">dtype</span>=object)&gt;), (<span class="string">'Embarked'</span>, &lt;tf.Tensor: shape=(3,), <span class="attribute">dtype</span>=string, <span class="attribute">numpy</span>=array([b<span class="string">'S'</span>, b<span class="string">'C'</span>, b<span class="string">'C'</span>], <span class="attribute">dtype</span>=object)&gt;)]) tf.Tensor([0 1 1], shape=(3,), <span class="attribute">dtype</span>=int32)</span><br></pre></td></tr></table></figure><h5 id="从文本文件构建数据管道"><a href="#从文本文件构建数据管道" class="headerlink" title="从文本文件构建数据管道"></a>从文本文件构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文本文件构建数据管道</span></span><br><span class="line"></span><br><span class="line">ds5 = tf.data.TextLineDataset(</span><br><span class="line">    filenames = [<span class="string">"./data/titanic/train.csv"</span>,<span class="string">"./data/titanic/test.csv"</span>]</span><br><span class="line">    ).skip(<span class="number">1</span>) <span class="comment">#略去第一行header</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds5.take(<span class="number">5</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(b'<span class="number">493</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">"Molson, Mr. Harry Markland"</span>,male,<span class="number">55.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">113787</span>,<span class="number">30.5</span>,C30,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">53</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">"Harper, Mrs. Henry Sleeper (Myna Haxtun)"</span>,female,<span class="number">49.0</span>,<span class="number">1</span>,<span class="number">0</span>,PC <span class="number">17572</span>,<span class="number">76.7292</span>,D33,C', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">388</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">"Buss, Miss. Kate"</span>,female,<span class="number">36.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">27849</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">192</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">"Carbines, Mr. William"</span>,male,<span class="number">19.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">28424</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">687</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="string">"Panula, Mr. Jaako Arnold"</span>,male,<span class="number">14.0</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">3101295</span>,<span class="number">39.6875</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br></pre></td></tr></table></figure><h5 id="从文件路径构建数据管道"><a href="#从文件路径构建数据管道" class="headerlink" title="从文件路径构建数据管道"></a>从文件路径构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ds6 = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> ds6.take(<span class="number">5</span>):</span><br><span class="line">    print(file)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">cifar2</span><span class="operator">/</span><span class="params">train</span><span class="operator">/</span><span class="params">automobile</span><span class="operator">/</span>1263.<span class="params">jpg</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">cifar2</span><span class="operator">/</span><span class="params">train</span><span class="operator">/</span><span class="params">airplane</span><span class="operator">/</span>2837.<span class="params">jpg</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">cifar2</span><span class="operator">/</span><span class="params">train</span><span class="operator">/</span><span class="params">airplane</span><span class="operator">/</span>4264.<span class="params">jpg</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">cifar2</span><span class="operator">/</span><span class="params">train</span><span class="operator">/</span><span class="params">automobile</span><span class="operator">/</span>4241.<span class="params">jpg</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'.<span class="operator">/</span><span class="params">data</span><span class="operator">/</span><span class="params">cifar2</span><span class="operator">/</span><span class="params">train</span><span class="operator">/</span><span class="params">automobile</span><span class="operator">/</span>192.<span class="params">jpg</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds6.map(load_image).take(<span class="number">2</span>)):</span><br><span class="line">    plt.figure(i)</span><br><span class="line">    plt.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    plt.title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/5-1-car2.jpg"></p><h5 id="从tfrecords文件构建数据管道"><a href="#从tfrecords文件构建数据管道" class="headerlink" title="从tfrecords文件构建数据管道"></a>从tfrecords文件构建数据管道</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># inpath：原始数据路径 outpath:TFRecord文件输出路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tfrecords</span><span class="params">(inpath,outpath)</span>:</span> </span><br><span class="line">    writer = tf.io.TFRecordWriter(outpath)</span><br><span class="line">    dirs = os.listdir(inpath)</span><br><span class="line">    <span class="keyword">for</span> index, name <span class="keyword">in</span> enumerate(dirs):</span><br><span class="line">        class_path = inpath +<span class="string">"/"</span>+ name+<span class="string">"/"</span></span><br><span class="line">        <span class="keyword">for</span> img_name <span class="keyword">in</span> os.listdir(class_path):</span><br><span class="line">            img_path = class_path + img_name</span><br><span class="line">            img = tf.io.read_file(img_path)</span><br><span class="line">            <span class="comment">#img = tf.image.decode_image(img)</span></span><br><span class="line">            <span class="comment">#img = tf.image.encode_jpeg(img) #统一成jpeg格式压缩</span></span><br><span class="line">            example = tf.train.Example(</span><br><span class="line">               features=tf.train.Features(feature=&#123;</span><br><span class="line">                    <span class="string">'label'</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),</span><br><span class="line">                    <span class="string">'img_raw'</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))</span><br><span class="line">               &#125;))</span><br><span class="line">            writer.write(example.SerializeToString())</span><br><span class="line">    writer.close()</span><br><span class="line">    </span><br><span class="line">create_tfrecords(<span class="string">"./data/cifar2/test/"</span>,<span class="string">"./data/cifar2_test.tfrecords/"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_example</span><span class="params">(proto)</span>:</span></span><br><span class="line">    description =&#123; <span class="string">'img_raw'</span> : tf.io.FixedLenFeature([], tf.string),</span><br><span class="line">                   <span class="string">'label'</span>: tf.io.FixedLenFeature([], tf.int64)&#125; </span><br><span class="line">    example = tf.io.parse_single_example(proto, description)</span><br><span class="line">    img = tf.image.decode_jpeg(example[<span class="string">"img_raw"</span>])   <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img, (<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    label = example[<span class="string">"label"</span>]</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br><span class="line"></span><br><span class="line">ds7 = tf.data.TFRecordDataset(<span class="string">"./data/cifar2_test.tfrecords"</span>).map(parse_example).shuffle(<span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">6</span>)) </span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> enumerate(ds7.take(<span class="number">9</span>)):</span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow((img/<span class="number">255.0</span>).numpy())</span><br><span class="line">    ax.set_title(<span class="string">"label = %d"</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/5-1-car9.jpg"></p><h4 id="应用数据转换"><a href="#应用数据转换" class="headerlink" title="应用数据转换"></a>应用数据转换</h4><p>Dataset数据结构应用非常灵活，因为它本质上是一个Sequece序列，其每个元素可以是各种类型，例如可以是张量，列表，字典，也可以是Dataset。</p><p>Dataset包含了非常丰富的数据转换功能。</p><ul><li><p>map: 将转换函数映射到数据集每一个元素。</p></li><li><p>flat_map: 将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</p></li><li><p>interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</p></li><li><p>filter: 过滤掉某些元素。</p></li><li><p>zip: 将两个长度相同的Dataset横向铰合。</p></li><li><p>concatenate: 将两个Dataset纵向连接。</p></li><li><p>reduce: 执行归并操作。</p></li><li><p>batch : 构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。</p></li><li><p>padded_batch: 构建批次，类似batch, 但可以填充到相同的形状。</p></li><li><p>window :构建滑动窗口，返回Dataset of Dataset.</p></li><li><p>shuffle: 数据顺序洗牌。</p></li><li><p>repeat: 重复数据若干次，不带参数时，重复无数次。</p></li><li><p>shard: 采样，从某个位置开始隔固定距离采样一个元素。</p></li><li><p>take: 采样，从开始位置取前几个元素。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#map:将转换函数映射到数据集每一个元素</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_map = ds.map(<span class="keyword">lambda</span> x:tf.strings.split(x,<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor([<span class="params">b</span>'<span class="params">hello</span>' <span class="params">b</span>'<span class="params">world</span>'], <span class="params">shape</span>=(2,)</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor([<span class="params">b</span>'<span class="params">hello</span>' <span class="params">b</span>'China'], <span class="params">shape</span>=(2,)</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor([<span class="params">b</span>'<span class="params">hello</span>' <span class="params">b</span>'Beijing'], <span class="params">shape</span>=(2,)</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#flat_map:将转换函数映射到数据集的每一个元素，并将嵌套的Dataset压平。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_flatmap = ds.flat_map(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_flatmap:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'China', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'Beijing', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># interleave: 效果类似flat_map,但可以将不同来源的数据夹在一起。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line">ds_interleave = ds.interleave(<span class="keyword">lambda</span> x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x,<span class="string">" "</span>)))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_interleave:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">world</span>', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'China', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'Beijing', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#filter:过滤掉某些元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="string">"hello world"</span>,<span class="string">"hello China"</span>,<span class="string">"hello Beijing"</span>])</span><br><span class="line"><span class="comment">#找出含有字母a或B的元素</span></span><br><span class="line">ds_filter = ds.filter(<span class="keyword">lambda</span> x: tf.strings.regex_full_match(x, <span class="string">".*[a|B].*"</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_filter:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> China', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(<span class="params">b</span>'<span class="params">hello</span> Beijing', <span class="params">shape</span>=()</span>, dtype=<span class="built_in">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#zip:将两个长度相同的Dataset横向铰合。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds3 = tf.data.Dataset.range(<span class="number">6</span>,<span class="number">9</span>)</span><br><span class="line">ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> ds_zip:</span><br><span class="line">    print(x.numpy(),y.numpy(),z.numpy())</span><br></pre></td></tr></table></figure><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">0 </span><span class="number">3</span> <span class="number">6</span></span><br><span class="line"><span class="symbol">1 </span><span class="number">4</span> <span class="number">7</span></span><br><span class="line"><span class="symbol">2 </span><span class="number">5</span> <span class="number">8</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#condatenate:将两个Dataset纵向连接。</span></span><br><span class="line"></span><br><span class="line">ds1 = tf.data.Dataset.range(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">ds2 = tf.data.Dataset.range(<span class="number">3</span>,<span class="number">6</span>)</span><br><span class="line">ds_concat = tf.data.Dataset.concatenate(ds1,ds2)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_concat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(0, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(2, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(3, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(4, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(5, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#reduce:执行归并操作。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5.0</span>])</span><br><span class="line">result = ds.reduce(<span class="number">0.0</span>,<span class="keyword">lambda</span> x,y:tf.add(x,y))</span><br><span class="line">result</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">tf.Tensor:</span> <span class="attr">shape</span>=<span class="string">(),</span> <span class="attr">dtype</span>=<span class="string">float32,</span> <span class="attr">numpy</span>=<span class="string">15.0</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#batch:构建批次，每次放一个批次。比原始数据增加一个维度。 其逆操作为unbatch。 </span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_batch = ds.batch(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">4</span>,), dtype=<span class="built_in">int</span>64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#padded_batch:构建批次，类似batch, 但可以填充到相同的形状。</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],[<span class="number">6</span>, <span class="number">7</span>],[<span class="number">8</span>]]</span><br><span class="line">ds = tf.data.Dataset.from_generator(<span class="keyword">lambda</span>: iter(elements), tf.int32)</span><br><span class="line"></span><br><span class="line">ds_padded_batch = ds.padded_batch(<span class="number">2</span>,padded_shapes = [<span class="number">4</span>,])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_padded_batch:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=<span class="built_in">int</span>32)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">6</span> <span class="number">7</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]], shape=(<span class="number">2</span>, <span class="number">4</span>), dtype=<span class="built_in">int</span>32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#window:构建滑动窗口，返回Dataset of Dataset.</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line"><span class="comment">#window返回的是Dataset of Dataset,可以用flat_map压平</span></span><br><span class="line">ds_window = ds.window(<span class="number">3</span>, shift=<span class="number">1</span>).flat_map(<span class="keyword">lambda</span> x: x.batch(<span class="number">3</span>,drop_remainder=<span class="literal">True</span>)) </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_window:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">4</span> <span class="number">5</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">5</span> <span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">6</span> <span class="number">7</span> <span class="number">8</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([<span class="number">7</span> <span class="number">8</span> <span class="number">9</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([ <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br><span class="line">tf.Tensor([ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>], shape=(<span class="number">3</span>,), dtype=<span class="built_in">int</span>64)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#shuffle:数据顺序洗牌。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shuffle = ds.shuffle(buffer_size = <span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shuffle:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(4, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(0, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(6, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(5, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(2, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(7, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(11, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(3, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(9, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(10, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(8, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#repeat:重复数据若干次，不带参数时，重复无数次。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">3</span>)</span><br><span class="line">ds_repeat = ds.repeat(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_repeat:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(0, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(2, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(0, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(2, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(0, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(2, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#shard:采样，从某个位置开始隔固定距离采样一个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_shard = ds.shard(<span class="number">3</span>,index = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_shard:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.<span class="constructor">Tensor(1, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(4, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(7, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br><span class="line">tf.<span class="constructor">Tensor(10, <span class="params">shape</span>=()</span>, dtype=<span class="built_in">int64</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#take:采样，从开始位置取前几个元素。</span></span><br><span class="line"></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">12</span>)</span><br><span class="line">ds_take = ds.take(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">list(ds_take.as_numpy_iterator())</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><h4 id="提升管道性能"><a href="#提升管道性能" class="headerlink" title="提升管道性能"></a>提升管道性能</h4><p>训练深度学习模型常常会非常耗时。</p><p>模型训练的耗时主要来自于两个部分，一部分来自<strong>数据准备</strong>，另一部分来自<strong>参数迭代</strong>。</p><p>参数迭代过程的耗时通常依赖于GPU来提升。</p><p>而数据准备过程的耗时则可以通过构建高效的数据管道进行提升。</p><p>以下是一些构建高效数据管道的建议。</p><ul><li><p>使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</p></li><li><p>使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。</p></li><li><p>使用 map 时设置num_parallel_calls 让数据转换过程多进程执行。</p></li><li><p>使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</p></li><li><p>使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。</p></li></ul><p><strong>1，使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    ts = tf.timestamp()</span><br><span class="line">    today_ts = ts%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>,end = <span class="string">""</span>)</span><br><span class="line">    tf.print(timestring)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据准备和参数迭代两个过程默认情况下是串行的。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要1s</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练过程预计耗时 10*2+10*1 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">    train_step()  </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 prefetch 方法让数据准备和参数迭代两个过程相互并行。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 max(10*2,10*1) = 20s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training with prefetch..."</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.data.experimental.AUTOTUNE 可以让程序自动选择合适的参数</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):</span><br><span class="line">    train_step()  </span><br><span class="line">    </span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure><p><strong>2，使用 interleave 方法可以让数据读取过程多进程执行,并将不同来源数据夹在一起。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.flat_map(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">4</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(b'<span class="number">493</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">"Molson, Mr. Harry Markland"</span>,male,<span class="number">55.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">113787</span>,<span class="number">30.5</span>,C30,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">53</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">"Harper, Mrs. Henry Sleeper (Myna Haxtun)"</span>,female,<span class="number">49.0</span>,<span class="number">1</span>,<span class="number">0</span>,PC <span class="number">17572</span>,<span class="number">76.7292</span>,D33,C', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">388</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">"Buss, Miss. Kate"</span>,female,<span class="number">36.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">27849</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">192</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">"Carbines, Mr. William"</span>,male,<span class="number">19.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">28424</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ds_files = tf.data.Dataset.list_files(<span class="string">"./data/titanic/*.csv"</span>)</span><br><span class="line">ds = ds_files.interleave(<span class="keyword">lambda</span> x:tf.data.TextLineDataset(x).skip(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> ds.take(<span class="number">8</span>):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.Tensor(b'<span class="number">181</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="string">"Sage, Miss. Constance Gladys"</span>,female,,<span class="number">8</span>,<span class="number">2</span>,CA. <span class="number">2343</span>,<span class="number">69.55</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">493</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">"Molson, Mr. Harry Markland"</span>,male,<span class="number">55.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">113787</span>,<span class="number">30.5</span>,C30,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">405</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="string">"Oreskovic, Miss. Marija"</span>,female,<span class="number">20.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">315096</span>,<span class="number">8.6625</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">53</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">"Harper, Mrs. Henry Sleeper (Myna Haxtun)"</span>,female,<span class="number">49.0</span>,<span class="number">1</span>,<span class="number">0</span>,PC <span class="number">17572</span>,<span class="number">76.7292</span>,D33,C', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">635</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="string">"Skoog, Miss. Mabel"</span>,female,<span class="number">9.0</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">347088</span>,<span class="number">27.9</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">388</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">"Buss, Miss. Kate"</span>,female,<span class="number">36.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">27849</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">701</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">"Astor, Mrs. John Jacob (Madeleine Talmadge Force)"</span>,female,<span class="number">18.0</span>,<span class="number">1</span>,<span class="number">0</span>,PC <span class="number">17757</span>,<span class="number">227.525</span>,C62 C64,C', shape=(), dtype=<span class="type">string</span>)</span><br><span class="line">tf.Tensor(b'<span class="number">192</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">"Carbines, Mr. William"</span>,male,<span class="number">19.0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">28424</span>,<span class="number">13.0</span>,,S', shape=(), dtype=<span class="type">string</span>)</span><br></pre></td></tr></table></figure><p><strong>3，使用 map 时设置num_parallel_calls 让数据转换过程多进行执行。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ds = tf.data.Dataset.list_files(<span class="string">"./data/cifar2/train/*/*.jpg"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span><span class="params">(img_path,size = <span class="params">(<span class="number">32</span>,<span class="number">32</span>)</span>)</span>:</span></span><br><span class="line">    label = <span class="number">1</span> <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">".*/automobile/.*"</span>) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) <span class="comment">#注意此处为jpeg格式</span></span><br><span class="line">    img = tf.image.resize(img,size)</span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map = ds.map(load_image)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end transformation..."</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多进程转换</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start parallel transformation..."</span>))</span><br><span class="line"></span><br><span class="line">ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> ds_map_parallel:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end parallel transformation..."</span>))</span><br></pre></td></tr></table></figure><p><strong>4，使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)*3 = 30s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟数据准备</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="comment">#假设每次准备数据需要2s</span></span><br><span class="line">        time.sleep(<span class="number">2</span>) </span><br><span class="line">        <span class="keyword">yield</span> i </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 cache 方法让数据在第一个epoch后缓存到内存中，仅限于数据集不大情形。</span></span><br><span class="line">ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟参数迭代</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#假设每一步训练需要0s</span></span><br><span class="line">    time.sleep(<span class="number">0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程预计耗时 (5*2+5*0)+(5*0+5*0)*2 = 10s</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start training..."</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> ds:</span><br><span class="line">        train_step()  </span><br><span class="line">    printbar()</span><br><span class="line">    tf.print(<span class="string">"epoch ="</span>,epoch,<span class="string">" ended"</span>)</span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end training..."</span>))</span><br></pre></td></tr></table></figure><p><strong>5，使用 map转换时，先batch, 然后采用向量化的转换方法对每个batch进行转换。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先map后batch</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_map_batch = ds.map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>).batch(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start scalar transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_map_batch:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end scalar transformation..."</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先batch后map</span></span><br><span class="line">ds = tf.data.Dataset.range(<span class="number">100000</span>)</span><br><span class="line">ds_batch_map = ds.batch(<span class="number">20</span>).map(<span class="keyword">lambda</span> x:x**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"start vector transformation..."</span>))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ds_batch_map:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">printbar()</span><br><span class="line">tf.print(tf.constant(<span class="string">"end vector transformation..."</span>))</span><br></pre></td></tr></table></figure><h3 id="特征列feature-column"><a href="#特征列feature-column" class="headerlink" title="特征列feature_column"></a>特征列feature_column</h3><p>特征列 通常用于对结构化数据实施特征工程时候使用，图像或者文本数据一般不会用到特征列。</p><h4 id="特征列用法概述"><a href="#特征列用法概述" class="headerlink" title="特征列用法概述"></a>特征列用法概述</h4><p>使用特征列可以将类别特征转换为one-hot编码特征，将连续特征构建分桶特征，以及对多个特征生成交叉特征等等。</p><p>要创建特征列，请调用 tf.feature_column 模块的函数。该模块中常用的九个函数如下图所示，所有九个函数都会返回一个 Categorical-Column 或一个<br>Dense-Column 对象，但却不会返回 bucketized_column，后者继承自这两个类。</p><p>注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/特征列9种.jpg"></p><ul><li>numeric_column 数值列，最常用。</li></ul><ul><li>bucketized_column 分桶列，由数值列生成，可以由一个数值列出多个特征，one-hot编码。</li></ul><ul><li>categorical_column_with_identity 分类标识列，one-hot编码，相当于分桶列每个桶为1个整数的情况。</li></ul><ul><li>categorical_column_with_vocabulary_list 分类词汇列，one-hot编码，由list指定词典。</li></ul><ul><li>categorical_column_with_vocabulary_file 分类词汇列，由文件file指定词典。</li></ul><ul><li>categorical_column_with_hash_bucket 哈希列，整数或词典较大时采用。</li></ul><ul><li>indicator_column 指标列，由Categorical Column生成，one-hot编码</li></ul><ul><li>embedding_column 嵌入列，由Categorical Column生成，嵌入矢量分布参数需要学习。嵌入矢量维数建议取类别数量的 4 次方根。</li></ul><ul><li>crossed_column 交叉列，可以由除categorical_column_with_hash_bucket的任意分类列构成。</li></ul><h4 id="特征列使用范例"><a href="#特征列使用范例" class="headerlink" title="特征列使用范例"></a>特征列使用范例</h4><p>以下是一个使用特征列解决Titanic生存问题的完整范例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印日志</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printlog</span><span class="params">(info)</span>:</span></span><br><span class="line">    nowtime = datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line">    print(<span class="string">"\n"</span>+<span class="string">"=========="</span>*<span class="number">8</span> + <span class="string">"%s"</span>%nowtime)</span><br><span class="line">    print(info+<span class="string">'...\n\n'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 一，构建数据管道</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step1: prepare dataset..."</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dftrain_raw = pd.read_csv(<span class="string">"./data/titanic/train.csv"</span>)</span><br><span class="line">dftest_raw = pd.read_csv(<span class="string">"./data/titanic/test.csv"</span>)</span><br><span class="line"></span><br><span class="line">dfraw = pd.concat([dftrain_raw,dftest_raw])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_dfdata</span><span class="params">(dfraw)</span>:</span></span><br><span class="line">    dfdata = dfraw.copy()</span><br><span class="line">    dfdata.columns = [x.lower() <span class="keyword">for</span> x <span class="keyword">in</span> dfdata.columns]</span><br><span class="line">    dfdata = dfdata.rename(columns=&#123;<span class="string">'survived'</span>:<span class="string">'label'</span>&#125;)</span><br><span class="line">    dfdata = dfdata.drop([<span class="string">'passengerid'</span>,<span class="string">'name'</span>],axis = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> col,dtype <span class="keyword">in</span> dict(dfdata.dtypes).items():</span><br><span class="line">        <span class="comment"># 判断是否包含缺失值</span></span><br><span class="line">        <span class="keyword">if</span> dfdata[col].hasnans:</span><br><span class="line">            <span class="comment"># 添加标识是否缺失列</span></span><br><span class="line">            dfdata[col + <span class="string">'_nan'</span>] = pd.isna(dfdata[col]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 填充</span></span><br><span class="line">            <span class="keyword">if</span> dtype <span class="keyword">not</span> <span class="keyword">in</span> [np.object,np.str,np.unicode]:</span><br><span class="line">                dfdata[col].fillna(dfdata[col].mean(),inplace = <span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dfdata[col].fillna(<span class="string">''</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span>(dfdata)</span><br><span class="line"></span><br><span class="line">dfdata = prepare_dfdata(dfraw)</span><br><span class="line">dftrain = dfdata.iloc[<span class="number">0</span>:len(dftrain_raw),:]</span><br><span class="line">dftest = dfdata.iloc[len(dftrain_raw):,:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 dataframe 导入数据 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df_to_dataset</span><span class="params">(df, shuffle=True, batch_size=<span class="number">32</span>)</span>:</span></span><br><span class="line">    dfdata = df.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'label'</span> <span class="keyword">not</span> <span class="keyword">in</span> dfdata.columns:</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = <span class="string">'list'</span>))</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        labels = dfdata.pop(<span class="string">'label'</span>)</span><br><span class="line">        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = <span class="string">'list'</span>), labels))  </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(buffer_size=len(dfdata))</span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br><span class="line"></span><br><span class="line">ds_train = df_to_dataset(dftrain)</span><br><span class="line">ds_test = df_to_dataset(dftest)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 二，定义特征列</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step2: make feature columns..."</span>)</span><br><span class="line"></span><br><span class="line">feature_columns = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数值列</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'age'</span>,<span class="string">'fare'</span>,<span class="string">'parch'</span>,<span class="string">'sibsp'</span>] + [</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> dfdata.columns <span class="keyword">if</span> c.endswith(<span class="string">'_nan'</span>)]:</span><br><span class="line">    feature_columns.append(tf.feature_column.numeric_column(col))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分桶列</span></span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">'age'</span>)</span><br><span class="line">age_buckets = tf.feature_column.bucketized_column(age, </span><br><span class="line">             boundaries=[<span class="number">18</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>, <span class="number">45</span>, <span class="number">50</span>, <span class="number">55</span>, <span class="number">60</span>, <span class="number">65</span>])</span><br><span class="line">feature_columns.append(age_buckets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类别列</span></span><br><span class="line"><span class="comment"># 注意：所有的Catogorical Column类型最终都要通过indicator_column转换成Dense Column类型才能传入模型！！</span></span><br><span class="line">sex = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'sex'</span>,vocabulary_list=[<span class="string">"male"</span>, <span class="string">"female"</span>]))</span><br><span class="line">feature_columns.append(sex)</span><br><span class="line"></span><br><span class="line">pclass = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line">feature_columns.append(pclass)</span><br><span class="line"></span><br><span class="line">ticket = tf.feature_column.indicator_column(</span><br><span class="line">     tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'ticket'</span>,<span class="number">3</span>))</span><br><span class="line">feature_columns.append(ticket)</span><br><span class="line"></span><br><span class="line">embarked = tf.feature_column.indicator_column(</span><br><span class="line">      tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">      key=<span class="string">'embarked'</span>,vocabulary_list=[<span class="string">'S'</span>,<span class="string">'C'</span>,<span class="string">'B'</span>]))</span><br><span class="line">feature_columns.append(embarked)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入列</span></span><br><span class="line">cabin = tf.feature_column.embedding_column(</span><br><span class="line">    tf.feature_column.categorical_column_with_hash_bucket(<span class="string">'cabin'</span>,<span class="number">32</span>),<span class="number">2</span>)</span><br><span class="line">feature_columns.append(cabin)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉列</span></span><br><span class="line">pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(</span><br><span class="line">          key=<span class="string">'pclass'</span>,vocabulary_list=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">crossed_feature = tf.feature_column.indicator_column(</span><br><span class="line">    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">feature_columns.append(crossed_feature)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 三，定义模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step3: define model..."</span>)</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">  layers.DenseFeatures(feature_columns), <span class="comment">#将特征列放入到tf.keras.layers.DenseFeatures中!!!</span></span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 四，训练模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step4: train model..."</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,</span><br><span class="line">          validation_data=ds_test,</span><br><span class="line">          epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line"><span class="comment"># 五，评估模型</span></span><br><span class="line"><span class="comment">#================================================================================</span></span><br><span class="line">printlog(<span class="string">"step5: eval model..."</span>)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_metric(history,<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense_features (DenseFeature multiple                  64        </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  3008      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              multiple                  4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_2 (Dense)              multiple                  65        </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 7,297</span><br><span class="line">Trainable params: 7,297</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/5-2-01-模型评估.jpg"></p><h3 id="激活函数activation"><a href="#激活函数activation" class="headerlink" title="激活函数activation"></a>激活函数activation</h3><p>激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。</p><p>如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。</p><p>目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，例如 swish、GELU 据称效果优于relu激活函数。</p><p>激活函数的综述介绍可以参考下面两篇文章。</p><p><a href="https://zhuanlan.zhihu.com/p/98472075" target="_blank" rel="external nofollow noopener noreferrer">《一文概览深度学习中的激活函数》</a></p><p><a href="https://zhuanlan.zhihu.com/p/98472075" target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/98472075</a></p><p><a href="https://zhuanlan.zhihu.com/p/98863801" target="_blank" rel="external nofollow noopener noreferrer">《从ReLU到GELU,一文概览神经网络中的激活函数》</a></p><p><a href="https://zhuanlan.zhihu.com/p/98863801" target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/98863801</a></p><h4 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h4><ul><li>tf.nn.sigmoid：将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/sigmoid.png"></p><ul><li>tf.nn.softmax：sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/softmax说明.jpg"></p><ul><li>tf.nn.tanh：将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/tanh.png"></p><ul><li>tf.nn.relu：修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/relu.png"></p><ul><li>tf.nn.leaky_relu：对修正线性单元的改进，解决了死亡relu问题。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/leaky_relu.png"></p><ul><li>tf.nn.elu：指数线性单元。对relu的改进，能够缓解死亡relu问题。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/elu.png"></p><ul><li>tf.nn.selu：扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/selu.png"></p><ul><li>tf.nn.swish：自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/swish.png"></p><ul><li>gelu：高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。</li></ul><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/gelu.png"></p><h4 id="在模型中使用激活函数"><a href="#在模型中使用激活函数" class="headerlink" title="在模型中使用激活函数"></a>在模型中使用激活函数</h4><p>在keras模型中使用激活函数一般有两种方式，一种是作为某些层的activation参数指定，另一种是显式添加layers.Activation激活层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>,input_shape = (<span class="literal">None</span>,<span class="number">16</span>),activation = tf.nn.relu)) <span class="comment">#通过activation参数指定</span></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br><span class="line">model.add(layers.Activation(tf.nn.softmax))  <span class="comment"># 显式添加layers.Activation激活层</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h3 id="模型层layers"><a href="#模型层layers" class="headerlink" title="模型层layers"></a>模型层layers</h3><p>深度学习模型一般由各种模型层组合而成。</p><p>tf.keras.layers内置了非常丰富的各种功能的模型层。例如，</p><p>layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout</p><p>layers.Conv2D,layers.MaxPooling2D,layers.Conv1D</p><p>layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional等等。</p><p>如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。</p><p>其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。</p><h4 id="内置模型层"><a href="#内置模型层" class="headerlink" title="内置模型层"></a>内置模型层</h4><p>一些常用的内置模型层简单介绍如下。</p><p><strong>基础层</strong></p><ul><li><p>Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</p></li><li><p>Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。</p></li><li><p>Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。</p></li><li><p>BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。</p></li><li><p>SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。</p></li><li><p>Input：输入层。通常使用Functional API方式构建模型时作为第一层。</p></li><li><p>DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。</p></li><li><p>Flatten：压平层，用于将多维张量压成一维。</p></li><li><p>Reshape：形状重塑层，改变输入张量的形状。</p></li><li><p>Concatenate：拼接层，将多个张量在某个维度上拼接。</p></li><li><p>Add：加法层。</p></li><li><p>Subtract： 减法层。</p></li><li><p>Maximum：取最大值层。</p></li><li><p>Minimum：取最小值层。</p></li></ul><p><strong>卷积网络相关层</strong></p><ul><li><p>Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数</p></li><li><p>Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数</p></li><li><p>Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数</p></li><li><p>SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。</p></li><li><p>DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。</p></li><li><p>Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。</p></li><li><p>LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。</p></li><li><p>MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。</p></li><li><p>AveragePooling2D: 二维平均池化层。</p></li><li><p>GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。</p></li><li><p>GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。</p></li></ul><p><strong>循环网络相关层</strong></p><ul><li><p>Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p></li><li><p>LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。</p></li><li><p>GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</p></li><li><p>SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</p></li><li><p>ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。</p></li><li><p>Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。</p></li><li><p>RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。</p></li><li><p>LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。</p></li><li><p>GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。</p></li><li><p>SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。</p></li><li><p>AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。</p></li><li><p>Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。</p></li><li><p>AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。</p></li><li><p>TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。</p></li></ul><h4 id="自定义模型层"><a href="#自定义模型层" class="headerlink" title="自定义模型层"></a>自定义模型层</h4><p>如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。</p><p>如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。</p><p>Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。</p><p>Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,regularizers</span><br><span class="line"></span><br><span class="line">mypower = layers.Lambda(<span class="keyword">lambda</span> x:tf.math.pow(x,<span class="number">2</span>))</span><br><span class="line">mypower(tf.range(<span class="number">5</span>))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: shape=(<span class="number">5</span>,), dtype=<span class="built_in">int</span>32, numpy=<span class="built_in">array</span>([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">9</span>, <span class="number">16</span>], dtype=<span class="built_in">int</span>32)&gt;</span><br></pre></td></tr></table></figure><p>Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units=<span class="number">32</span>, **kwargs)</span>:</span></span><br><span class="line">        super(Linear, self).__init__(**kwargs)</span><br><span class="line">        self.units = units</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#build方法一般定义Layer需要被训练的参数。    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span> </span><br><span class="line">        self.w = self.add_weight(<span class="string">"w"</span>,shape=(input_shape[<span class="number">-1</span>], self.units),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>) <span class="comment">#注意必须要有参数名称"w",否则会报错</span></span><br><span class="line">        self.b = self.add_weight(<span class="string">"b"</span>,shape=(self.units,),</span><br><span class="line">                                 initializer=<span class="string">'random_normal'</span>,</span><br><span class="line">                                 trainable=<span class="literal">True</span>)</span><br><span class="line">        super(Linear,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#call方法一般定义正向传播运算逻辑，__call__方法调用了它。  </span></span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span> </span><br><span class="line">        <span class="keyword">return</span> tf.matmul(inputs, self.w) + self.b</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(Linear, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'units'</span>: self.units&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built)</span><br><span class="line"><span class="comment">#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充</span></span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.built)</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">8</span>)</span><br><span class="line">print(linear.built)</span><br><span class="line">linear.build(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)) </span><br><span class="line">print(linear.compute_output_shape(input_shape = (<span class="literal">None</span>,<span class="number">16</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight hy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line">(<span class="name">None</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">linear = Linear(units = <span class="number">16</span>)</span><br><span class="line">print(linear.built)</span><br><span class="line"><span class="comment">#如果built = False，调用__call__时会先调用build方法, 再调用call方法。</span></span><br><span class="line">linear(tf.random.uniform((<span class="number">100</span>,<span class="number">64</span>))) </span><br><span class="line">print(linear.built)</span><br><span class="line">config = linear.get_config()</span><br><span class="line">print(config)</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="string">&#123;'name':</span> <span class="string">'linear_3'</span><span class="string">,</span> <span class="attr">'trainable':</span> <span class="literal">True</span><span class="string">,</span> <span class="attr">'dtype':</span> <span class="string">'float32'</span><span class="string">,</span> <span class="attr">'units':</span> <span class="number">16</span><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"><span class="comment">#注意该处的input_shape会被模型加工，无需使用None代表样本数量维</span></span><br><span class="line">model.add(Linear(units = <span class="number">1</span>,input_shape = (<span class="number">2</span>,)))  </span><br><span class="line">print(<span class="string">"model.input_shape: "</span>,model.input_shape)</span><br><span class="line">print(<span class="string">"model.output_shape: "</span>,model.output_shape)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model.input_shape:  (None, 2)</span><br><span class="line">model.output_shape:  (None, 1)</span><br><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">linear (Linear)              (None, 1)                 3         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 3</span><br><span class="line">Trainable params: 3</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer = <span class="string">"sgd"</span>,loss = <span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">print(model.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 h5模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model.h5"</span>,save_format = <span class="string">"h5"</span>)</span><br><span class="line">model_loaded_keras = tf.keras.models.load_model(</span><br><span class="line">    <span class="string">"./data/linear_model.h5"</span>,custom_objects=&#123;<span class="string">"Linear"</span>:Linear&#125;)</span><br><span class="line">print(model_loaded_keras.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存成 tf模型</span></span><br><span class="line">model.save(<span class="string">"./data/linear_model"</span>,save_format = <span class="string">"tf"</span>)</span><br><span class="line">model_loaded_tf = tf.keras.models.load_model(<span class="string">"./data/linear_model"</span>)</span><br><span class="line">print(model_loaded_tf.predict(tf.constant([[<span class="number">3.0</span>,<span class="number">2.0</span>],[<span class="number">4.0</span>,<span class="number">5.0</span>]])))</span><br></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[[-0.04092304]</span></span><br><span class="line"><span class="string"> [-0.06150477]]</span></span><br><span class="line"><span class="string">[[-0.04092304]</span></span><br><span class="line"><span class="string"> [-0.06150477]]</span></span><br><span class="line">INFO:tensorflow:Assets written to: ./data/linear_model/assets</span><br><span class="line"><span class="string">[[-0.04092304]</span></span><br><span class="line"><span class="string"> [-0.06150477]]</span></span><br></pre></td></tr></table></figure><h3 id="损失函数losses"><a href="#损失函数losses" class="headerlink" title="损失函数losses"></a>损失函数losses</h3><p>一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）</p><p>对于keras模型，目标函数中的正则化项一般在各层中指定，例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。</p><p>损失函数在模型编译时候指定。对于回归模型，通常使用的损失函数是均方损失函数 mean_squared_error。</p><p>对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。</p><p>对于多分类模型，如果label是one-hot编码的，则使用类别交叉熵损失函数 categorical_crossentropy。如果label是类别序号编码的，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。</p><p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,regularizers,constraints</span><br></pre></td></tr></table></figure><h4 id="损失函数和正则化项"><a href="#损失函数和正则化项" class="headerlink" title="损失函数和正则化项"></a>损失函数和正则化项</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, input_dim=<span class="number">64</span>,</span><br><span class="line">                kernel_regularizer=regularizers.l2(<span class="number">0.01</span>), </span><br><span class="line">                activity_regularizer=regularizers.l1(<span class="number">0.01</span>),</span><br><span class="line">                kernel_constraint = constraints.MaxNorm(max_value=<span class="number">2</span>, axis=<span class="number">0</span>))) </span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>,</span><br><span class="line">        kernel_regularizer=regularizers.l1_l2(<span class="number">0.01</span>,<span class="number">0.01</span>),activation = <span class="string">"sigmoid"</span>))</span><br><span class="line">model.compile(optimizer = <span class="string">"rmsprop"</span>,</span><br><span class="line">        loss = <span class="string">"binary_crossentropy"</span>,metrics = [<span class="string">"AUC"</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">dense (Dense)                (None, 64)                4160      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense_1 (Dense)              (None, 10)                650       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 4,810</span><br><span class="line">Trainable params: 4,810</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="内置损失函数"><a href="#内置损失函数" class="headerlink" title="内置损失函数"></a>内置损失函数</h4><p>内置的损失函数一般有类的实现和函数的实现两种形式。</p><p>如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。</p><p>常用的一些内置损失函数说明如下。</p><ul><li><p>mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）</p></li><li><p>mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)</p></li><li><p>mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)</p></li><li><p>Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)</p></li><li><p>binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)</p></li><li><p>categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)</p></li><li><p>sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)</p></li><li><p>hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)</p></li><li><p>kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)</p></li><li><p>cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)</p></li></ul><h4 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h4><p>自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。</p><p>也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。</p><p>下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。</p><p>它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。</p><p>它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。</p><p>从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。</p><p>详见《5分钟理解Focal Loss与GHM——解决样本不平衡利器》</p><p><a href="https://zhuanlan.zhihu.com/p/80594704" target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/80594704</a></p><script type="math/tex; mode=display">focal\_loss(y,p) = \begin{cases}-\alpha  (1-p)^{\gamma}\log(p) &\text{if y = 1}\\-(1-\alpha) p^{\gamma}\log(1-p) &\text{if y = 0}\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss</span><span class="params">(gamma=<span class="number">2.</span>, alpha=<span class="number">0.75</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">focal_loss_fixed</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    <span class="keyword">return</span> focal_loss_fixed</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(tf.keras.losses.Loss)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,gamma=<span class="number">2.0</span>,alpha=<span class="number">0.75</span>,name = <span class="string">"focal_loss"</span>)</span>:</span></span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        bce = tf.losses.binary_crossentropy(y_true, y_pred)</span><br><span class="line">        p_t = (y_true * y_pred) + ((<span class="number">1</span> - y_true) * (<span class="number">1</span> - y_pred))</span><br><span class="line">        alpha_factor = y_true * self.alpha + (<span class="number">1</span> - y_true) * (<span class="number">1</span> - self.alpha)</span><br><span class="line">        modulating_factor = tf.pow(<span class="number">1.0</span> - p_t, self.gamma)</span><br><span class="line">        loss = tf.reduce_sum(alpha_factor * modulating_factor * bce,axis = <span class="number">-1</span> )</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="评估指标metrics"><a href="#评估指标metrics" class="headerlink" title="评估指标metrics"></a>评估指标metrics</h3><p>损失函数除了作为模型训练时候的优化目标，也能够作为模型好坏的一种评价指标。但通常人们还会从其它角度评估模型的好坏。</p><p>这就是评估指标。通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。</p><p>但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。</p><p>编译模型时，可以通过列表形式指定多个评估指标。</p><p>如果有需要，也可以自定义评估指标。</p><p>自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。</p><p>也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。</p><p>由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。</p><p>如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离整个epoch数据一次计算的结果。</p><h4 id="常用的内置评估指标"><a href="#常用的内置评估指标" class="headerlink" title="常用的内置评估指标"></a>常用的内置评估指标</h4><ul><li><p>MeanSquaredError（均方误差，用于回归，可以简写为MSE，函数形式为mse）</p></li><li><p>MeanAbsoluteError (平均绝对值误差，用于回归，可以简写为MAE，函数形式为mae)</p></li><li><p>MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)</p></li><li><p>RootMeanSquaredError (均方根误差，用于回归)</p></li><li><p>Accuracy (准确率，用于分类，可以用字符串”Accuracy”表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)</p></li><li><p>Precision (精确率，用于二分类，Precision = TP/(TP+FP))</p></li><li><p>Recall (召回率，用于二分类，Recall = TP/(TP+FN))</p></li><li><p>TruePositives (真正例，用于二分类)</p></li><li><p>TrueNegatives (真负例，用于二分类)</p></li><li><p>FalsePositives (假正例，用于二分类)</p></li><li><p>FalseNegatives (假负例，用于二分类)</p></li><li><p>AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)</p></li><li><p>CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）</p></li><li><p>SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)</p></li><li><p>MeanIoU (Intersection-Over-Union，常用于图像分割)</p></li><li><p>TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)</p></li><li><p>SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)</p></li><li><p>Mean (平均值)</p></li><li><p>Sum (求和)</p></li></ul><h4 id="自定义评估指标"><a href="#自定义评估指标" class="headerlink" title="自定义评估指标"></a>自定义评估指标</h4><p>我们以金融风控领域常用的KS指标为例，示范自定义评估指标。</p><p>KS指标适合二分类问题，其计算方式为 KS=max(TPR-FPR).</p><p>其中TPR=TP/(TP+FN) , FPR = FP/(FP+TN) </p><p>TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF)。</p><p>KS指标就是正样本和负样本累积分布曲线差值的最大值。</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/KS_curve.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics</span><br><span class="line"></span><br><span class="line"><span class="comment">#函数形式的自定义评估指标</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ks</span><span class="params">(y_true,y_pred)</span>:</span></span><br><span class="line">    y_true = tf.reshape(y_true,(<span class="number">-1</span>,))</span><br><span class="line">    y_pred = tf.reshape(y_pred,(<span class="number">-1</span>,))</span><br><span class="line">    length = tf.shape(y_true)[<span class="number">0</span>]</span><br><span class="line">    t = tf.math.top_k(y_pred,k = length,sorted = <span class="literal">False</span>)</span><br><span class="line">    y_pred_sorted = tf.gather(y_pred,t.indices)</span><br><span class="line">    y_true_sorted = tf.gather(y_true,t.indices)</span><br><span class="line">    cum_positive_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))</span><br><span class="line">    cum_negative_ratio = tf.truediv(</span><br><span class="line">        tf.cumsum(<span class="number">1</span> - y_true_sorted),tf.reduce_sum(<span class="number">1</span> - y_true_sorted))</span><br><span class="line">    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">    <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line">tf.print(ks(y_true,y_pred))</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.625</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#类形式的自定义评估指标</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KS</span><span class="params">(metrics.Metric)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name = <span class="string">"ks"</span>, **kwargs)</span>:</span></span><br><span class="line">        super(KS,self).__init__(name=name,**kwargs)</span><br><span class="line">        self.true_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"tp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">        self.false_positives = self.add_weight(</span><br><span class="line">            name = <span class="string">"fp"</span>,shape = (<span class="number">101</span>,), initializer = <span class="string">"zeros"</span>)</span><br><span class="line">   </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_state</span><span class="params">(self,y_true,y_pred)</span>:</span></span><br><span class="line">        y_true = tf.cast(tf.reshape(y_true,(<span class="number">-1</span>,)),tf.bool)</span><br><span class="line">        y_pred = tf.cast(<span class="number">100</span>*tf.reshape(y_pred,(<span class="number">-1</span>,)),tf.int32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tf.range(<span class="number">0</span>,tf.shape(y_true)[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> y_true[i]:</span><br><span class="line">                self.true_positives[y_pred[i]].assign(</span><br><span class="line">                    self.true_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.false_positives[y_pred[i]].assign(</span><br><span class="line">                    self.false_positives[y_pred[i]]+<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> (self.true_positives,self.false_positives)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span><span class="params">(self)</span>:</span></span><br><span class="line">        cum_positive_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))</span><br><span class="line">        cum_negative_ratio = tf.truediv(</span><br><span class="line">            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))</span><br><span class="line">        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) </span><br><span class="line">        <span class="keyword">return</span> ks_value</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_true = tf.constant([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">0</span>]])</span><br><span class="line">y_pred = tf.constant([[<span class="number">0.6</span>],[<span class="number">0.1</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.7</span>],[<span class="number">0.7</span>],</span><br><span class="line">                      [<span class="number">0.7</span>],[<span class="number">0.4</span>],[<span class="number">0.4</span>],[<span class="number">0.5</span>],[<span class="number">0.8</span>],[<span class="number">0.3</span>],[<span class="number">0.5</span>],[<span class="number">0.3</span>]])</span><br><span class="line"></span><br><span class="line">myks = KS()</span><br><span class="line">myks.update_state(y_true,y_pred)</span><br><span class="line">tf.print(myks.result())</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.625</span></span><br></pre></td></tr></table></figure><h3 id="回调函数callbacks"><a href="#回调函数callbacks" class="headerlink" title="回调函数callbacks"></a>回调函数callbacks</h3><p>tf.keras的回调函数实际上是一个类，一般是在model.fit时作为参数指定，用于控制在训练过程开始或者在训练过程结束，在每个epoch训练开始或者训练结束，在每个batch训练开始或者训练结束时执行一些操作，例如收集一些日志信息，改变学习率等超参数，提前终止训练过程等等。</p><p>同样地，针对model.evaluate或者model.predict也可以指定callbacks参数，用于控制在评估或预测开始或者结束时，在每个batch开始或者结束时执行一些操作，但这种用法相对少见。</p><p>大部分时候，keras.callbacks子模块中定义的回调函数类已经足够使用了，如果有特定的需要，我们也可以通过对keras.callbacks.Callbacks实施子类化构造自定义的回调函数。</p><p>所有回调函数都继承至 keras.callbacks.Callbacks基类，拥有params和model这两个属性。</p><p>其中params 是一个dict，记录了训练相关参数 (例如 verbosity, batch size, number of epochs 等等)。</p><p>model即当前关联的模型的引用。</p><p>此外，对于回调类中的一些方法如on_epoch_begin,on_batch_end，还会有一个输入参数logs, 提供有关当前epoch或者batch的一些信息，并能够记录计算结果，如果model.fit指定了多个回调函数类，这些logs变量将在这些回调函数类的同名函数间依顺序传递。</p><h4 id="内置回调函数"><a href="#内置回调函数" class="headerlink" title="内置回调函数"></a>内置回调函数</h4><ul><li><p>BaseLogger： 收集每个epoch上metrics在各个batch上的平均值，对stateful_metrics参数中的带中间状态的指标直接拿最终值无需对各个batch平均，指标均值结果将添加到logs变量中。该回调函数被所有模型默认添加，且是第一个被添加的。</p></li><li><p>History： 将BaseLogger计算的各个epoch的metrics结果记录到history这个dict变量中，并作为model.fit的返回值。该回调函数被所有模型默认添加，在BaseLogger之后被添加。</p></li><li><p>EarlyStopping： 当被监控指标在设定的若干个epoch后没有提升，则提前终止训练。</p></li><li><p>TensorBoard： 为Tensorboard可视化保存日志信息。支持评估指标，计算图，模型参数等的可视化。</p></li><li><p>ModelCheckpoint： 在每个epoch后保存模型。</p></li><li><p>ReduceLROnPlateau：如果监控指标在设定的若干个epoch后没有提升，则以一定的因子减少学习率。</p></li><li><p>TerminateOnNaN：如果遇到loss为NaN，提前终止训练。</p></li><li><p>LearningRateScheduler：学习率控制器。给定学习率lr和epoch的函数关系，根据该函数关系在每个epoch前调整学习率。</p></li><li><p>CSVLogger：将每个epoch后的logs结果记录到CSV文件中。</p></li><li><p>ProgbarLogger：将每个epoch后的logs结果打印到标准输出流中。</p></li></ul><h4 id="自定义回调函数"><a href="#自定义回调函数" class="headerlink" title="自定义回调函数"></a>自定义回调函数</h4><p>可以使用callbacks.LambdaCallback编写较为简单的回调函数，也可以通过对callbacks.Callback子类化编写更加复杂的回调函数逻辑。</p><p>如果需要深入学习tf.Keras中的回调函数，不要犹豫阅读内置回调函数的源代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models,losses,metrics,callbacks</span><br><span class="line"><span class="keyword">import</span> tensorflow.keras.backend <span class="keyword">as</span> K</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示范使用LambdaCallback编写较为简单的回调函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">json_log = open(<span class="string">'./data/keras_log.json'</span>, mode=<span class="string">'wt'</span>, buffering=<span class="number">1</span>)</span><br><span class="line">json_logging_callback = callbacks.LambdaCallback(</span><br><span class="line">    on_epoch_end=<span class="keyword">lambda</span> epoch, logs: json_log.write(</span><br><span class="line">        json.dumps(dict(epoch = epoch,**logs)) + <span class="string">'\n'</span>),</span><br><span class="line">    on_train_end=<span class="keyword">lambda</span> logs: json_log.close()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示范通过Callback子类化编写回调函数（LearningRateScheduler的源代码）</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LearningRateScheduler</span><span class="params">(callbacks.Callback)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, schedule, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(LearningRateScheduler, self).__init__()</span><br><span class="line">        self.schedule = schedule</span><br><span class="line">        self.verbose = verbose</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_begin</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self.model.optimizer, <span class="string">'lr'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Optimizer must have a "lr" attribute.'</span>)</span><br><span class="line">        <span class="keyword">try</span>:  </span><br><span class="line">            lr = float(K.get_value(self.model.optimizer.lr))</span><br><span class="line">            lr = self.schedule(epoch, lr)</span><br><span class="line">        <span class="keyword">except</span> TypeError:  <span class="comment"># Support for old API for backward compatibility</span></span><br><span class="line">            lr = self.schedule(epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The output of the "schedule" function '</span></span><br><span class="line">                             <span class="string">'should be float.'</span>)</span><br><span class="line">        <span class="keyword">if</span> isinstance(lr, ops.Tensor) <span class="keyword">and</span> <span class="keyword">not</span> lr.dtype.is_floating:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'The dtype of Tensor should be float'</span>)</span><br><span class="line">        K.set_value(self.model.optimizer.lr, K.get_value(lr))</span><br><span class="line">        <span class="keyword">if</span> self.verbose &gt; <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'\nEpoch %05d: LearningRateScheduler reducing learning '</span></span><br><span class="line">                 <span class="string">'rate to %s.'</span> % (epoch + <span class="number">1</span>, lr))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span><span class="params">(self, epoch, logs=None)</span>:</span></span><br><span class="line">        logs = logs <span class="keyword">or</span> &#123;&#125;</span><br><span class="line">        logs[<span class="string">'lr'</span>] = K.get_value(self.model.optimizer.lr)</span><br></pre></td></tr></table></figure><h2 id="高阶API"><a href="#高阶API" class="headerlink" title="高阶API"></a>高阶API</h2><p>TensorFlow的高阶API主要是tensorflow.keras.models.</p><p>本章我们主要详细介绍tensorflow.keras.models相关的以下内容。</p><ul><li>模型的构建（Sequential、functional API、Model子类化）</li><li>模型的训练（内置fit方法、内置train_on_batch方法、自定义训练循环、单GPU训练模型、多GPU训练模型、TPU训练模型）</li><li>模型的部署（tensorflow serving部署模型、使用spark(scala)调用tensorflow模型）</li></ul><h3 id="构建模型的3种方法"><a href="#构建模型的3种方法" class="headerlink" title="构建模型的3种方法"></a>构建模型的3种方法</h3><p>可以使用以下3种方式构建模型：</p><ul><li>使用Sequential按层顺序构建模型，适合于顺序结构的模型</li><li>使用函数式API构建任意结构模型，如果模型有多输入或者多输出，或者模型需要共享权重，或者模型具有残差连接等非顺序结构，推荐使用函数式API进行创建</li><li>继承Model基类构建自定义模型，如果无特定必要，尽可能避免使用Model子类化的方式构建模型，这种方式提供了极大的灵活性，但也有更大的概率出错。</li></ul><p>下面以IMDB电影评论的分类问题为例，演示3种创建模型的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm </span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_token_path = <span class="string">"./data/imdb/train_token.csv"</span></span><br><span class="line">test_token_path = <span class="string">"./data/imdb/test_token.csv"</span></span><br><span class="line"></span><br><span class="line">MAX_WORDS = <span class="number">10000</span>  <span class="comment"># We will only consider the top 10,000 words in the dataset</span></span><br><span class="line">MAX_LEN = <span class="number">200</span>  <span class="comment"># We will cut reviews after 200 words</span></span><br><span class="line">BATCH_SIZE = <span class="number">20</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建管道</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_line</span><span class="params">(line)</span>:</span></span><br><span class="line">    t = tf.strings.split(line,<span class="string">"\t"</span>)</span><br><span class="line">    label = tf.reshape(tf.cast(tf.strings.to_number(t[<span class="number">0</span>]),tf.int32),(<span class="number">-1</span>,))</span><br><span class="line">    features = tf.cast(tf.strings.to_number(tf.strings.split(t[<span class="number">1</span>],<span class="string">" "</span>)),tf.int32)</span><br><span class="line">    <span class="keyword">return</span> (features,label)</span><br><span class="line"></span><br><span class="line">ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line">ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \</span><br><span class="line">   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \</span><br><span class="line">   .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">   .prefetch(tf.data.experimental.AUTOTUNE)</span><br></pre></td></tr></table></figure><h4 id="Sequential按层顺序创建模型"><a href="#Sequential按层顺序创建模型" class="headerlink" title="Sequential按层顺序创建模型"></a>Sequential按层顺序创建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/Sequential模型结构.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">baselogger = callbacks.BaseLogger(stateful_metrics=[<span class="string">"AUC"</span>])</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">        epochs = <span class="number">6</span>,callbacks=[baselogger,tensorboard_callback])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'svg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span><span class="params">(history, metric)</span>:</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">'val_'</span>+metric]</span><br><span class="line">    epochs = range(<span class="number">1</span>, len(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">'bo--'</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">'ro-'</span>)</span><br><span class="line">    plt.title(<span class="string">'Training and validation '</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">"train_"</span>+metric, <span class="string">'val_'</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/6-1-fit模型.jpg"></p><h4 id="函数式API创建任意结构模型"><a href="#函数式API创建任意结构模型" class="headerlink" title="函数式API创建任意结构模型"></a>函数式API创建任意结构模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">inputs = layers.Input(shape=[MAX_LEN])</span><br><span class="line">x  = layers.Embedding(MAX_WORDS,<span class="number">7</span>)(inputs)</span><br><span class="line"></span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch1 = layers.MaxPool1D(<span class="number">3</span>)(branch1)</span><br><span class="line">branch1 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">3</span>,activation=<span class="string">"relu"</span>)(branch1)</span><br><span class="line">branch1 = layers.GlobalMaxPool1D()(branch1)</span><br><span class="line"></span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch2 = layers.MaxPool1D(<span class="number">5</span>)(branch2)</span><br><span class="line">branch2 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">5</span>,activation=<span class="string">"relu"</span>)(branch2)</span><br><span class="line">branch2 = layers.GlobalMaxPool1D()(branch2)</span><br><span class="line"></span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">64</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(x)</span><br><span class="line">branch3 = layers.MaxPool1D(<span class="number">7</span>)(branch3)</span><br><span class="line">branch3 = layers.SeparableConv1D(<span class="number">32</span>,<span class="number">7</span>,activation=<span class="string">"relu"</span>)(branch3)</span><br><span class="line">branch3 = layers.GlobalMaxPool1D()(branch3)</span><br><span class="line"></span><br><span class="line">concat = layers.Concatenate()([branch1,branch2,branch3])</span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)(concat)</span><br><span class="line"></span><br><span class="line">model = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">Model: "model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to                     </span><br><span class="line">==================================================================================================</span><br><span class="line">input_1 (InputLayer)            [(None, 200)]        0                                            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">embedding (Embedding)           (None, 200, 7)       70000       input_1[<span class="string">0</span>][<span class="symbol">0</span>]                    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>2 (SeparableCo (None, 196, 64)      547         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>4 (SeparableCo (None, 194, 64)      561         embedding[<span class="string">0</span>][<span class="symbol">0</span>]                  </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_</span>conv1d[<span class="string">0</span>][<span class="symbol">0</span>]           </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1D)  (None, 39, 64)       0           separable<span class="emphasis">_conv1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>2 (MaxPooling1D)  (None, 27, 64)       0           separable<span class="emphasis">_conv1d_</span>4[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]              </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>3 (SeparableCo (None, 35, 32)       2400        max<span class="emphasis">_pooling1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">separable<span class="emphasis">_conv1d_</span>5 (SeparableCo (None, 21, 32)       2528        max<span class="emphasis">_pooling1d_</span>2[<span class="string">0</span>][<span class="symbol">0</span>]            </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d (GlobalMax (None, 32)           0           separable<span class="emphasis">_conv1d_</span>1[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_1 (GlobalM (None, 32)           0           separable_</span>conv1d_3[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">global<span class="emphasis">_max_</span>pooling1d<span class="emphasis">_2 (GlobalM (None, 32)           0           separable_</span>conv1d_5[<span class="string">0</span>][<span class="symbol">0</span>]         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">concatenate (Concatenate)       (None, 96)           0           global<span class="emphasis">_max_</span>pooling1d[<span class="string">0</span>][<span class="symbol">0</span>]       </span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_1[0][0]     </span></span><br><span class="line"><span class="code">                                                                 global_max_pooling1d_2[0][0]     </span></span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br><span class="line">dense (Dense)                   (None, 1)            97          concatenate[<span class="string">0</span>][<span class="symbol">0</span>]                </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 78,938</span><br><span class="line">Trainable params: 78,938</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="emphasis">___</span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/FunctionalAPI模型结构.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">logdir = <span class="string">"./data/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">32</span>s <span class="number">32</span>ms/step - loss: <span class="number">0.5527</span> - accuracy: <span class="number">0.6758</span> - AUC: <span class="number">0.7731</span> - val_loss: <span class="number">0.3646</span> - val_accuracy: <span class="number">0.8426</span> - val_AUC: <span class="number">0.9192</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.3024</span> - accuracy: <span class="number">0.8737</span> - AUC: <span class="number">0.9444</span> - val_loss: <span class="number">0.3281</span> - val_accuracy: <span class="number">0.8644</span> - val_AUC: <span class="number">0.9350</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.2158</span> - accuracy: <span class="number">0.9159</span> - AUC: <span class="number">0.9715</span> - val_loss: <span class="number">0.3461</span> - val_accuracy: <span class="number">0.8666</span> - val_AUC: <span class="number">0.9363</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.1492</span> - accuracy: <span class="number">0.9464</span> - AUC: <span class="number">0.9859</span> - val_loss: <span class="number">0.4017</span> - val_accuracy: <span class="number">0.8568</span> - val_AUC: <span class="number">0.9311</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">24</span>s <span class="number">24</span>ms/step - loss: <span class="number">0.0944</span> - accuracy: <span class="number">0.9696</span> - AUC: <span class="number">0.9939</span> - val_loss: <span class="number">0.4998</span> - val_accuracy: <span class="number">0.8550</span> - val_AUC: <span class="number">0.9233</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">26</span>s <span class="number">26</span>ms/step - loss: <span class="number">0.0526</span> - accuracy: <span class="number">0.9865</span> - AUC: <span class="number">0.9977</span> - val_loss: <span class="number">0.6463</span> - val_accuracy: <span class="number">0.8462</span> - val_AUC: <span class="number">0.9138</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/6-1-2-train.jpg"></p><h4 id="Model子类化创建自定义模型"><a href="#Model子类化创建自定义模型" class="headerlink" title="Model子类化创建自定义模型"></a>Model子类化创建自定义模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先自定义一个残差模块，为自定义Layer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kernel_size, **kwargs)</span>:</span></span><br><span class="line">        super(ResBlock, self).__init__(**kwargs)</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.conv1 = layers.Conv1D(filters=<span class="number">64</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv2 = layers.Conv1D(filters=<span class="number">32</span>,kernel_size=self.kernel_size,</span><br><span class="line">                                   activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.conv3 = layers.Conv1D(filters=input_shape[<span class="number">-1</span>],</span><br><span class="line">                                   kernel_size=self.kernel_size,activation = <span class="string">"relu"</span>,padding=<span class="string">"same"</span>)</span><br><span class="line">        self.maxpool = layers.MaxPool1D(<span class="number">2</span>)</span><br><span class="line">        super(ResBlock,self).build(input_shape) <span class="comment"># 相当于设置self.built = True</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv1(inputs)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = layers.Add()([inputs,x])</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span>  </span><br><span class="line">        config = super(ResBlock, self).get_config()</span><br><span class="line">        config.update(&#123;<span class="string">'kernel_size'</span>: self.kernel_size&#125;)</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试ResBlock</span></span><br><span class="line">resblock = ResBlock(kernel_size = <span class="number">3</span>)</span><br><span class="line">resblock.build(input_shape = (<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br><span class="line">resblock.compute_output_shape(input_shape=(<span class="literal">None</span>,<span class="number">200</span>,<span class="number">7</span>))</span><br></pre></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">TensorShape</span><span class="params">([None, <span class="number">100</span>, <span class="number">7</span>])</span></span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义模型，实际上也可以使用Sequential或者FunctionalAPI</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImdbModel</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ImdbModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self,input_shape)</span>:</span></span><br><span class="line">        self.embedding = layers.Embedding(MAX_WORDS,<span class="number">7</span>)</span><br><span class="line">        self.block1 = ResBlock(<span class="number">7</span>)</span><br><span class="line">        self.block2 = ResBlock(<span class="number">5</span>)</span><br><span class="line">        self.dense = layers.Dense(<span class="number">1</span>,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        super(ImdbModel,self).build(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x = layers.Flatten()(x)</span><br><span class="line">        x = self.dense(x)</span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line">model = ImdbModel()</span><br><span class="line">model.build(input_shape =(<span class="literal">None</span>,<span class="number">200</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'Nadam'</span>,</span><br><span class="line">            loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>,<span class="string">"AUC"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model: "imdb_model"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        multiple                  70000     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res_block (ResBlock)         multiple                  19143     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">res<span class="emphasis">_block_</span>1 (ResBlock)       multiple                  13703     </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                multiple                  351       </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 103,197</span><br><span class="line">Trainable params: 103,197</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/Model子类化模型结构.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">logdir = <span class="string">"./tflogs/keras_model/"</span> + datetime.datetime.now().strftime(<span class="string">"%Y%m%d-%H%M%S"</span>)</span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=<span class="number">1</span>)</span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,</span><br><span class="line">                    epochs = <span class="number">6</span>,callbacks=[tensorboard_callback])</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">47</span>s <span class="number">47</span>ms/step - loss: <span class="number">0.5629</span> - accuracy: <span class="number">0.6618</span> - AUC: <span class="number">0.7548</span> - val_loss: <span class="number">0.3422</span> - val_accuracy: <span class="number">0.8510</span> - val_AUC: <span class="number">0.9286</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.2648</span> - accuracy: <span class="number">0.8903</span> - AUC: <span class="number">0.9576</span> - val_loss: <span class="number">0.3276</span> - val_accuracy: <span class="number">0.8650</span> - val_AUC: <span class="number">0.9410</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.1573</span> - accuracy: <span class="number">0.9439</span> - AUC: <span class="number">0.9846</span> - val_loss: <span class="number">0.3861</span> - val_accuracy: <span class="number">0.8682</span> - val_AUC: <span class="number">0.9390</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">42</span>s <span class="number">42</span>ms/step - loss: <span class="number">0.0849</span> - accuracy: <span class="number">0.9706</span> - AUC: <span class="number">0.9950</span> - val_loss: <span class="number">0.5324</span> - val_accuracy: <span class="number">0.8616</span> - val_AUC: <span class="number">0.9292</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">43</span>s <span class="number">43</span>ms/step - loss: <span class="number">0.0393</span> - accuracy: <span class="number">0.9876</span> - AUC: <span class="number">0.9986</span> - val_loss: <span class="number">0.7693</span> - val_accuracy: <span class="number">0.8566</span> - val_AUC: <span class="number">0.9132</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">6</span></span><br><span class="line"><span class="number">1000</span>/<span class="number">1000</span> [==============================] - <span class="number">44</span>s <span class="number">44</span>ms/step - loss: <span class="number">0.0222</span> - accuracy: <span class="number">0.9926</span> - AUC: <span class="number">0.9994</span> - val_loss: <span class="number">0.9328</span> - val_accuracy: <span class="number">0.8584</span> - val_AUC: <span class="number">0.9052</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_metric(history,<span class="string">"AUC"</span>)</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/6-1-3-fit模型.jpg"></p><h3 id="训练模型的3种方法"><a href="#训练模型的3种方法" class="headerlink" title="训练模型的3种方法"></a>训练模型的3种方法</h3><p>模型的训练主要有内置fit方法、内置tran_on_batch方法、自定义训练循环。</p><p>注：fit_generator方法在tf.keras中不推荐使用，其功能已经被fit包含。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure><h4 id="内置fit方法"><a href="#内置fit方法" class="headerlink" title="内置fit方法"></a>内置fit方法</h4><p>该方法功能非常强大, 支持对numpy array, tf.data.Dataset以及 Python generator数据进行训练。</p><p>并且可以通过设置回调函数实现对训练过程的复杂控制逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">11</span>s <span class="number">37</span>ms/step - loss: <span class="number">2.0231</span> - sparse_categorical_accuracy: <span class="number">0.4636</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7450</span> - val_loss: <span class="number">1.7346</span> - val_sparse_categorical_accuracy: <span class="number">0.5534</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7560</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">31</span>ms/step - loss: <span class="number">1.5079</span> - sparse_categorical_accuracy: <span class="number">0.6091</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7901</span> - val_loss: <span class="number">1.5475</span> - val_sparse_categorical_accuracy: <span class="number">0.6109</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7792</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">1.2204</span> - sparse_categorical_accuracy: <span class="number">0.6823</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8448</span> - val_loss: <span class="number">1.5455</span> - val_sparse_categorical_accuracy: <span class="number">0.6367</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8001</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.9382</span> - sparse_categorical_accuracy: <span class="number">0.7543</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9075</span> - val_loss: <span class="number">1.6780</span> - val_sparse_categorical_accuracy: <span class="number">0.6398</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8032</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">34</span>ms/step - loss: <span class="number">0.6791</span> - sparse_categorical_accuracy: <span class="number">0.8255</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9513</span> - val_loss: <span class="number">1.9426</span> - val_sparse_categorical_accuracy: <span class="number">0.6376</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7956</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">33</span>ms/step - loss: <span class="number">0.5063</span> - sparse_categorical_accuracy: <span class="number">0.8762</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9716</span> - val_loss: <span class="number">2.2141</span> - val_sparse_categorical_accuracy: <span class="number">0.6291</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">37</span>ms/step - loss: <span class="number">0.4031</span> - sparse_categorical_accuracy: <span class="number">0.9050</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9817</span> - val_loss: <span class="number">2.4126</span> - val_sparse_categorical_accuracy: <span class="number">0.6264</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7947</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">35</span>ms/step - loss: <span class="number">0.3380</span> - sparse_categorical_accuracy: <span class="number">0.9205</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9881</span> - val_loss: <span class="number">2.5366</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7974</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">10</span>s <span class="number">36</span>ms/step - loss: <span class="number">0.2921</span> - sparse_categorical_accuracy: <span class="number">0.9299</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9909</span> - val_loss: <span class="number">2.6564</span> - val_sparse_categorical_accuracy: <span class="number">0.6242</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">9</span>s <span class="number">30</span>ms/step - loss: <span class="number">0.2613</span> - sparse_categorical_accuracy: <span class="number">0.9334</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9947</span> - val_loss: <span class="number">2.7365</span> - val_sparse_categorical_accuracy: <span class="number">0.6220</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8005</span></span><br></pre></td></tr></table></figure><h4 id="内置train-on-batch方法"><a href="#内置train-on-batch方法" class="headerlink" title="内置train_on_batch方法"></a>内置train_on_batch方法</h4><p>该内置方法相比较fit方法更加灵活，可以不通过回调函数而直接在批次层次上更加精细地控制训练的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"> </span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br><span class="line">model = compile_model(model)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epoches)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epoches+<span class="number">1</span>):</span><br><span class="line">        model.reset_metrics()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在后期降低学习率</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">5</span>:</span><br><span class="line">            model.optimizer.lr.assign(model.optimizer.lr/<span class="number">2.0</span>)</span><br><span class="line">            tf.print(<span class="string">"Lowering optimizer Learning Rate...\n\n"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_result = model.train_on_batch(x, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_result = model.test_on_batch(x, y,reset_metrics=<span class="literal">False</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(<span class="string">"epoch = "</span>,epoch)</span><br><span class="line">            print(<span class="string">"train:"</span>,dict(zip(model.metrics_names,train_result)))</span><br><span class="line">            print(<span class="string">"valid:"</span>,dict(zip(model.metrics_names,valid_result)))</span><br><span class="line">            print(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">================================================================================13:09:19</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">1</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.82411176</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.77272725</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8636364</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.9265995</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.5743544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.75779164</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:27</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">2</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.6006621</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">1.844159</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6126447</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7920748</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:35</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">3</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.36935613</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.163433</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.63312554</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.8045414</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:42</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">4</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.2304088</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.90909094</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">2.8911984</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6344613</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">Lowering</span> <span class="string">optimizer</span> <span class="string">Learning</span> <span class="string">Rate...</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:51</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">5</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.111194365</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">3.6431572</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6295637</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7978629</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:09:59</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">6</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.07741702</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.95454544</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.074161</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6255565</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:07</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">7</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.056113098</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.4461513</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6273375</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79652715</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:17</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">8</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.043448802</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">4.7687583</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6224399</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.79741764</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:26</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">9</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.035002146</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.130505</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6175423</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.794301</span><span class="string">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="string">================================================================================13:10:34</span></span><br><span class="line"><span class="string">epoch</span> <span class="string">=</span>  <span class="number">10</span></span><br><span class="line"><span class="attr">train:</span> <span class="string">&#123;'loss':</span> <span class="number">0.028303564</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">1.0</span><span class="string">&#125;</span></span><br><span class="line"><span class="attr">valid:</span> <span class="string">&#123;'loss':</span> <span class="number">5.4559293</span><span class="string">,</span> <span class="attr">'sparse_categorical_accuracy':</span> <span class="number">0.6148709</span><span class="string">,</span> <span class="attr">'sparse_top_k_categorical_accuracy':</span> <span class="number">0.7947462</span><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h4 id="自定义训练循环"><a href="#自定义训练循环" class="headerlink" title="自定义训练循环"></a>自定义训练循环</h4><p>自定义训练循环无需编译模型，直接利用优化器根据损失函数反向传播迭代参数，拥有最高的灵活性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">03</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">2.02051544</span>,Accuracy:<span class="number">0.460253835</span>,Valid Loss:<span class="number">1.75700927</span>,Valid Accuracy:<span class="number">0.536954582</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">09</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.510795</span>,Accuracy:<span class="number">0.610665798</span>,Valid Loss:<span class="number">1.55349839</span>,Valid Accuracy:<span class="number">0.616206586</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">17</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.19221532</span>,Accuracy:<span class="number">0.696170092</span>,Valid Loss:<span class="number">1.52315605</span>,Valid Accuracy:<span class="number">0.651380241</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">23</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.90101546</span>,Accuracy:<span class="number">0.766310394</span>,Valid Loss:<span class="number">1.68327653</span>,Valid Accuracy:<span class="number">0.648263574</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.655430496</span>,Accuracy:<span class="number">0.831329346</span>,Valid Loss:<span class="number">1.90872383</span>,Valid Accuracy:<span class="number">0.641139805</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.492730737</span>,Accuracy:<span class="number">0.877866864</span>,Valid Loss:<span class="number">2.09966016</span>,Valid Accuracy:<span class="number">0.63223511</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">44</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.391238362</span>,Accuracy:<span class="number">0.904030263</span>,Valid Loss:<span class="number">2.27431226</span>,Valid Accuracy:<span class="number">0.625111282</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">51</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.327761739</span>,Accuracy:<span class="number">0.922066331</span>,Valid Loss:<span class="number">2.42568827</span>,Valid Accuracy:<span class="number">0.617542326</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">12</span>:<span class="number">58</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.285573095</span>,Accuracy:<span class="number">0.930527747</span>,Valid Loss:<span class="number">2.55942106</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">13</span>:<span class="number">13</span>:<span class="number">05</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.255482465</span>,Accuracy:<span class="number">0.936094403</span>,Valid Loss:<span class="number">2.67789412</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br></pre></td></tr></table></figure><h3 id="使用单GPU训练模型"><a href="#使用单GPU训练模型" class="headerlink" title="使用单GPU训练模型"></a>使用单GPU训练模型</h3><p>深度学习的训练过程常常非常耗时，一个模型训练几个小时是家常便饭，训练几天也是常有的事情，有时候甚至要训练几十天。</p><p>训练过程的耗时主要来自于两个部分，一部分来自数据准备，另一部分来自参数迭代。</p><p>当数据准备过程还是模型训练时间的主要瓶颈时，我们可以使用更多进程来准备数据。</p><p>当参数迭代过程成为训练时间的主要瓶颈时，我们通常的方法是应用GPU或者Google的TPU来进行加速。</p><p>详见《用GPU加速Keras模型——Colab免费GPU使用攻略》</p><p><a href="https://zhuanlan.zhihu.com/p/68509398" target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/68509398</a></p><p>无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。</p><p>但如果是在公司或者学校实验室的服务器环境，存在多个GPU和多个使用者时，为了不让单个同学的任务占用全部GPU资源导致其他同学无法使用（tensorflow默认获取全部GPU的全部内存资源权限，但实际上只使用一个GPU的部分资源），我们通常会在开头增加以下几行代码以控制每个任务使用的GPU编号和显存大小，以便其他同学也能够同时训练模型。</p><p>在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p><p>注：以下代码只能在Colab 上才能正确执行。</p><p>可通过以下colab链接测试效果《tf_单GPU》：</p><p><a href="https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe" target="_blank" rel="external nofollow noopener noreferrer">https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensorflow_version 2.x</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> * </span><br><span class="line"></span><br><span class="line"><span class="comment">#打印时间分割线</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printbar</span><span class="params">()</span>:</span></span><br><span class="line">    today_ts = tf.timestamp()%(<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    hour = tf.cast(today_ts//<span class="number">3600</span>+<span class="number">8</span>,tf.int32)%tf.constant(<span class="number">24</span>)</span><br><span class="line">    minite = tf.cast((today_ts%<span class="number">3600</span>)//<span class="number">60</span>,tf.int32)</span><br><span class="line">    second = tf.cast(tf.floor(today_ts%<span class="number">60</span>),tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timeformat</span><span class="params">(m)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> tf.strings.length(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"0&#123;&#125;"</span>,m))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span>(tf.strings.format(<span class="string">"&#123;&#125;"</span>,m))</span><br><span class="line">    </span><br><span class="line">    timestring = tf.strings.join([timeformat(hour),timeformat(minite),</span><br><span class="line">                timeformat(second)],separator = <span class="string">":"</span>)</span><br><span class="line">    tf.print(<span class="string">"=========="</span>*<span class="number">8</span>+timestring)</span><br></pre></td></tr></table></figure><h4 id="GPU设置"><a href="#GPU设置" class="headerlink" title="GPU设置"></a>GPU设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gpus = tf.config.list_physical_devices(<span class="string">"GPU"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    gpu0 = gpus[<span class="number">0</span>] <span class="comment">#如果有多个GPU，仅使用第0个GPU</span></span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu0, <span class="literal">True</span>) <span class="comment">#设置GPU显存用量按需使用</span></span><br><span class="line">    <span class="comment"># 或者也可以设置GPU显存为固定使用量(例如：4G)</span></span><br><span class="line">    <span class="comment">#tf.config.experimental.set_virtual_device_configuration(gpu0,</span></span><br><span class="line">    <span class="comment">#    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) </span></span><br><span class="line">    tf.config.set_visible_devices([gpu0],<span class="string">"GPU"</span>)</span><br></pre></td></tr></table></figure><p>比较GPU和CPU的计算速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/gpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br><span class="line"><span class="number">2.24953778e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">01</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">printbar()</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">    tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">    a = tf.random.uniform((<span class="number">10000</span>,<span class="number">100</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    b = tf.random.uniform((<span class="number">100</span>,<span class="number">100000</span>),minval = <span class="number">0</span>,maxval = <span class="number">3.0</span>)</span><br><span class="line">    c = a@b</span><br><span class="line">    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = <span class="number">0</span>),axis=<span class="number">0</span>))</span><br><span class="line">printbar()</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">34</span></span><br><span class="line"><span class="number">2.24953795e+11</span></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">37</span>:<span class="number">40</span></span><br></pre></td></tr></table></figure><h4 id="准备数据-10"><a href="#准备数据-10" class="headerlink" title="准备数据"></a>准备数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure><h4 id="定义模型-10"><a href="#定义模型-10" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line">model = create_model()</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Model: "sequential"</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, 300, 7)            216874    </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d (Conv1D)              (None, 296, 64)           2304      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max_pooling1d (MaxPooling1D) (None, 148, 64)           0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">conv1d_1 (Conv1D)            (None, 146, 32)           6176      </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">max<span class="emphasis">_pooling1d_</span>1 (MaxPooling1 (None, 73, 32)            0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">flatten (Flatten)            (None, 2336)              0         </span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br><span class="line">dense (Dense)                (None, 46)                107502    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 332,856</span><br><span class="line">Trainable params: 332,856</span><br><span class="line">Non-trainable params: 0</span><br><span class="line"><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span><span class="strong">_____</span></span><br></pre></td></tr></table></figure><h4 id="训练模型-10"><a href="#训练模型-10" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizers.Nadam()</span><br><span class="line">loss_func = losses.SparseCategoricalCrossentropy()</span><br><span class="line"></span><br><span class="line">train_loss = metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">valid_loss = metrics.Mean(name=<span class="string">'valid_loss'</span>)</span><br><span class="line">valid_metric = metrics.SparseCategoricalAccuracy(name=<span class="string">'valid_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(features,training = <span class="literal">True</span>)</span><br><span class="line">        loss = loss_func(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    train_loss.update_state(loss)</span><br><span class="line">    train_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_step</span><span class="params">(model, features, labels)</span>:</span></span><br><span class="line">    predictions = model(features)</span><br><span class="line">    batch_loss = loss_func(labels, predictions)</span><br><span class="line">    valid_loss.update_state(batch_loss)</span><br><span class="line">    valid_metric.update_state(labels, predictions)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model,ds_train,ds_valid,epochs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tf.range(<span class="number">1</span>,epochs+<span class="number">1</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_train:</span><br><span class="line">            train_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> features, labels <span class="keyword">in</span> ds_valid:</span><br><span class="line">            valid_step(model,features,labels)</span><br><span class="line"></span><br><span class="line">        logs = <span class="string">'Epoch=&#123;&#125;,Loss:&#123;&#125;,Accuracy:&#123;&#125;,Valid Loss:&#123;&#125;,Valid Accuracy:&#123;&#125;'</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> epoch%<span class="number">1</span> ==<span class="number">0</span>:</span><br><span class="line">            printbar()</span><br><span class="line">            tf.print(tf.strings.format(logs,</span><br><span class="line">            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))</span><br><span class="line">            tf.print(<span class="string">""</span>)</span><br><span class="line">            </span><br><span class="line">        train_loss.reset_states()</span><br><span class="line">        valid_loss.reset_states()</span><br><span class="line">        train_metric.reset_states()</span><br><span class="line">        valid_metric.reset_states()</span><br><span class="line"></span><br><span class="line">train_model(model,ds_train,ds_test,<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">26</span></span><br><span class="line">Epoch=<span class="number">1</span>,Loss:<span class="number">1.96735072</span>,Accuracy:<span class="number">0.489200622</span>,Valid Loss:<span class="number">1.64124215</span>,Valid Accuracy:<span class="number">0.582813919</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">28</span></span><br><span class="line">Epoch=<span class="number">2</span>,Loss:<span class="number">1.4640888</span>,Accuracy:<span class="number">0.624805152</span>,Valid Loss:<span class="number">1.5559175</span>,Valid Accuracy:<span class="number">0.607747078</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">30</span></span><br><span class="line">Epoch=<span class="number">3</span>,Loss:<span class="number">1.20681274</span>,Accuracy:<span class="number">0.68581605</span>,Valid Loss:<span class="number">1.58494771</span>,Valid Accuracy:<span class="number">0.622439921</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">31</span></span><br><span class="line">Epoch=<span class="number">4</span>,Loss:<span class="number">0.937500894</span>,Accuracy:<span class="number">0.75361836</span>,Valid Loss:<span class="number">1.77466083</span>,Valid Accuracy:<span class="number">0.621994674</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">33</span></span><br><span class="line">Epoch=<span class="number">5</span>,Loss:<span class="number">0.693960547</span>,Accuracy:<span class="number">0.822199941</span>,Valid Loss:<span class="number">2.00267363</span>,Valid Accuracy:<span class="number">0.6197685</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">35</span></span><br><span class="line">Epoch=<span class="number">6</span>,Loss:<span class="number">0.519614</span>,Accuracy:<span class="number">0.870296121</span>,Valid Loss:<span class="number">2.23463202</span>,Valid Accuracy:<span class="number">0.613980412</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">37</span></span><br><span class="line">Epoch=<span class="number">7</span>,Loss:<span class="number">0.408562034</span>,Accuracy:<span class="number">0.901246965</span>,Valid Loss:<span class="number">2.46969271</span>,Valid Accuracy:<span class="number">0.612199485</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">39</span></span><br><span class="line">Epoch=<span class="number">8</span>,Loss:<span class="number">0.339028627</span>,Accuracy:<span class="number">0.920062363</span>,Valid Loss:<span class="number">2.68585229</span>,Valid Accuracy:<span class="number">0.615316093</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">41</span></span><br><span class="line">Epoch=<span class="number">9</span>,Loss:<span class="number">0.293798745</span>,Accuracy:<span class="number">0.92930305</span>,Valid Loss:<span class="number">2.88995624</span>,Valid Accuracy:<span class="number">0.613535166</span></span><br><span class="line"></span><br><span class="line">================================================================================<span class="number">17</span>:<span class="number">13</span>:<span class="number">43</span></span><br><span class="line">Epoch=<span class="number">10</span>,Loss:<span class="number">0.263130337</span>,Accuracy:<span class="number">0.936651051</span>,Valid Loss:<span class="number">3.09705234</span>,Valid Accuracy:<span class="number">0.612644672</span></span><br></pre></td></tr></table></figure><h3 id="使用多GPU训练模型"><a href="#使用多GPU训练模型" class="headerlink" title="使用多GPU训练模型"></a>使用多GPU训练模型</h3><p>如果使用多GPU训练模型，推荐使用内置fit方法，较为方便，仅需添加2行代码。</p><p>在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 GPU</p><p>注：以下代码只能在Colab 上才能正确执行。</p><p>可通过以下colab链接测试效果《tf_多GPU》：</p><p><a href="https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-" target="_blank" rel="external nofollow noopener noreferrer">https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-</a></p><p>MirroredStrategy过程简介：</p><ul><li>训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；</li><li>每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；</li><li>N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；</li><li>使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；</li><li>使用梯度求和的结果更新本地变量（镜像变量）；</li><li>当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此处在colab上使用1个GPU模拟出两个逻辑GPU进行多GPU训练</span></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    <span class="comment"># 设置两个逻辑GPU模拟多GPU训练</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tf.config.experimental.set_virtual_device_configuration(gpus[<span class="number">0</span>],</span><br><span class="line">            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>),</span><br><span class="line">             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=<span class="number">1024</span>)])</span><br><span class="line">        logical_gpus = tf.config.experimental.list_logical_devices(<span class="string">'GPU'</span>)</span><br><span class="line">        print(len(gpus), <span class="string">"Physical GPU,"</span>, len(logical_gpus), <span class="string">"Logical GPUs"</span>)</span><br><span class="line">    <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure><h4 id="准备数据-11"><a href="#准备数据-11" class="headerlink" title="准备数据"></a>准备数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure><h4 id="定义模型-11"><a href="#定义模型-11" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure><h4 id="训练模型-11"><a href="#训练模型-11" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#增加以下两行代码</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()  </span><br><span class="line"><span class="keyword">with</span> strategy.scope(): </span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br><span class="line">    </span><br><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">WARNING</span>:tensorflow:NCCL <span class="keyword">is</span> <span class="keyword">not</span> supported <span class="keyword">when</span> <span class="keyword">using</span> virtual GPUs, fallingback <span class="keyword">to</span> reduction <span class="keyword">to</span> one device</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:<span class="keyword">Using</span> MirroredStrategy <span class="keyword">with</span> devices (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>)</span><br><span class="line">Model: "sequential"</span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (<span class="keyword">type</span>)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (<span class="keyword">None</span>, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (<span class="keyword">None</span>, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (<span class="keyword">None</span>, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (<span class="keyword">None</span>, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (<span class="keyword">None</span>, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (<span class="keyword">None</span>, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (<span class="keyword">None</span>, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, <span class="keyword">validate</span> <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:CPU:0'</span>,).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="keyword">INFO</span>:tensorflow:Reduce <span class="keyword">to</span> /job:localhost/<span class="keyword">replica</span>:<span class="number">0</span>/task:<span class="number">0</span>/device:GPU:<span class="number">0</span> <span class="keyword">then</span> broadcast <span class="keyword">to</span> (<span class="string">'/job:localhost/replica:0/task:0/device:GPU:0'</span>, <span class="string">'/job:localhost/replica:0/task:0/device:GPU:1'</span>).</span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">15</span>s <span class="number">53</span>ms/step - loss: <span class="number">2.0270</span> - sparse_categorical_accuracy: <span class="number">0.4653</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7481</span> - val_loss: <span class="number">1.7517</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7578</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.5206</span> - sparse_categorical_accuracy: <span class="number">0.6045</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7938</span> - val_loss: <span class="number">1.5715</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7983</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">1.2178</span> - sparse_categorical_accuracy: <span class="number">0.6843</span> - sparse_top_k_categorical_accuracy: <span class="number">0.8547</span> - val_loss: <span class="number">1.5232</span> - val_sparse_categorical_accuracy: <span class="number">0.6327</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">13</span>ms/step - loss: <span class="number">0.9127</span> - sparse_categorical_accuracy: <span class="number">0.7648</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9113</span> - val_loss: <span class="number">1.6527</span> - val_sparse_categorical_accuracy: <span class="number">0.6296</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8201</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.6606</span> - sparse_categorical_accuracy: <span class="number">0.8321</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9525</span> - val_loss: <span class="number">1.8791</span> - val_sparse_categorical_accuracy: <span class="number">0.6158</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8219</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.4919</span> - sparse_categorical_accuracy: <span class="number">0.8799</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9725</span> - val_loss: <span class="number">2.1282</span> - val_sparse_categorical_accuracy: <span class="number">0.6037</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8112</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3947</span> - sparse_categorical_accuracy: <span class="number">0.9051</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9814</span> - val_loss: <span class="number">2.3033</span> - val_sparse_categorical_accuracy: <span class="number">0.6046</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8094</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.3335</span> - sparse_categorical_accuracy: <span class="number">0.9207</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9863</span> - val_loss: <span class="number">2.4255</span> - val_sparse_categorical_accuracy: <span class="number">0.5993</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8099</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2919</span> - sparse_categorical_accuracy: <span class="number">0.9304</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9911</span> - val_loss: <span class="number">2.5571</span> - val_sparse_categorical_accuracy: <span class="number">0.6020</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8126</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">4</span>s <span class="number">14</span>ms/step - loss: <span class="number">0.2617</span> - sparse_categorical_accuracy: <span class="number">0.9342</span> - sparse_top_k_categorical_accuracy: <span class="number">0.9937</span> - val_loss: <span class="number">2.6700</span> - val_sparse_categorical_accuracy: <span class="number">0.6077</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.8148</span></span><br><span class="line">CPU times: <span class="keyword">user</span> <span class="number">1</span>min <span class="number">2</span>s, sys: <span class="number">8.59</span> s, total: <span class="number">1</span>min <span class="number">10</span>s</span><br><span class="line">Wall <span class="type">time</span>: <span class="number">58.5</span> s</span><br></pre></td></tr></table></figure><h3 id="使用TPU训练模型"><a href="#使用TPU训练模型" class="headerlink" title="使用TPU训练模型"></a>使用TPU训练模型</h3><p>如果想尝试使用Google Colab上的TPU来训练模型，也是非常方便，仅需添加6行代码。</p><p>在Colab笔记本中：修改-&gt;笔记本设置-&gt;硬件加速器 中选择 TPU</p><p>注：以下代码只能在Colab 上才能正确执行。</p><p>可通过以下colab链接测试效果《tf_TPU》：</p><p><a href="https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s" target="_blank" rel="external nofollow noopener noreferrer">https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h4 id="准备数据-12"><a href="#准备数据-12" class="headerlink" title="准备数据"></a>准备数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">300</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()</span><br><span class="line">x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)</span><br><span class="line">x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)</span><br><span class="line"></span><br><span class="line">MAX_WORDS = x_train.max()+<span class="number">1</span></span><br><span class="line">CAT_NUM = y_train.max()+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br><span class="line">   </span><br><span class="line">ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \</span><br><span class="line">          .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">          .prefetch(tf.data.experimental.AUTOTUNE).cache()</span><br></pre></td></tr></table></figure><h4 id="定义模型-12"><a href="#定义模型-12" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    model = models.Sequential()</span><br><span class="line"></span><br><span class="line">    model.add(layers.Embedding(MAX_WORDS,<span class="number">7</span>,input_length=MAX_LEN))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">64</span>,kernel_size = <span class="number">5</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Conv1D(filters = <span class="number">32</span>,kernel_size = <span class="number">3</span>,activation = <span class="string">"relu"</span>))</span><br><span class="line">    model.add(layers.MaxPool1D(<span class="number">2</span>))</span><br><span class="line">    model.add(layers.Flatten())</span><br><span class="line">    model.add(layers.Dense(CAT_NUM,activation = <span class="string">"softmax"</span>))</span><br><span class="line">    <span class="keyword">return</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compile_model</span><span class="params">(model)</span>:</span></span><br><span class="line">    model.compile(optimizer=optimizers.Nadam(),</span><br><span class="line">                loss=losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(<span class="number">5</span>)]) </span><br><span class="line">    <span class="keyword">return</span>(model)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="训练模型-12"><a href="#训练模型-12" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#增加以下6行代码</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=<span class="string">'grpc://'</span> + os.environ[<span class="string">'COLAB_TPU_ADDR'</span>])</span><br><span class="line">tf.config.experimental_connect_to_cluster(resolver)</span><br><span class="line">tf.tpu.experimental.initialize_tpu_system(resolver)</span><br><span class="line">strategy = tf.distribute.experimental.TPUStrategy(resolver)</span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = create_model()</span><br><span class="line">    model.summary()</span><br><span class="line">    model = compile_model(model)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">WARNING:tensorflow:TPU system <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span> has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.</span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Initializing the TPU system: <span class="number">10.26</span><span class="number">.134</span><span class="number">.242</span>:<span class="number">8470</span></span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Clearing <span class="keyword">out</span> eager caches</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Finished initializing TPU system.</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:Found TPU system:</span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Workers: <span class="number">1</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Num TPU Cores Per Worker: <span class="number">8</span></span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:CPU:<span class="number">0</span>, CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">0</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">1</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">2</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">3</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">4</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">5</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">6</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU:<span class="number">7</span>, TPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:TPU_SYSTEM:<span class="number">0</span>, TPU_SYSTEM, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:<span class="number">0</span>/task:<span class="number">0</span>/device:XLA_CPU:<span class="number">0</span>, XLA_CPU, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">Model: <span class="string">"sequential"</span></span><br><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">embedding (Embedding)        (None, <span class="number">300</span>, <span class="number">7</span>)            <span class="number">216874</span>    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d (Conv1D)              (None, <span class="number">296</span>, <span class="number">64</span>)           <span class="number">2304</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d (MaxPooling1D) (None, <span class="number">148</span>, <span class="number">64</span>)           <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv1d_1 (Conv1D)            (None, <span class="number">146</span>, <span class="number">32</span>)           <span class="number">6176</span>      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling1d_1 (MaxPooling1 (None, <span class="number">73</span>, <span class="number">32</span>)            <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten (Flatten)            (None, <span class="number">2336</span>)              <span class="number">0</span>         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense (Dense)                (None, <span class="number">46</span>)                <span class="number">107502</span>    </span><br><span class="line">=================================================================</span><br><span class="line">Total params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Trainable params: <span class="number">332</span>,<span class="number">856</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(ds_train,validation_data = ds_test,epochs = <span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Train <span class="keyword">for</span> <span class="number">281</span> steps, validate <span class="keyword">for</span> <span class="number">71</span> steps</span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">12</span>s <span class="number">43</span>ms/step - loss: <span class="number">3.4466</span> - sparse_categorical_accuracy: <span class="number">0.4332</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7180</span> - val_loss: <span class="number">3.3179</span> - val_sparse_categorical_accuracy: <span class="number">0.5352</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7195</span></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.3251</span> - sparse_categorical_accuracy: <span class="number">0.5405</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7302</span> - val_loss: <span class="number">3.3082</span> - val_sparse_categorical_accuracy: <span class="number">0.5463</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">20</span>ms/step - loss: <span class="number">3.2961</span> - sparse_categorical_accuracy: <span class="number">0.5729</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7280</span> - val_loss: <span class="number">3.3026</span> - val_sparse_categorical_accuracy: <span class="number">0.5499</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2751</span> - sparse_categorical_accuracy: <span class="number">0.5924</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7276</span> - val_loss: <span class="number">3.2957</span> - val_sparse_categorical_accuracy: <span class="number">0.5543</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7217</span></span><br><span class="line">Epoch <span class="number">5</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2655</span> - sparse_categorical_accuracy: <span class="number">0.6008</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7290</span> - val_loss: <span class="number">3.3022</span> - val_sparse_categorical_accuracy: <span class="number">0.5490</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">6</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2616</span> - sparse_categorical_accuracy: <span class="number">0.6041</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7295</span> - val_loss: <span class="number">3.3015</span> - val_sparse_categorical_accuracy: <span class="number">0.5503</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7235</span></span><br><span class="line">Epoch <span class="number">7</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2595</span> - sparse_categorical_accuracy: <span class="number">0.6059</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7322</span> - val_loss: <span class="number">3.3064</span> - val_sparse_categorical_accuracy: <span class="number">0.5454</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7266</span></span><br><span class="line">Epoch <span class="number">8</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">6</span>s <span class="number">21</span>ms/step - loss: <span class="number">3.2591</span> - sparse_categorical_accuracy: <span class="number">0.6063</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7327</span> - val_loss: <span class="number">3.3025</span> - val_sparse_categorical_accuracy: <span class="number">0.5481</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7231</span></span><br><span class="line">Epoch <span class="number">9</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">19</span>ms/step - loss: <span class="number">3.2588</span> - sparse_categorical_accuracy: <span class="number">0.6062</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7332</span> - val_loss: <span class="number">3.2992</span> - val_sparse_categorical_accuracy: <span class="number">0.5521</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7257</span></span><br><span class="line">Epoch <span class="number">10</span>/<span class="number">10</span></span><br><span class="line"><span class="number">281</span>/<span class="number">281</span> [==============================] - <span class="number">5</span>s <span class="number">18</span>ms/step - loss: <span class="number">3.2577</span> - sparse_categorical_accuracy: <span class="number">0.6073</span> - sparse_top_k_categorical_accuracy: <span class="number">0.7363</span> - val_loss: <span class="number">3.2981</span> - val_sparse_categorical_accuracy: <span class="number">0.5516</span> - val_sparse_top_k_categorical_accuracy: <span class="number">0.7306</span></span><br><span class="line">CPU times: user <span class="number">18.9</span> s, sys: <span class="number">3.86</span> s, total: <span class="number">22.7</span> s</span><br><span class="line">Wall time: <span class="number">1</span>min <span class="number">1</span>s</span><br></pre></td></tr></table></figure><h3 id="使用tensorflow-serving部署模型"><a href="#使用tensorflow-serving部署模型" class="headerlink" title="使用tensorflow-serving部署模型"></a>使用tensorflow-serving部署模型</h3><p>TensorFlow训练好的模型以tensorflow原生方式保存成protobuf文件后可以用许多方式部署运行。</p><p>例如：通过 tensorflow-js 可以用javascrip脚本加载模型并在浏览器中运行模型。</p><p>通过 tensorflow-lite 可以在移动和嵌入式设备上加载并运行TensorFlow模型。</p><p>通过 tensorflow-serving 可以加载模型后提供网络接口API服务，通过任意编程语言发送网络请求都可以获取模型预测结果。</p><p>通过 tensorFlow for Java接口，可以在Java或者spark(scala)中调用tensorflow模型进行预测。</p><p>我们主要介绍tensorflow serving部署模型、使用spark(scala)调用tensorflow模型的方法。</p><h4 id="tensorflow-serving模型部署概述"><a href="#tensorflow-serving模型部署概述" class="headerlink" title="tensorflow serving模型部署概述"></a>tensorflow serving模型部署概述</h4><p>使用 tensorflow serving 部署模型要完成以下步骤。</p><ul><li><p>准备protobuf模型文件。</p></li><li><p>安装tensorflow serving。</p></li><li><p>启动tensorflow serving 服务。</p></li><li><p>向API服务发送请求，获取预测结果。</p></li></ul><p>可通过以下colab链接测试效果《tf_serving》：<br><a href="https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8" target="_blank" rel="external nofollow noopener noreferrer">https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">2.</span>x</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h4 id="准备protobuf模型文件"><a href="#准备protobuf模型文件" class="headerlink" title="准备protobuf模型文件"></a>准备protobuf模型文件</h4><p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],</span><br><span class="line">    mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>) <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看保存的模型文件</span></span><br><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">assets</span><span class="selector-tag">saved_model</span><span class="selector-class">.pb</span><span class="selector-tag">variables</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">MetaGraphDef <span class="keyword">with</span> tag-<span class="keyword">set</span>: <span class="string">'serve'</span> contains the <span class="keyword">following</span> SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'__saved_model_init_op'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'__saved_model_init_op'</span>] tensor_info:</span><br><span class="line">        dtype: DT_INVALID</span><br><span class="line">        shape: unknown_rank</span><br><span class="line">        <span class="keyword">name</span>: NoOp</span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: </span><br><span class="line"></span><br><span class="line">signature_def[<span class="string">'serving_default'</span>]:</span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">input</span>(s):</span><br><span class="line">    inputs[<span class="string">'inputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">name</span>: serving_default_inputs:<span class="number">0</span></span><br><span class="line">  The given SavedModel SignatureDef contains the <span class="keyword">following</span> <span class="keyword">output</span>(s):</span><br><span class="line">    outputs[<span class="string">'outputs'</span>] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">name</span>: StatefulPartitionedCall:<span class="number">0</span></span><br><span class="line">  Method <span class="keyword">name</span> <span class="keyword">is</span>: tensorflow/serving/predict</span><br><span class="line"><span class="keyword">WARNING</span>:tensorflow:<span class="keyword">From</span> /tensorflow<span class="number">-2.1</span><span class="number">.0</span>/python3<span class="number">.6</span>/tensorflow_core/python/ops/resource_variable_ops.py:<span class="number">1786</span>: <span class="keyword">calling</span> BaseResourceVariable.__init__ (<span class="keyword">from</span> tensorflow.python.ops.resource_variable_ops) <span class="keyword">with</span> <span class="keyword">constraint</span> <span class="keyword">is</span> deprecated <span class="keyword">and</span> will be removed <span class="keyword">in</span> a future version.</span><br><span class="line">Instructions <span class="keyword">for</span> updating:</span><br><span class="line"><span class="keyword">If</span> <span class="keyword">using</span> Keras pass *_constraint arguments <span class="keyword">to</span> layers.</span><br><span class="line"></span><br><span class="line">Defined Functions:</span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'__call__'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'_default_save_signature'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">Function</span> <span class="keyword">Name</span>: <span class="string">'call_and_return_all_conditional_losses'</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#1</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">True</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">Option</span> <span class="comment">#2</span></span><br><span class="line">      Callable <span class="keyword">with</span>:</span><br><span class="line">        Argument <span class="comment">#1</span></span><br><span class="line">          inputs: TensorSpec(shape=(<span class="keyword">None</span>, <span class="number">2</span>), dtype=tf.float32, <span class="keyword">name</span>=<span class="string">'inputs'</span>)</span><br><span class="line">        Argument <span class="comment">#2</span></span><br><span class="line">          DType: <span class="built_in">bool</span></span><br><span class="line">          <span class="keyword">Value</span>: <span class="literal">False</span></span><br><span class="line">        Argument <span class="comment">#3</span></span><br><span class="line">          DType: NoneType</span><br><span class="line">          <span class="keyword">Value</span>: <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h4 id="安装-tensorflow-serving"><a href="#安装-tensorflow-serving" class="headerlink" title="安装 tensorflow serving"></a>安装 tensorflow serving</h4><p>安装 tensorflow serving 有2种主要方法：通过Docker镜像安装，通过apt安装。</p><p>通过Docker镜像安装是最简单，最直接的方法，推荐采用。</p><p>Docker可以理解成一种容器，其上面可以给各种不同的程序提供独立的运行环境。</p><p>一般业务中用到tensorflow的企业都会有运维同学通过Docker 搭建 tensorflow serving.</p><p>无需算法工程师同学动手安装，以下安装过程仅供参考。</p><p>不同操作系统机器上安装Docker的方法可以参照以下链接。</p><p>Windows: <a href="https://www.runoob.com/docker/windows-docker-install.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.runoob.com/docker/windows-docker-install.html</a></p><p>MacOs: <a href="https://www.runoob.com/docker/macos-docker-install.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.runoob.com/docker/macos-docker-install.html</a></p><p>CentOS: <a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.runoob.com/docker/centos-docker-install.html</a></p><p>安装Docker成功后，使用如下命令加载 tensorflow/serving 镜像到Docker中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tensorflow/serving</span><br></pre></td></tr></table></figure><h4 id="启动-tensorflow-serving-服务"><a href="#启动-tensorflow-serving-服务" class="headerlink" title="启动 tensorflow serving 服务"></a>启动 tensorflow serving 服务</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!docker run -t --rm -p <span class="number">8501</span>:<span class="number">8501</span> \</span><br><span class="line">    -v <span class="string">"/Users/.../data/linear_model/"</span> \</span><br><span class="line">    -e MODEL_NAME=linear_model \</span><br><span class="line">    tensorflow/serving &amp; &gt;server.log <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="向API服务发送请求"><a href="#向API服务发送请求" class="headerlink" title="向API服务发送请求"></a>向API服务发送请求</h4><p>可以使用任何编程语言的http功能发送请求，下面示范linux的 curl 命令发送请求，以及Python的requests库发送请求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!curl -d <span class="string">'&#123;"instances": [[1.0, 2.0], [5.0,7.0]]&#125;'</span> \</span><br><span class="line">    -X POST http://localhost:<span class="number">8501</span>/v1/models/linear_model:predict</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"predictions"</span>: [[<span class="number">3.06546211</span>], [<span class="number">6.02843142</span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json,requests</span><br><span class="line"></span><br><span class="line">data = json.dumps(&#123;<span class="string">"signature_name"</span>: <span class="string">"serving_default"</span>, <span class="string">"instances"</span>: [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">5.0</span>,<span class="number">7.0</span>]]&#125;)</span><br><span class="line">headers = &#123;<span class="string">"content-type"</span>: <span class="string">"application/json"</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">'http://localhost:8501/v1/models/linear_model:predict'</span>, </span><br><span class="line">        data=data, headers=headers)</span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">"predictions"</span>]</span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">3.06546211</span>], [<span class="number">6.02843142</span>]]</span><br></pre></td></tr></table></figure><h3 id="使用spark-scala调用tensorflow2-0训练好的模型"><a href="#使用spark-scala调用tensorflow2-0训练好的模型" class="headerlink" title="使用spark-scala调用tensorflow2.0训练好的模型"></a>使用spark-scala调用tensorflow2.0训练好的模型</h3><p>本篇文章介绍在spark中调用训练好的tensorflow模型进行预测的方法。</p><p>本文内容的学习需要一定的spark和scala基础。</p><p>如果使用pyspark的话会比较简单，只需要在每个executor上用Python加载模型分别预测就可以了。</p><p>但工程上为了性能考虑，通常使用的是scala版本的spark。</p><p>本篇文章我们通过TensorFlow for Java 在spark中调用训练好的tensorflow模型。</p><p>利用spark的分布式计算能力，从而可以让训练好的tensorflow模型在成百上千的机器上分布式并行执行模型推断。</p><h4 id="spark-scala调用tensorflow模型概述"><a href="#spark-scala调用tensorflow模型概述" class="headerlink" title="spark-scala调用tensorflow模型概述"></a>spark-scala调用tensorflow模型概述</h4><p>在spark(scala)中调用tensorflow模型进行预测需要完成以下几个步骤。</p><p>（1）准备protobuf模型文件</p><p>（2）创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</p><p>（3）在spark(scala)项目中driver端加载tensorflow模型调试成功</p><p>（4）在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</p><p>（5） 在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</p><h4 id="准备protobuf模型文件-1"><a href="#准备protobuf模型文件-1" class="headerlink" title="准备protobuf模型文件"></a>准备protobuf模型文件</h4><p>我们使用tf.keras 训练一个简单的线性回归模型，并保存成protobuf文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers,optimizers</span><br><span class="line"></span><br><span class="line"><span class="comment">## 样本数量</span></span><br><span class="line">n = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成测试用数据集</span></span><br><span class="line">X = tf.random.uniform([n,<span class="number">2</span>],minval=<span class="number">-10</span>,maxval=<span class="number">10</span>) </span><br><span class="line">w0 = tf.constant([[<span class="number">2.0</span>],[<span class="number">-1.0</span>]])</span><br><span class="line">b0 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">Y = X@w0 + b0 + tf.random.normal([n,<span class="number">1</span>],mean = <span class="number">0.0</span>,stddev= <span class="number">2.0</span>)  <span class="comment"># @表示矩阵乘法,增加正态扰动</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 建立模型</span></span><br><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">inputs = layers.Input(shape = (<span class="number">2</span>,),name =<span class="string">"inputs"</span>) <span class="comment">#设置输入名字为inputs</span></span><br><span class="line">outputs = layers.Dense(<span class="number">1</span>, name = <span class="string">"outputs"</span>)(inputs) <span class="comment">#设置输出名字为outputs</span></span><br><span class="line">linear = models.Model(inputs = inputs,outputs = outputs)</span><br><span class="line">linear.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 使用fit方法进行训练</span></span><br><span class="line">linear.compile(optimizer=<span class="string">"rmsprop"</span>,loss=<span class="string">"mse"</span>,metrics=[<span class="string">"mae"</span>])</span><br><span class="line">linear.fit(X,Y,batch_size = <span class="number">8</span>,epochs = <span class="number">100</span>)  </span><br><span class="line"></span><br><span class="line">tf.print(<span class="string">"w = "</span>,linear.layers[<span class="number">1</span>].kernel)</span><br><span class="line">tf.print(<span class="string">"b = "</span>,linear.layers[<span class="number">1</span>].bias)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将模型保存成pb格式文件</span></span><br><span class="line">export_path = <span class="string">"./data/linear_model/"</span></span><br><span class="line">version = <span class="string">"1"</span>       <span class="comment">#后续可以通过版本号进行模型版本迭代与管理</span></span><br><span class="line">linear.save(export_path+version, save_format=<span class="string">"tf"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls &#123;export_path+version&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看模型文件相关信息</span></span><br><span class="line">!saved_model_cli show --dir &#123;export_path+str(version)&#125; --all</span><br></pre></td></tr></table></figure><p>模型文件信息中这些标红的部分都是后面有可能会用到的。</p><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/模型文件信息.png"></p><h4 id="创建spark-scala-项目，在项目中添加java版本的tensorflow对应的jar包依赖"><a href="#创建spark-scala-项目，在项目中添加java版本的tensorflow对应的jar包依赖" class="headerlink" title="创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖"></a>创建spark(scala)项目，在项目中添加java版本的tensorflow对应的jar包依赖</h4><p>如果使用maven管理项目，需要添加如下 jar包依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.tensorflow<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>tensorflow<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.15.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>也可以从下面网址中直接下载 org.tensorflow.tensorflow的jar包</p><p>以及其依赖的org.tensorflow.libtensorflow 和 org.tensorflowlibtensorflow_jni的jar包 放到项目中。</p><p><a href="https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0" target="_blank" rel="external nofollow noopener noreferrer">https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0</a></p><h4 id="在spark-scala-项目中driver端加载tensorflow模型调试成功"><a href="#在spark-scala-项目中driver端加载tensorflow模型调试成功" class="headerlink" title="在spark(scala)项目中driver端加载tensorflow模型调试成功"></a>在spark(scala)项目中driver端加载tensorflow模型调试成功</h4><p>我们的示范代码在jupyter notebook中进行演示，需要安装toree以支持spark(scala)。</p><!-- #region --><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：load函数的第二个参数一般都是“serve”，可以从模型文件相关信息中找到</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//注：在java版本的tensorflow中还是类似tensorflow1.0中静态计算图的模式，需要建立Session, 指定feed的数据和fetch的结果, 然后 run.</span></span><br><span class="line"><span class="comment">//注：如果有多个数据需要喂入，可以连续使用多个feed方法</span></span><br><span class="line"><span class="comment">//注：输入必须是float类型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sess = bundle.session()</span><br><span class="line"><span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">2.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"><span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">         .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">y.copyTo(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(x != <span class="literal">null</span>) x.close()</span><br><span class="line"><span class="keyword">if</span>(y != <span class="literal">null</span>) y.close()</span><br><span class="line"><span class="keyword">if</span>(sess != <span class="literal">null</span>) sess.close()</span><br><span class="line"><span class="keyword">if</span>(bundle != <span class="literal">null</span>) bundle.close()  </span><br><span class="line"></span><br><span class="line">result</span><br></pre></td></tr></table></figure><!-- #endregion --><p>输出如下：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9878292</span>))</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/TfDriver.png"></p><h4 id="在spark-scala-项目中通过RDD在executor上加载tensorflow模型调试成功"><a href="#在spark-scala-项目中通过RDD在executor上加载tensorflow模型调试成功" class="headerlink" title="在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功"></a>在spark(scala)项目中通过RDD在executor上加载tensorflow模型调试成功</h4><p>下面我们通过广播机制将Driver端加载的TensorFlow模型传递到各个executor上，并在executor上分布式地调用模型进行推断。</p><!-- #region --><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .appName(<span class="string">"TfRDD"</span>)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment">//在Driver端加载模型</span></span><br><span class="line"><span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">   .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//利用广播将模型发送到executor上</span></span><br><span class="line"><span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line"></span><br><span class="line"><span class="comment">//构造数据集</span></span><br><span class="line"><span class="keyword">val</span> rdd_data = sc.makeRDD(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">6.0</span>f,<span class="number">7.0</span>f),<span class="type">Array</span>(<span class="number">8.0</span>f,<span class="number">3.0</span>f)))</span><br><span class="line"></span><br><span class="line"><span class="comment">//通过mapPartitions调用模型进行批量推断</span></span><br><span class="line"><span class="keyword">val</span> rdd_result = rdd_data.mapPartitions(iter =&gt; &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> arr = iter.toArray</span><br><span class="line">    <span class="keyword">val</span> model = broads.value</span><br><span class="line">    <span class="keyword">val</span> sess = model.session()</span><br><span class="line">    <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(arr)</span><br><span class="line">    <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">             .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将预测结果拷贝到相同shape的Float类型的Array中</span></span><br><span class="line">    <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">    y.copyTo(result)</span><br><span class="line">    result.iterator</span><br><span class="line">    </span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd_result.take(<span class="number">5</span>)</span><br><span class="line">bundle.close</span><br></pre></td></tr></table></figure><!-- #endregion --><p>输出如下：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array(<span class="name">Array</span>(<span class="number">3.019596</span>), Array(<span class="number">3.9264367</span>), Array(<span class="number">7.8607616</span>), Array(<span class="number">15.974984</span>))</span><br></pre></td></tr></table></figure><p><img alt data-src="https://github.com/SimpCosm/eat_tensorflow2_in_30_days_ipynbs/raw/master/data/TfRDD.png"></p><h4 id="在spark-scala-项目中通过DataFrame在executor上加载tensorflow模型调试成功"><a href="#在spark-scala-项目中通过DataFrame在executor上加载tensorflow模型调试成功" class="headerlink" title="在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功"></a>在spark(scala)项目中通过DataFrame在executor上加载tensorflow模型调试成功</h4><p>除了可以在Spark的RDD数据上调用tensorflow模型进行分布式推断，</p><p>我们也可以在DataFrame数据上调用tensorflow模型进行分布式推断。</p><p>主要思路是将推断方法注册成为一个sparkSQL函数。</p><!-- #region --><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">WrappedArray</span></span><br><span class="line"><span class="keyword">import</span> org.&#123;tensorflow=&gt;tf&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TfDataFrame</span> <span class="keyword">extends</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">        .builder()</span><br><span class="line">        .appName(<span class="string">"TfDataFrame"</span>)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">        .getOrCreate()</span><br><span class="line">        <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> bundle = tf.<span class="type">SavedModelBundle</span> </span><br><span class="line">           .load(<span class="string">"/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1"</span>,<span class="string">"serve"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> broads = sc.broadcast(bundle)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造预测函数，并将其注册成sparkSQL的udf</span></span><br><span class="line">        <span class="keyword">val</span> tfpredict = (features:<span class="type">WrappedArray</span>[<span class="type">Float</span>])  =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> bund = broads.value</span><br><span class="line">            <span class="keyword">val</span> sess = bund.session()</span><br><span class="line">            <span class="keyword">val</span> x = tf.<span class="type">Tensor</span>.create(<span class="type">Array</span>(features.toArray))</span><br><span class="line">            <span class="keyword">val</span> y =  sess.runner().feed(<span class="string">"serving_default_inputs:0"</span>, x)</span><br><span class="line">                     .fetch(<span class="string">"StatefulPartitionedCall:0"</span>).run().get(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">val</span> result = <span class="type">Array</span>.ofDim[<span class="type">Float</span>](y.shape()(<span class="number">0</span>).toInt,y.shape()(<span class="number">1</span>).toInt)</span><br><span class="line">            y.copyTo(result)</span><br><span class="line">            <span class="keyword">val</span> y_pred = result(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">            y_pred</span><br><span class="line">        &#125;</span><br><span class="line">        spark.udf.register(<span class="string">"tfpredict"</span>,tfpredict)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//构造DataFrame数据集，将features放到一列中</span></span><br><span class="line">        <span class="keyword">val</span> dfdata = sc.parallelize(<span class="type">List</span>(<span class="type">Array</span>(<span class="number">1.0</span>f,<span class="number">2.0</span>f),<span class="type">Array</span>(<span class="number">3.0</span>f,<span class="number">5.0</span>f),<span class="type">Array</span>(<span class="number">7.0</span>f,<span class="number">8.0</span>f))).toDF(<span class="string">"features"</span>)</span><br><span class="line">        dfdata.show </span><br><span class="line">        </span><br><span class="line">        <span class="comment">//调用sparkSQL预测函数，增加一个新的列作为y_preds</span></span><br><span class="line">        <span class="keyword">val</span> dfresult = dfdata.selectExpr(<span class="string">"features"</span>,<span class="string">"tfpredict(features) as y_preds"</span>)</span><br><span class="line">        dfresult.show </span><br><span class="line">        bundle.close</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- #endregion --><!-- #region --><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TfDataFrame</span>.main(<span class="type">Array</span>())</span><br></pre></td></tr></table></figure><!-- #endregion --><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">+----------+</span></span><br><span class="line">|  features|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line">|[1.0, 2.0]|</span><br><span class="line">|[3.0, 5.0]|</span><br><span class="line">|[7.0, 8.0]|</span><br><span class="line"><span class="code">+----------+</span></span><br><span class="line"></span><br><span class="line"><span class="code">+----------+</span>---------+</span><br><span class="line">|  features|  y<span class="emphasis">_preds|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br><span class="line"><span class="emphasis">|[1.0, 2.0]| 3.019596|</span></span><br><span class="line"><span class="emphasis">|[3.0, 5.0]|3.9264367|</span></span><br><span class="line"><span class="emphasis">|[7.0, 8.0]| 8.828995|</span></span><br><span class="line"><span class="emphasis">+----------+---------+</span></span><br></pre></td></tr></table></figure><p>以上我们分别在spark 的RDD数据结构和DataFrame数据结构上实现了调用一个tf.keras实现的线性回归模型进行分布式模型推断。</p><p>在本例基础上稍作修改则可以用spark调用训练好的各种复杂的神经网络模型进行分布式模型推断。</p><p>但实际上tensorflow并不仅仅适合实现神经网络，其底层的计算图语言可以表达各种数值计算过程。</p><p>利用其丰富的低阶API，我们可以在tensorflow2.0上实现任意机器学习模型，</p><p>结合tf.Module提供的便捷的封装功能，我们可以将训练好的任意机器学习模型导出成模型文件并在spark上分布式调用执行。</p><p>这无疑为我们的工程应用提供了巨大的想象空间。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/lyhue1991/eat_tensorflow2_in_30_days" target="_blank" rel="external nofollow noopener noreferrer">eat_tensorflow2_in_30_days</a></li><li><a href="https://github.com/snowkylin/tensorflow-handbook" target="_blank" rel="external nofollow noopener noreferrer">简单粗暴Tensorflow2</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近年来，深度神经网络技术被大规模地使用在搜索、推荐、广告、翻译、语音、图像和视频等领域。与此同时，深度学习也在推动一些人类最重大的工程挑战，比如自动驾驶技术、医疗诊断和预测、个性化学习、加速科学发展（比如天文发现）、跨语言的自由交流（比如实时翻译），更通用的人工智能系统（比如 AlphaGo）等。&lt;/p&gt;
&lt;p&gt;TensorFlow 是开源的端到端的机器学习平台，提供了丰富的工具链，推动了机器学习的前沿研究，支撑了大规模生产使用，支持多平台灵活部署。2019年10月，谷歌正式发布TensorFlow 2.0，相比于TensorFlow 1.0，TensorFlow 2 重点关注易用性，默认推荐使用 Keras 作为高阶 API，同时兼具可扩展性和高性能，默认为动态图方式执行。本文作为 Tensorflow2 学习笔记，主要参考&lt;a href=&quot;https://github.com/lyhue1991/eat_tensorflow2_in_30_days&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;eat_tensorflow2_in_30_days&lt;/a&gt;，对照着原教程在Docker环境下对于TensorFlow2进行学习，感谢原作者的贡献。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2021-01-05_tf_logo_social.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="tensorflow" scheme="http://houmin.cc/tags/tensorflow/"/>
    
      <category term="深度学习" scheme="http://houmin.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【深度学习】卷积神经网络</title>
    <link href="http://houmin.cc/posts/77a2fe8f/"/>
    <id>http://houmin.cc/posts/77a2fe8f/</id>
    <published>2020-12-15T12:00:44.000Z</published>
    <updated>2021-01-03T14:57:50.033Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在 <a href="https://houmin.cc/posts/e1b7513f/">人工神经网络</a> 中我们介绍了人工神经网络这样的全连接网络，它是深度学习的基础。然而，全连接网络存在着参数数量过多等问题，本文将介绍 <strong>卷积神经网络(Convolutional Neural Network, CNN)</strong> 。</p><a id="more"></a><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><h3 id="局部感知"><a href="#局部感知" class="headerlink" title="局部感知"></a>局部感知</h3><h3 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h3><h3 id="下采样操作"><a href="#下采样操作" class="headerlink" title="下采样操作"></a>下采样操作</h3><h2 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h2><h3 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h3><p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：</p><p><img alt="卷积层运算过程" data-src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-06-19-juanji.gif"></p><p>这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p><p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：</p><p><img alt="25种不同的卷积核" data-src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-06-19-150926.jpg"></p><p><strong>总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。</strong></p><p>这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。<br> 在这个卷积层，有两个关键操作：</p><ul><li>局部关联。每个神经元看做一个滤波器(filter)</li><li>窗口(receptive field)滑动， filter对局部数据计算</li></ul><p>先介绍卷积层遇到的几个名词：</p><ul><li>深度/depth（解释见下图）</li><li>步长/stride （窗口一次滑动的长度）</li><li>填充值/zero-padding</li></ul><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-cf4779043e6ceba1.png"></p><p>填充值是什么呢？以下图为例子，比如有这么一个5 <em> 5的图片（一个格子一个像素），我们滑动窗口取2</em>2，步长取2，那么我们发现还剩下1个像素没法滑完，那怎么办呢？</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-b23ec0c9735cc013.png"></p><p>那我们在原先的矩阵加了一层填充值，使得变成6*6的矩阵，那么窗口就可以刚好把所有像素遍历完。这就是填充值的作用。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-4155a0952c380247.png"></p><p>卷积的计算（注意，下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值）</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-549acda410aa48fe.jpg"></p><p> 这里的蓝色矩阵就是输入的图像，粉色矩阵就是<strong>卷积层的神经元</strong>，这里表示了有两个神经元（w0,w1）。<strong>绿色矩阵就是经过卷积运算后的输出矩阵</strong>，这里的步长设置为2。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-54d90dad81065a59.jpg"></p><p>蓝色的矩阵(输入图像)对粉色的矩阵（filter）进行矩阵内积计算并将三个内积运算的结果与偏置值b相加（比如上面图的计算：2+（-2+1-2）+（1-2-2） + 1= 2 - 3 - 3 + 1 = -3），计算后的值就是绿框矩阵的一个元素。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-e3a757491994589f.png"></p><p>下面的动态图形象地展示了卷积层的计算过程：</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-5ca69abe03f57d72.gif"></p><p><strong>参数共享机制</strong></p><ul><li>在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。</li><li>需要估算的权重个数减少: AlexNet 1亿 =&gt; 3.5w</li><li>一组固定的权重和不同窗口内数据做内积: 卷积</li></ul><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-7b4cfaa9f9cf2930.png"></p><h3 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a>Pooling Layer</h3><p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</p><p><img alt="池化层过程" data-src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-06-19-chihua.gif"></p><p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p><p><strong>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</strong></p><p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。<br> 简而言之，<strong>如果输入是图像的话，那么池化层的最主要作用就是压缩图像</strong>。</p><p>这里再展开叙述池化层的具体作用。</p><ol><li><p>特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</p></li><li><p>特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。</p></li><li><p>在一定程度上防止过拟合，更方便优化。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-3fb425fc223b853a.jpg"></p><p>池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。<br> 这里就说一下Max pooling，其实思想非常简单。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-9338f08f69f43297.jpg"></p><p>对于每个2 <em> 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2 </em> 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p></li></ol><h3 id="ReLU-Layer"><a href="#ReLU-Layer" class="headerlink" title="ReLU Layer"></a>ReLU Layer</h3><p>把卷积层输出结果做非线性映射。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-6eafbff92d32d7d9.jpg"></p><p>CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。</p><p><img alt="img" data-src="https:////upload-images.jianshu.io/upload_images/1845730-8898029e12b1bcf6.png"></p><p>激励层的实践经验：<br> ①不要用sigmoid！不要用sigmoid！不要用sigmoid！<br> ② 首先试RELU，因为快，但要小心点<br> ③ 如果2失效，请用Leaky ReLU或者Maxout<br> ④ 某些情况下tanh倒是有不错的结果，但是很少</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h3 id="Fully-Connected-Layer"><a href="#Fully-Connected-Layer" class="headerlink" title="Fully Connected Layer"></a>Fully Connected Layer</h3><p>这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p><p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p><p><img alt="全连接层" data-src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-06-19-quanlianjie.png"></p><p>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：</p><p><strong>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</strong></p><p><strong><img alt="LeNet-5网络结构" data-src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-06-19-lenet.png"></strong></p><h3 id="Loss-Layer"><a href="#Loss-Layer" class="headerlink" title="Loss Layer"></a>Loss Layer</h3><h2 id="一般CNN结构依次为"><a href="#一般CNN结构依次为" class="headerlink" title="一般CNN结构依次为"></a>一般CNN结构依次为</h2><p>1.INPUT<br>2.[[CONV -&gt; RELU]<em>N -&gt; POOL?]</em>M<br>3.[FC -&gt; RELU]*K<br>4.FC</p><h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://docs.google.com/presentation/d/1N5EgIfY9nst75cq20M27SjOSiSG1c7uAhZ0RngwGVzc/edit#slide=id.g27caba1471_82_12" target="_blank" rel="external nofollow noopener noreferrer">https://docs.google.com/presentation/d/1N5EgIfY9nst75cq20M27SjOSiSG1c7uAhZ0RngwGVzc/edit#slide=id.g27caba1471_82_12</a></li><li><a href="https://cuijiahua.com/blog/2018/12/dl-10.html" target="_blank" rel="external nofollow noopener noreferrer">https://cuijiahua.com/blog/2018/12/dl-10.html</a></li><li><a href="https://nndl.github.io/" target="_blank" rel="external nofollow noopener noreferrer">https://nndl.github.io/</a></li><li><a href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" target="_blank" rel="external nofollow noopener noreferrer">https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53</a></li><li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Definition" target="_blank" rel="external nofollow noopener noreferrer">https://en.wikipedia.org/wiki/Convolutional_neural_network#Definition</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 &lt;a href=&quot;https://houmin.cc/posts/e1b7513f/&quot;&gt;人工神经网络&lt;/a&gt; 中我们介绍了人工神经网络这样的全连接网络，它是深度学习的基础。然而，全连接网络存在着参数数量过多等问题，本文将介绍 &lt;strong&gt;卷积神经网络(Convolutional Neural Network, CNN)&lt;/strong&gt; 。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2021-01-03_convolutional-neural-network.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="深度学习" scheme="http://houmin.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积" scheme="http://houmin.cc/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="神经网络" scheme="http://houmin.cc/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="CNN" scheme="http://houmin.cc/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>【深度学习】人工神经网络</title>
    <link href="http://houmin.cc/posts/e1b7513f/"/>
    <id>http://houmin.cc/posts/e1b7513f/</id>
    <published>2020-12-14T12:00:30.000Z</published>
    <updated>2021-01-03T09:18:07.023Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>除了SVM、决策树等算法，人工神经网络是机器学习的另一个重要分支，它是深度学习的基础。人工神经网络是通过模仿生物神经网络系统结构和功能，提出了一种 <strong>非线性统计性模型</strong> ，用于对函数的近似和估计。人工神经网络以其独特的网络结构和处理信息的方法，在自动控制领域、组合优化问题、模式识别、图形处理、自然语言处理等诸多领域，已经取得了辉煌的成绩，本文将介绍其基本模型和核心算法实现。</p><a id="more"></a><h2 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经网络最早的设计思路来自于生物学中的神经元，从结构、实现机理和功能上模拟神经网络系统：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dl-neuron.png"></p><p>回想下高中生物知识，传统的神经元模型由树突、细胞核、细胞体、突触和神经末梢组成：</p><ul><li>突触前（神经元）细胞的树突或细胞体接受刺激，产生兴奋或抑制。</li><li>动作电位传到神经末梢，导致神经递质释放。</li><li>使突触后（神经元）细胞的树突或细胞体接受刺激。</li></ul><p>对于人工神经网络，神经元的输入 $x_i$ 对应于生物神经元的树突，输入 $x_i$ 向细胞体传播脉冲，相当于输入权值 $w_i$，通过细胞核对输入的数据和权值参数进行加权求和。传播细胞体的脉冲相当于人工神经元的激活函数，最终输出结果 $y$ 作为下一个神经元的输入。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dl-neuron2.png"></p><p>可以看到人工神经网络最基本的处理单元 —— 神经元的基本组成单位为：</p><ul><li>连接 Connetion：神经元中数据流动的表达方式</li><li>求和节点 Summation Node ：对输入信号和权值的乘积进行求和</li><li>激活函数 Activate Function：一个非线性函数，对输出信号进行控制</li></ul><p>将上述模型进行抽象，得到神经元基本模型：</p><ul><li>$x_1, x_2, …, x_n$ 为输入信号的各个分量</li><li>$w_1, w_2, …, w_n$ 为神经元各个突触的权值</li><li>$b$  为神经元的偏置参数</li><li>$\sum$  为求和节点，$ z = \sum_{i=1}^n w_i * x_i  + b $</li><li>$f$ 为激活函数，一般为非线性函数</li><li>$y$ 为该神经元的输出</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dp-neuron3.svg"></p><p>该神经元模型的数学表达式为：</p><script type="math/tex; mode=display">y = f (\sum_{i=1}^n w_i * x_i  + b )</script><p>可以看到，神经元模型就是一个基本的函数，本质上做的是数据的映射，$ \mathbf{X} $ 为输入向量，$y$ 为输出变量，神经元对应着 $y = \varphi(\mathbf{X})$ ，而 $\mathbf{W}$ 和 $b$ 则是这个函数的参数。单个神经元如果没有加上激活函数，可以看作是一个线性模型，而 $\mathbf{W}$ 和 $b$ 则是这个线性模型的参数。</p><script type="math/tex; mode=display">y = \mathbf{W}^\mathsf{T} \mathbf{X} + b</script><p>回想下本科的线性几何课程，<strong>线性模型的任何组合仍然是线性模型</strong>。但是对于现实数据而言，很多数据都是<strong>线性不可分</strong>的，需要的是<strong>非线性模型</strong>。另外，对于一个拥有很多特征的复杂数据集进行线性回归是代价很高的，需要高昂的计算代价。因此，我们需要在神经元模型中引入一个 <strong>非线性单元</strong>，也就是这里的 <strong>激活函数</strong>，使得神经元模型能够更好的解决复杂的数据分布问题。</p><h3 id="多层神经网络"><a href="#多层神经网络" class="headerlink" title="多层神经网络"></a>多层神经网络</h3><p>人工神经网络由许多神经元组合而成，神经元组成的信息处理网络具有并行分布结构，因此有了更复杂的人工神经网络。一个多层人工神经网络 ANN 由输入层、隐藏层、输出层组成，第 <code>k-1</code> 层网络神经元的输出是第 <code>k</code> 层神经元的输入。下面是一个简单的两层神经网络，我们将输入层称为第零层：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dp-back-propagation.svg"></p><p>人工神经网络的输入层与输出层的节点数往往固定，这取决于我们的输入和输出，而隐层数和隐层节点则可以自由指定。人工神经网络的关键不是节点而是连接，每层的神经元与下一层的多个神经元相连接，每条连接线都有独自的权重参数，这些参数往往通过训练得到。</p><p>在这个图中，$w_{ij}$ 表示第 $k-1$ 层的第 $i$ 个节点到的权重第 $k$ 层的第 $j$ 个节点</p><p>根据上面的公式，我们可以容易得出：</p><script type="math/tex; mode=display">o_1 = f(w_{11} * x_1 + w_{21} * x_2 + w_{31} * x_3 + b_1) \\o_2 = f(w_{12} * x_1 + w_{22} * x_2 + w_{32} * x_3 + b_2) \\o_3 = f(w_{13} * x_1 + w_{23} * x_2 + w_{33} * x_3 + b_3) \\o_3 = f(w_{14} * x_1 + w_{24} * x_2 + w_{34} * x_3 + b_4)</script><p>令</p><script type="math/tex; mode=display">\mathbf{X} = \begin{bmatrix}x_1\\ x_2\\ x_3 \end{bmatrix},\mathbf{w_1} = \begin{bmatrix}w_{11}\\ w_{21}\\ w_{31} \end{bmatrix},\mathbf{w_2} = \begin{bmatrix}w_{12}\\ w_{22}\\ w_{32} \end{bmatrix},\mathbf{w_3} = \begin{bmatrix}w_{13}\\ w_{23}\\ w_{33} \end{bmatrix},\mathbf{w_4} = \begin{bmatrix}w_{14}\\ w_{24}\\ w_{34} \end{bmatrix}</script><p>则</p><script type="math/tex; mode=display">o_1 = f(\mathbf{w_1}^\mathsf{T} \mathbf{X} + b_1)\\o_2 = f(\mathbf{w_2}^\mathsf{T} \mathbf{X} + b_2)\\o_3 = f(\mathbf{w_3}^\mathsf{T} \mathbf{X} + b_3)\\o_4 = f(\mathbf{w_4}^\mathsf{T} \mathbf{X} + b_4)\\</script><p>令</p><script type="math/tex; mode=display">\mathbf{o} = \begin{bmatrix} o_1\\ o_2\\ o_3\\ o_4 \end{bmatrix}, \mathbf{W} = \begin{bmatrix} \mathbf{w_1}^\mathsf{T} \\ \mathbf{w_2}^\mathsf{T}\\ \mathbf{w_3}^\mathsf{T}\\ \mathbf{w_4}^\mathsf{T} \end{bmatrix} = \begin{bmatrix}w_{11}, w_{21}, w_{31} \\w_{12}, w_{22}, w_{32}\\w_{13}, w_{23}, w_{33}\\w_{14}, w_{24}, w_{34} \end{bmatrix},\mathbf{B} = \begin{bmatrix}b_1\\ b_2\\ b_3\\ b_4 \end{bmatrix}\\f(\begin{bmatrix} o_1\\ o_2\\ o_3\\ o_4 \end{bmatrix}) = \begin{bmatrix} f(o_1)\\ f(o_2)\\ f(o_3)\\ f(o_4) \end{bmatrix}</script><p>则有，</p><script type="math/tex; mode=display">\mathbf{o} = f ( \mathbf{W} \mathbf{X} + \mathbf{B})</script><p>在这个公式说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数，其中每个变量的定义如下：</p><ul><li>$f$ 是激活函数</li><li>$\mathbf{W}$ 是第 $k$ 层的权重矩阵<ul><li>它的每一个行向量对应着第 $k$ 层的每个节点，也就是说如果第 $k$ 层的有 $N$ 个节点，则 $\mathbf{W}$ 共有 $N$ 个行向量</li><li>如果第$k-1$层有 $M$ 个节点，则 $\mathbf{W}$ 的每个行向量的长度为 $M$ ，对应着第$k-1$层$M$ 个节点的求和</li></ul></li><li>$B$ 是第 $k$ 层的偏置向量，其长度与第 $k$ 层的节点数相同</li><li>$\mathbf{X}$ 是第 $k$ 层的输入向量，也正是第 $k-1$ 层的输出向量</li><li>$\mathbf{o}$ 是第 $k$ 层输出向量</li></ul><p>因此，如果我们将上面的简单神经网络增加层数到4层，如下图所示（注意，这里画图有点偷懒，中间应该是全连接网络，为了简单这里没有全部连起来）</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dl-multi-layer-ann.svg"></p><p>则我们可以算出每一层的输出向量如下：</p><script type="math/tex; mode=display">\mathbf{o_1} = f ( \mathbf{W_1} \mathbf{X} + \mathbf{B_1}) \\\mathbf{o_2} = f ( \mathbf{W_2} \mathbf{o_1} + \mathbf{B_2}) \\\mathbf{o_3} = f ( \mathbf{W_3} \mathbf{o_2} + \mathbf{B_3}) \\\mathbf{Y} = f ( \mathbf{W_4} \mathbf{o_3} + \mathbf{B_4}) \\</script><h2 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h2><h3 id="前向传播算法"><a href="#前向传播算法" class="headerlink" title="前向传播算法"></a>前向传播算法</h3><p>OK，假设我们现在根据某个应用场景，构建了一个多层神经网络的模型，并且根据训练数据获得了网络的所有参数（输入层、输出层、隐层的节点数、权重矩阵 $W$ 和偏置向量 $B$）。如果这个模型参数合理的话，那么对于新的输入数据，这个模型能够预测出合理的输出结果。所谓的预测，就是将向量化的数据从神经网络的输入层开始输入，顺着数据流动的方向在网络中计算，直到数据传输到输出层并输出，这也就是 <strong>前向传播算法</strong>。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dp-forward-propagation.svg"></p><p>假设我们有如下定义：</p><ul><li>$w_{ij}^k$：第 $k$ 层的第 $j$ 个节点对于来自第 $k-1$层的第 $i$ 个节点的权重</li><li>$b_j^k$：第 $k$ 层的第 $j$ 个节点的偏置</li><li>$net_j^k$：第 $k$ 层的第 $j$ 个节点的 <code>net input value</code>，也就是激活函数的输入</li><li>$o_j^k$：第 $k$ 层的第 $j$ 个节点的输出，也就是激活函数的输出</li><li>$r_k$：第 $k$ 层的节点数目</li><li>$M$：神经网络输入向量 $X$ 的大小，即 $r_0 = M$</li><li>$L$：全连接神经网络的层数，最简单的神经元的层数为 $1$</li><li>$N$：神经网络输出向量 $Y$ 的大小，即 $r_L = N$</li></ul><p>则对于第 $k$ 层的第 $j$ 个节点，有</p><script type="math/tex; mode=display">net_j^k = \sum_{i=1}^{r_{k-1}} w_{ij}^{k}o_i^{k-1} + b_j^k</script><p>也就是说，第 $k$ 层的第 $j$ 个节点的净输入为第 $k-1$ 层的所有节点输出值的加权和再加上第 $k$ 层第 $j$ 个节点的偏置。</p><ul><li>当 $k=1$时，第 $k-1=0$ 层的输出向量 $O^0$ 就是输入向量 $X$，此时 $\begin{bmatrix}o_1^0, o_2^0, \dots, o_M^0 \end{bmatrix} = \begin{bmatrix}x_1, x_2, \dots, x_M \end{bmatrix} $</li><li>当 $k = L$时，也即是最后一层的输出向量 $O^L$ 就是输出向量 $Y$，此时 $\begin{bmatrix}o_1^L, o_2^L, \dots, o_N^L \end{bmatrix} = \begin{bmatrix}y_1, y_2, \dots, y_N \end{bmatrix} $</li></ul><p>因此，一旦确定了神经网络的参数，就可以通过上述公式迭代算出神经网络的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(network_structure, weight, bias)</span>:</span></span><br><span class="line">  <span class="keyword">for</span> j = <span class="number">1.</span>..M</span><br><span class="line">  o[<span class="number">0</span>][j] = x[j]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> k = <span class="number">1.</span>..L</span><br><span class="line">    <span class="keyword">for</span> j = <span class="number">1.</span>..r[k]</span><br><span class="line">      net[k][j] = bias[k][j]</span><br><span class="line">      <span class="keyword">for</span> i = <span class="number">1.</span>..r[k<span class="number">-1</span>]</span><br><span class="line">        net[k][j] += weight[k][i][j] * o[k<span class="number">-1</span>][i] </span><br><span class="line">      o[k][j] = f(net[k][j])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> j = <span class="number">1.</span>..N</span><br><span class="line">    y[j] = o[L][j]</span><br></pre></td></tr></table></figure><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>如上所说，一旦确定好神经网络模型中的权值矩阵 $W$ 和偏置向量 $B$ ，就可以基于模型进行预测了。现在的问题是，如何得到这些参数的值呢？这就要通过原始数据<strong>训练</strong>得到了。神经网络的训练实际上是通过算法不算修改权值矩阵$W$ 和偏置向量 $B$ ，使其尽可能与真实模型逼近，以使得整个神经网络的预测效果最佳。具体做法如下：</p><ol><li>给所有权值矩阵$W$ 和偏置向量 $B$ 赋予随机值</li><li>利用前向传播算法基于随机的权值矩阵$W$ 和偏置向量 $B$ 来得到训练样本的预测值 $\hat{y}$</li><li>计算损失函数 $loss = (\hat{y} - y)^2$，优化目标是改变神经网络中的参数，使得损失函数的值最小</li></ol><p>因此，对于神经网络的优化问题就转换为对参数的优化，减少损失直至损失收敛，当损失函数收敛到一定程度时就可以结束训练，保存训练后神经网络的参数。</p><p>在微积分中，对多元函数的参数求偏导，求得参数的偏导数以向量的形式表达就是 <strong>梯度</strong>。如下图所示，对于损失函数 $loss$ 的参数 $\theta$ 求梯度即是 $\frac{\partial{loss}}{\partial{\theta}}$ 。在数学上，梯度越大，则函数的变化越大。也就是说，沿着梯度向量的方向函数增加最快，易于找到函数的最大值；沿着与梯度向量相反的方向函数减少最快，易于找到函数的最小值。</p><p>对于损失函数来说，为了找到其最小值，需要沿着与梯度向量相反的方向 $-\frac{\partial{loss}}{\partial{\theta}}$ 更新参数 $\theta$，这样可以使得梯度减少最快，直至损失收敛至最小值。这即是 <strong>梯度下降算法 （Gradient Descent）</strong>，其基本公式为：</p><script type="math/tex; mode=display">\theta = \theta - \alpha \frac{\partial{loss}}{\partial{\theta}}</script><p>其中，$\alpha \in \mathbf{R}$ 为学习率，用于控制梯度下降的幅度。我们可以将损失函数看成是参数 $\theta$ 的函数，优化的目的就是找到参数 $\theta_x$ 使得损失函数最小。具体的做法就是，每次计算参数 $\theta_i$ 在当前位置时函数的梯度，然后让参数 $\theta_i$ 顺着梯度的反方向前进一段距离，不断重复该过程，直到梯度趋近于零的时候，算法认为找到的损失函数的最小值并停止计算。此时的参数即是目标 $\theta_x$ 神经网络的参数。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2021-01-02_gradient-descent.png"></p><p>梯度下降的算法变种有很多，下面对常用的梯度下降算法进行介绍。</p><h4 id="批量梯度下降算法-BGD"><a href="#批量梯度下降算法-BGD" class="headerlink" title="批量梯度下降算法 BGD"></a>批量梯度下降算法 BGD</h4><p>批量梯度下降算法 <strong>Batch Gradient Descent</strong> 中，所有样本都参与参数 $w$ 的更新。假设有 $m$ 个样本，$m$ 个样本都参与调整参数 $w$，因此得到一个标准的梯度。</p><ul><li>优点：易于得到全局最优解，总体迭代次数不多</li><li>缺点：当样本数目很多时，训练时间过长，收敛速度变慢</li></ul><h4 id="随机梯度下降算法-SGD"><a href="#随机梯度下降算法-SGD" class="headerlink" title="随机梯度下降算法 SGD"></a>随机梯度下降算法 SGD</h4><p>随机梯度下降算法 <strong>Stochastic Gradient Descent</strong> 中，梯度是从 $m$ 个样本中随机抽取 $n$ 个样本进行求解的</p><ul><li>优点：训练速度快，每次迭代计算量少</li><li>缺点：准确度下降，得到的不一定是全局最优，总体迭代次数比较多</li></ul><h4 id="小批量随机梯度下降算法-Min-batch-SGD"><a href="#小批量随机梯度下降算法-Min-batch-SGD" class="headerlink" title="小批量随机梯度下降算法 Min-batch SGD"></a>小批量随机梯度下降算法 Min-batch SGD</h4><p>小批量随机梯度下降算法是对BGD和SGD的折衷方法：每次随机从 $m$ 个样本中抽取 $k$ 进行迭代求梯度，每一次迭代的抽取方式都是随机的，因此部分样本会重复。这样做的好处是，计算梯度时让数据和数据之间产生关联，避免数据最终只能收敛到局部最优解。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2021-01-02_sgd-bgd.jpg"></p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>反向传播算法，全称是 <strong>back propagation of errors</strong>，本质上就是利用梯度下降算法来计算神经网络的参数，它从输出层开始向输入层的方向一层一层往前算起，计算出每一层误差的梯度，从而更新神经网络的参数，下面将从数学上推导反向传播算法的具体实现。</p><p>假设我们有以下定义：</p><ul><li>$w_{ij}^k$：第 $k$ 层的第 $j$ 个节点对于来自第 $k-1$层的第 $i$ 个节点的权重</li><li>$b_j^k$：第 $k$ 层的第 $j$ 个节点的偏置</li><li>$net_j^k$：第 $k$ 层的第 $j$ 个节点的 <code>net input value</code>，也就是激活函数的输入</li><li>$o_j^k$：第 $k$ 层的第 $j$ 个节点的输出，也就是激活函数的输出</li><li>$r_k$：第 $k$ 层的节点数目</li><li>$M$：神经网络输入向量 $X$ 的大小，即 $r_0 = M$</li><li>$L$：全连接神经网络的层数，最简单的神经元的层数为 $1$</li><li>$N$：神经网络输出向量 $Y$ 的大小，即 $r_L = N$</li><li>前馈神经网络，其中 $\theta$ 是网络的参数，对应的就是权值 $w_{ij}^k$ 和偏置 $b_j^k$</li><li>$E(\theta)$：神经网络的预测值与实际值的误差函数</li><li>$f$：激活函数</li><li>$f_o$：输出层的激活函数</li></ul><p>根据梯度下降算法，我们的目标是找到最佳的网络参数，使得误差函数最小：</p><script type="math/tex; mode=display">\theta^{t+1} = \theta^t - \alpha \frac{\partial E(\theta)}{\partial \theta}</script><p>其中 $\theta^t$ 是神经网络在计算梯度的第 $t$ 次迭代中的参数。</p><p>根据最小均方误差，我们可以得到</p><script type="math/tex; mode=display">E(\theta) = \frac{1}{2N} \sum_{i=1}^N (\hat{y_i} - y_i)^2</script><h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><p>计算梯度</p><script type="math/tex; mode=display">\frac{\partial E(\theta)}{\partial w_{ij}^k} = \frac{1}{N} \sum_{d=1}^N \frac{\partial}{\partial w_{ij}^k} ( \frac{1}{2} (\hat{y_d} - y_d)^2 ) = \frac{1}{N} \sum_{d=1}^N \frac{\partial E_d}{\partial w_{ij}^k}</script><p>其中，</p><script type="math/tex; mode=display">E = \frac{1}{2} (\hat{y} - y)^2</script><p>这里为了表达方便，省略了 $E_d$, $\hat{y_d}$, $y_d$ 中的下标 $d$</p><p>也就是说，总的误差函数梯度是输出层每一个节点误差梯度的算术平均值，接下来我们看如何计算 $\frac{\partial E}{\partial w_{ij}^k}$</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dp-forward-propagation.svg"></p><p>根据链式法则，</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{ij}^k} = \frac{\partial E}{\partial net_{j}^k} \frac{\partial net_{j}^k}{\partial w_{ij}^k}</script><p>我们将右式第一项记作误差，也就是第 $k$ 层第 $j$ 个节点的误差</p><script type="math/tex; mode=display">\delta_j^k = \frac{\partial E}{\partial net_{j}^k}</script><p>对于右式第二项，我们先回顾前向传播算法有</p><script type="math/tex; mode=display">net_j^k = \sum_{i=1}^{r_{k-1}} w_{ij}^{k}o_i^{k-1} + b_j^k \\</script><p>为了简化数学表达，我们可以把第 $k$ 层第 $j$ 个节点的偏置视作来自第$k-1$ 层的节点0的输入，其中$o_0^{k-1} = 1$，则</p><script type="math/tex; mode=display">b_j^k = w_{0j}^k =  w_{0j}^k * o_0^{k-1}</script><p> 故</p><script type="math/tex; mode=display">net_j^k = \sum_{i=1}^{r_{k-1}} w_{ij}^{k}o_i^{k-1} + b_j^k = \sum_{i=0}^{r_{k-1}} w_{ij}^{k}o_i^{k-1}</script><p>所以计算梯度简化如下</p><script type="math/tex; mode=display">\frac{\partial net_{j}^k}{\partial w_{ij}^k} = \frac{\partial}{\partial w_{ij}^k} (\sum_{l=0}^{r_{k-1}} w_{lj}^{k}o_l^{k-1}) = \sum_{l=0}^{r_{k-1}} \frac{\partial}{\partial w_{ij}^k} (w_{lj}^{k}o_l^{k-1}) = 0 + \dots + \frac{\partial}{\partial w_{ij}^k} (w_{ij}^{k}o_i^{k-1}) + \dots + 0 = o_i^{k-1}</script><p>综上，</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{ij}^k} = \delta_j^k o_i^{k-1}</script><p>得到梯度的表达式之后，可以根据是输出层还是隐藏层具体计算。</p><h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>如向所述，我们将通过梯度下降的方法来迭代计算神经网络的参数，首先看输出层，我们需要计算出 $\delta_j^L$。</p><script type="math/tex; mode=display">\delta_j^L = \frac{\partial E}{\partial net_{j}^L}</script><p>而</p><script type="math/tex; mode=display">E = \frac{1}{2} (\hat{y} - y) ^ 2 = \frac{1}{2} (f_o(net_j) - y)^2 \\</script><p>则有</p><script type="math/tex; mode=display">\delta_j^L = \frac{\partial E}{\partial net_{j}^L} = (f_o(net_j) - y)f_o^\prime(net_j) = (\hat{y} - y)f_o^\prime(net_j)</script><p>于是得到梯度的计算公式：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{ij}^L} = \delta_j^L o_i^{L-1} = (\hat{y} - y)f_o^\prime(net_j) o_i^{L-1}</script><h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><p>对于隐藏层，我们也需要算出第 $k$ 层第 $j$ 个节点的误差 $\delta_j^k$ ，它将通过影响第 $k+1$ 层所有节点的净输入 $net_i^{k+1}$来影响最终的误差$E$。</p><p>因此，我们通过链式法则将 $E$ 先对第 $k+1$ 层所有节点的净输入 $net_i^{k+1}$ 求导，然后再将 $net_i^{k+1}$ 对 $net_i^{k}$  求导：</p><script type="math/tex; mode=display">\delta_j^k = \frac{\partial E}{\partial net_j^k} = \sum_{l=1}^{r_{k+1}}  \frac{\partial E}{\partial net_l^{k+1}}  \frac{\partial net_l^{k+1}}{\partial net_j^k}</script><p>注意这里的 $l$ 范围是 1 到 $r^{k+1}$，$l$ 没有从$0$开始是因为，第 $k+1$ 层的净输入 $net_0^{k+1}$ 实际上为第 $k$ 层节点$0$的输出 $o_{0}^{k}$ 乘以权值 $ w_{0j}^{k+1} $ 是固定的，它不取决于第 $k$ 层的输出。</p><p>我们知道，上式的第一个偏微分已经在第 $k+1$ 层计算误差得到，</p><script type="math/tex; mode=display">\frac{\partial E}{\partial net_l^{k+1}}  =  \delta_l^{k+1}</script><p>而对于第二个偏微分，我们将 $net_l^{k+1}$ 展开，</p><script type="math/tex; mode=display">net_l^{k+1} = \sum_{j=0}^{r_{k}} w_{jl}^{k+1}o_j^{k} = \sum_{j=0}^{r_{k}} w_{jl}^{k+1}f(net_j^k)</script><p>这里的 $f(x)$ 是隐藏层的激活函数，所以可以得到第二个偏微分的公式，</p><script type="math/tex; mode=display">\frac{\partial net_l^{k+1}}{\partial net_j^k} = \frac{\partial}{\partial net_j^k}\sum_{j=0}^{r_{k}} w_{jl}^{k+1}f(net_j^k) = w_{jl}^{k+1}f^\prime(net_j^k)</script><p>故我们得到了反向传播公式，</p><script type="math/tex; mode=display">\delta_j^k = \frac{\partial E}{\partial net_j^k} = \sum_{l=1}^{r_{k+1}}  \frac{\partial E}{\partial net_l^{k+1}}  \frac{\partial net_l^{k+1}}{\partial net_j^k} = f^\prime(net_j^k)\sum_{l=1}^{r_{k+1}} \delta_l^{k+1} w_{jl}^{k+1}</script><p>因此，可以从 $\delta_l^{k+1}$ 迭代计算出 $\delta_j^k$ ，换句话说，第 $k$ 层的误差 $\delta_j^k$ 依赖于第 $k+1$ 层的误差 $\delta_l^{k+1}$计算而来。这就是反向传播名称的来源，误差沿着神经网络反向流动，从最后一层流向第一层。一旦计算出了输出层的误差，我们就可以沿着神经网络迭代算出隐藏层的误差，通过乘上一个系数 $f^\prime(net_j^k)$。</p><p>计算出误差之后，我们就可以得到梯度的公式，</p><script type="math/tex; mode=display">\frac{\partial E}{\partial w_{ij}^k} = \delta_j^L o_i^{k-1} = g^\prime(net_j^k)o_i^{k-1}\sum_{l=1}^{r_{k+1}} \delta_l^{k+1} w_{jl}^{k+1}</script><p>注意到在这个公式中，我们需要知道 $net_j^k$ 和 $o_i^{k-1}$，这些需要在前向传播的时候计算并保存。也就是说，每一次迭代中，</p><ul><li>首先进行前向传播的计算，根据设定的模型参数，从输入层到输出层，同时保存每一层的 $net_j^k$ 和 $o_j^k$</li><li>然后进行反向传播的计算，从输出层开始，以输出层的误差作为输入，计算每一层每个节点中误差的梯度</li><li>最后我们通过算出的梯度更新参数的值，然后进入下一次迭代</li></ul><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>最后在这里梳理下上面推导的公式和整个算法的流程。</p><p>对于梯度计算：</p><script type="math/tex; mode=display">\begin{equation}\frac{\partial E}{\partial w_{ij}^k} = \delta_j^k o_i^{k-1}\end{equation}</script><p>对于输出层的误差计算：</p><script type="math/tex; mode=display">\begin{equation}\delta_j^L = f_o^\prime(net_j) (\hat{y} - y)\end{equation}</script><p>对于隐藏层的误差计算：</p><script type="math/tex; mode=display">\begin{equation}\delta_j^k = g_0^\prime(net_{j}^k) \sum_{l=1}^{r^{k+1}} w_{jl}^{k+1} \delta_l^{k+1}\end{equation}</script><p>将所有的误差结合起来：</p><script type="math/tex; mode=display">\begin{equation}\frac{\partial E(\theta)}{\partial w_{ij}^k} = \frac{1}{N} \sum_{d=1}^N \frac{\partial}{\partial w_{ij}^k} ( \frac{1}{2} (\hat{y_d} - y_d)^2 ) = \frac{1}{N} \sum_{d=1}^N \frac{\partial E_d}{\partial w_{ij}^k}\end{equation}</script><p>更新参数：</p><script type="math/tex; mode=display">\begin{equation}\Delta w_{ij}^k = -\alpha \frac{\partial E(\theta)}{\partial w_{ij}^k}\end{equation}</script><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2021-01-02_back-propagation.svg"></p><p>整体流程如下：</p><ol><li><p>随机初始化权值参数 $w_{ij}^k$</p></li><li><p><strong>前向传播计算</strong>，从输入层到输出层，对于第 $k$ 层的第 $j$ 个节点，基于 $w_{ij}^k$ 计算出 $net_j^k$，$o_j^k$ 和 $\hat{y_d}$</p></li><li><p><strong>反向传播计算</strong>，从输出层到输入层，对于第 $k$ 层的第 $j$ 个节点，通过公式2和公式3计算出 $\delta_j^k$，</p><p>然后通过公式1计算出梯度 $\frac{\partial E}{\partial w_{ij}^k}$</p></li><li><p><strong>将所有节点的误差结合起来</strong>，通过公式4将所有的输出节点的误差结合起来</p></li><li><p><strong>更新权值参数</strong>，根据公式5更新权值参数，然后进入第2步进行下一轮迭代计算，直到误差收敛</p></li></ol><h2 id="调参与正则化优化"><a href="#调参与正则化优化" class="headerlink" title="调参与正则化优化"></a>调参与正则化优化</h2><p>上一小节中，我们介绍了神经网络的训练与预测：在明确了神经网络的模型后，我们就可以通过定义合理的损失函数，模型根据反向传播算法和随机梯度下降算法，自动地修正网络模型的参数（$W$ 和 $b$ ），并对训练数据的特征进行学习。</p><p>但是，这里似乎还有很多问题没有解决：</p><ul><li>神经网络应该有多少层</li><li>每一层应该有多少隐藏单元</li><li>学习速率应该是多少</li><li>各层应该采用哪些激活函数</li><li>应该选用哪种损失函数</li><li>梯度下降算法的参数应该如何选择</li><li>……</li></ul><p>所有的这些超参数不可能在一开始就预测出来，实际上通常的情况是，首先有个初步想法，比如构建一个含有特定层数、隐藏单元等等的神经网络，然后在运行和测试中得到该神经网络的运行结果，并不断迭代更新自己的方案。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h4><p>线性函数是最基本的激活函数，其因变量与自变量有直接的比例关系，因此线性变换类似于线性回归。</p><script type="math/tex; mode=display">f(x) = ax + b</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span><span class="params">(x, a, b)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> a * x + b</span><br></pre></td></tr></table></figure><h4 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h4><p><code>Sigmoid</code> 函数是一种在不删除数据的情况下，减少数据的极值或异常值的函数。</p><script type="math/tex; mode=display">s(x) = \frac{1}{1 + e^{-ax}}</script><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_sigmoid.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x, w = <span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.sum(np.exp(-wx))</span><br></pre></td></tr></table></figure><h4 id="双曲正切函数"><a href="#双曲正切函数" class="headerlink" title="双曲正切函数"></a>双曲正切函数</h4><p>双曲正切函数 <code>tanh</code> 与 <code>sigmoid</code> 函数蕾丝，不同的是，<code>tanh</code> 的归一范围是 -1 到 1，而不是 0 到 1，因此 tanh 的优点是可以更容易地处理附属。</p><script type="math/tex; mode=display">tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}</script><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_tanh.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> np.tanh(h)</span><br></pre></td></tr></table></figure><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h4><p><code>ReLU</code> 函数满足仿生学中的稀疏性，只有当输入值高于一定数目时才激活该神经元节点。当输入值低于0时进行限制，当输入值上升到某一阈值以上时，函数中的自变量与因变量成线性关系。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_relu.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> x <span class="keyword">if</span> x &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h4><p><code>Softmax</code> 函数的本质是将一个 K 维的任意实数向量，映射成另一个 K 维的实数向量，其中向量中的每一个元素取值都介于（0，1）范围内。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-21_softmax.jpg"></p><script type="math/tex; mode=display">softmax(x_j) = \frac{e^{x_j}}{\sum_{k=1}^K e^{x_k}} j \in [1, K]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> np.exp(x) / np.sum(np.exp(x))</span><br></pre></td></tr></table></figure><h4 id="激活函数的选择"><a href="#激活函数的选择" class="headerlink" title="激活函数的选择"></a>激活函数的选择</h4><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>在神经网络中，损失函数用来评价网络模型输出的预测值 $\hat{\vec{Y}} = f(\vec{X})$ 与真实值 $\vec{Y}$ 之间的差异。这里使用 $L(\vec{Y}, \hat{\vec{Y}})$ 来表示损失函数，它是一个非负值函数。损失值越小，网络模型的性能就越好，所以优化算法目的就是让损失函数尽可能的小。</p><h4 id="损失函数的定义"><a href="#损失函数的定义" class="headerlink" title="损失函数的定义"></a>损失函数的定义</h4><p>假设网络模型中有 N 个样本，样本的输入和输出向量为 $(\vec{X}, \vec{Y}) = (x_i, y_i), i \in [1, N]$ ，那么总损失函数  $L(\vec{Y}, \hat{\vec{Y}})$ 为每一个输出预测值与真实值的误差之和。</p><script type="math/tex; mode=display">L(\vec{Y}, \hat{\vec{Y}}) = \sum_{i=0}^N l(y, \hat{y_i})</script><p>值得注意的是，机器学习问题主要分为回归和分类问题，对分类模型和回归模型进行评估时会使用不同的损失函数，下面将分别对回归模型和分类模型的损失函数进行介绍。</p><h4 id="回归损失函数"><a href="#回归损失函数" class="headerlink" title="回归损失函数"></a>回归损失函数</h4><h5 id="均方误差损失函数，MSE"><a href="#均方误差损失函数，MSE" class="headerlink" title="均方误差损失函数，MSE"></a>均方误差损失函数，MSE</h5><h5 id="平均绝对误差损失函数，MAE"><a href="#平均绝对误差损失函数，MAE" class="headerlink" title="平均绝对误差损失函数，MAE"></a>平均绝对误差损失函数，MAE</h5><h5 id="均方误差对数损失函数，MSLE"><a href="#均方误差对数损失函数，MSLE" class="headerlink" title="均方误差对数损失函数，MSLE"></a>均方误差对数损失函数，MSLE</h5><h4 id="分类损失函数"><a href="#分类损失函数" class="headerlink" title="分类损失函数"></a>分类损失函数</h4><h5 id="Logistic-损失函数"><a href="#Logistic-损失函数" class="headerlink" title="Logistic 损失函数"></a>Logistic 损失函数</h5><h5 id="负对数似然损失函数"><a href="#负对数似然损失函数" class="headerlink" title="负对数似然损失函数"></a>负对数似然损失函数</h5><h5 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h5><h5 id="Hinge损失函数"><a href="#Hinge损失函数" class="headerlink" title="Hinge损失函数"></a>Hinge损失函数</h5><h5 id="指数损失函数"><a href="#指数损失函数" class="headerlink" title="指数损失函数"></a>指数损失函数</h5><h4 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h4><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><h4 id="动量"><a href="#动量" class="headerlink" title="动量"></a>动量</h4><h3 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h3><h3 id="数据集扩展"><a href="#数据集扩展" class="headerlink" title="数据集扩展"></a>数据集扩展</h3><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="Zero-Centralization"><a href="#Zero-Centralization" class="headerlink" title="Zero Centralization"></a>Zero Centralization</h4><h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><h4 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis, PCA"></a>Principal Component Analysis, PCA</h4><h4 id="Whitening"><a href="#Whitening" class="headerlink" title="Whitening"></a>Whitening</h4><h4 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h4><h3 id="网络过度拟合"><a href="#网络过度拟合" class="headerlink" title="网络过度拟合"></a>网络过度拟合</h3><h3 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h3><p>正则化的最大作用是防止过度拟合，提高网络模型的泛化能力，具体实现方法是在损失函数中增加惩罚因子。</p><h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h4><h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h4><h4 id="最大约束范式"><a href="#最大约束范式" class="headerlink" title="最大约束范式"></a>最大约束范式</h4><h4 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a>Dropout 层</h4><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://brilliant.org/wiki/backpropagation" target="_blank" rel="external nofollow noopener noreferrer">Back Propagation Explained</a></li><li><a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="external nofollow noopener noreferrer">WikiPedia: Back Propagation</a></li><li><a href="http://www.ai-start.com/dl2017/" target="_blank" rel="external nofollow noopener noreferrer">深度学习笔记</a></li><li><a href="https://nndl.github.io/" target="_blank" rel="external nofollow noopener noreferrer">https://nndl.github.io/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;除了SVM、决策树等算法，人工神经网络是机器学习的另一个重要分支，它是深度学习的基础。人工神经网络是通过模仿生物神经网络系统结构和功能，提出了一种 &lt;strong&gt;非线性统计性模型&lt;/strong&gt; ，用于对函数的近似和估计。人工神经网络以其独特的网络结构和处理信息的方法，在自动控制领域、组合优化问题、模式识别、图形处理、自然语言处理等诸多领域，已经取得了辉煌的成绩，本文将介绍其基本模型和核心算法实现。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-31_dp-forward-propagation.svg" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="深度学习" scheme="http://houmin.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://houmin.cc/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="反向传播" scheme="http://houmin.cc/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
      <category term="梯度下降" scheme="http://houmin.cc/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="前向传播" scheme="http://houmin.cc/tags/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>十字路口</title>
    <link href="http://houmin.cc/posts/64c2f65e/"/>
    <id>http://houmin.cc/posts/64c2f65e/</id>
    <published>2020-12-12T11:23:16.000Z</published>
    <updated>2020-12-13T11:31:32.452Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>改变的路总是很难走，尽管有挫折，但是仍要努力向前。这里是2020年「朝花夕拾」第二十八期 <code>十字路口</code>，这应该是 2020 年 「朝花夕拾」倒数第二期了。就在今天，北京又迎来了小雪，虽然只有不到两个小时，仍然给人带来些许欣喜。十字路口，尽管前面存在着各种不确定，唯一确定的是你希望找到一个更加自洽的自我。</p>    <div id="aplayer-RYoKNEWx" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="346836" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>很久没有出门啦，说好的二十四节气好久没有进展，北京大冬天的真的就想一个人呆在家里。好啦，继续看数据吧：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-12_rescue-time.png"></p><p>这周感觉工作效率一般般，因为没有制定一周规划，整个星期都显得目标不明确。现在越工作越感觉自己知道的东西太少了，在专业上还有很多很多需要去学习和努力的地方。</p><p>接下来是 <code>Forest</code> 和 <code>Running</code>，这周跑步很不在状态，虽然也跑了三次，但是还是没有找到跑步的状态：</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt="Forest - Nov 29 ~ Dec 05, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-12_forest.jpg"></div><div class="group-picture-column" style="width: 50%;"><img alt="Running - Nov 29 ~ Dec 05, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-12_running.jpg"></div></div></div></div><p>下面是睡眠数据总结，可以看到上周的周三晚上过了一点半才睡觉，第二天整天都昏昏沉沉的，导致当天都没有跑步。回忆了一下，那天是看《褚时健传》看的兴起，一下子就看到了很晚，这个太要不得了，下周要避免。另外一个就是，周五晚上普遍睡的比较晚，周六早上也起的很晚，比如这周到11点才从床上爬起来，这个太难受了。</p><div id="echarts1730" style="width: 100%;height: 600px;margin: 0 auto"></div><script type="text/javascript" src="https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js"></script><script type="text/javascript" src="http://gallery.echartsjs.com/dep/echarts/map/js/china.js"></script><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('echarts1730'));  // 指定图表的配置项和数据  var dateTime = ['2020-11-29', '2020-11-30', '2020-12-01', '2020-12-02', '2020-12-03', '2020-12-04', '2020-12-05', '2020-12-06', '2020-12-07', '2020-12-08', '2020-12-09', '2020-12-10', '2020-12-11', '2020-12-12', '2020-12-13'];var sleepTime = [-0.95, -0.18, 4.03, -0.55, 0.58, 0.38, 1.13, -0.08, 1.37, -0.08, 1.7, -0.38, 0.57, 1.05, -0.93];var awakeTime = [7.8, 8.67, 7.72, 7.83, 6.82, 4.85, 9.28, 7.38, 7.23, 4.7, 8.2, 4.95, 8.17, 11.45, 6.7];var awakeTimeBar = [];for (let i = 0; i < sleepTime.length; ++i) {    var awakeTimeValue = awakeTime[i];    if (sleepTime[i] > 0) {        awakeTimeValue = awakeTime[i] - sleepTime[i];    }    awakeTimeBar.push(awakeTimeValue);}option = {    title: {        text: '睡眠监控'     },    tooltip: {        trigger: 'axis',        formatter: function(params) {            function getHourMinute(timeValue) {                if (timeValue < 0) timeValue = 24 + timeValue;                var m = Math.floor((timeValue % 1) * 60);                m = m.toString().padStart(2, '0');                var h = Math.floor(timeValue);                return {                    hour: h,                    minute: m                }            }            var sleepValue = params[0].data;            var awakeValue = params[1].data;            var sleepTime = getHourMinute(sleepValue);            var awakeTime = getHourMinute(awakeValue);            var totalTime = getHourMinute(awakeValue - sleepValue);            return params[0].name + '<br />'                + params[0].seriesName + ": " + sleepTime.hour + ":" + sleepTime.minute + '<br />'                + params[1].seriesName + ": " + awakeTime.hour + ":" + awakeTime.minute + '<br />'                +  "睡眠时长: " + totalTime.hour + ":" + totalTime.minute;        }    },    toolbox: {        feature: {            dataView: {show: true, readOnly: false},            restore: {show: true},            saveAsImage: {show: true}        }    },    legend: {        data: ['入睡时间', '起床时间', '睡眠时间']    },    xAxis: [        {            type: 'category',            data: dateTime,            axisPointer: {                type: 'shadow'            }        }    ],    yAxis: [        {            type: 'value',            axisLine: {                show: false            },            name: '时间',            axisLabel: {                formatter: function (h) {                    h = Math.floor(h);                    if (h < 0) {                        return h + 24 + ':00';                    } else {                        return h + ':00';                    }                 },                 margin: 20            }        }    ],    series: [        {            name: '入睡时间',            type: 'line',            data: sleepTime        },        {            name: '起床时间',            type: 'line',            data: awakeTime        },        {            type: 'bar',            stack: '总量',            data: sleepTime,            itemStyle: {                normal: {                    color: function(params) {                        if (params.data > 0) return 'rgba(0,0,0,0)';                        else return '#2F4554'                    }                }            }        },        {            type: 'bar',            stack: '总量',            data: awakeTimeBar,            itemStyle: {                normal: {                    color: function(params) {                       return '#2F4554'                    }                }            }        }    ]};  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);</script><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;改变的路总是很难走，尽管有挫折，但是仍要努力向前。这里是2020年「朝花夕拾」第二十八期 &lt;code&gt;十字路口&lt;/code&gt;，这应该是 2020 年 「朝花夕拾」倒数第二期了。就在今天，北京又迎来了小雪，虽然只有不到两个小时，仍然给人带来些许欣喜。十字路口，尽管前面存在着各种不确定，唯一确定的是你希望找到一个更加自洽的自我。&lt;/p&gt;

    &lt;div id=&quot;aplayer-RYoKNEWx&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;346836&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-13_cross.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="十字路口" scheme="http://houmin.cc/tags/%E5%8D%81%E5%AD%97%E8%B7%AF%E5%8F%A3/"/>
    
  </entry>
  
  <entry>
    <title>远离他们</title>
    <link href="http://houmin.cc/posts/36fc760d/"/>
    <id>http://houmin.cc/posts/36fc760d/</id>
    <published>2020-12-05T11:23:16.000Z</published>
    <updated>2020-12-13T10:38:03.322Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>像燃烧的火球在天边下坠，这是今天旁晚的晚霞，从没拍到过这样的烟云。这里是「朝花夕拾」第二十七期 <code>远离他们</code>，原谅我的起名无能，我只是觉得这首歌很好听。最近开始捡起了阅读，不再是刷着手机睡觉，而是看着Kindle入眠。相比之前，感觉自己的状态好了很多，也越来越觉得以前从信息流中漫无目的滑过的浅薄。远离他们，远离浮躁、远离喧嚣、远离无用的信息流，沉淀下来，做些有意义的事情（强行点题，手工狗头） </p>    <div id="aplayer-hVBwvcLq" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1447573210" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p><img alt="坠落烟云" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-05_drop.png"></p><p>例行数据回顾，首先是 <code>RescueTime</code>：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-05_rescue-time.png"></p><p>这周记录的时间相对上周要少，因为周日出去浪了，所以基本没有碰电脑。总体来说，这周的企业微信用的比上周要多，因为多了一些沟通的事情，这周主要的事情也就是在自己学习 <code>k8s</code>，总结自己的博客中。还是继续坚持之前的做法，用固定的时间来去解决沟通的问题。</p><p>接下来是 <code>Forest</code> 和 <code>Running</code>，现在工作的时候很少刷手机了，只有在吃饭的时候刷刷手机，但是就在那一个小时也容易一直拖拉，需要进一步改变。跑步方面，这周坚持了四次，比上周数据有所增加，但是说实话一直还没有找到跑步的感觉，继续加油叭。之前大四跑步疯狂的时候，说自己要跑马拉松，但是最后鸽了，这次看能不能跑上明年的北马呢？</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt="Forest - Nov 29 ~ Dec 05, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-05_forest.jpg"></div><div class="group-picture-column" style="width: 50%;"><img alt="Running - Nov 29 ~ Dec 05, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-05_running.jpg"></div></div></div></div><p>接下来是睡眠数据总结，这里我更新了处理数据的 <code>Python</code> 脚本，主要的展示逻辑放在了 <code>ECharts</code> 里面：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/env python3</span><br><span class="line"># -*- coding: UTF<span class="number">-8</span> -*-</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">datafile = <span class="string">"sleep.csv"</span></span><br><span class="line"></span><br><span class="line">def get_time_value(timeArray):</span><br><span class="line">    <span class="keyword">return</span> timeArray.tm_hour + round(float(timeArray.tm_min) / float(<span class="number">60</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">def get_time(timestamp):</span><br><span class="line">    timeStamp = float(timestamp)</span><br><span class="line">    timeArray = time.localtime(timeStamp)</span><br><span class="line">    <span class="keyword">return</span> timeArray</span><br><span class="line"></span><br><span class="line">def parse_csv(datafile):</span><br><span class="line">    dateTime = []</span><br><span class="line">    sleepTime = []</span><br><span class="line">    awakeTime = []</span><br><span class="line">    with open(datafile, <span class="string">"r"</span>) as f:</span><br><span class="line">        r = csv.DictReader(f)</span><br><span class="line">        <span class="keyword">for</span> line in r:</span><br><span class="line">            start, stop = line[<span class="string">"start"</span>], line[<span class="string">"stop"</span>]</span><br><span class="line">            pStart = get_time(start)</span><br><span class="line">            pStop = get_time(stop)</span><br><span class="line">            pDate = time.strftime(<span class="string">"%Y-%m-%d"</span>, pStop)</span><br><span class="line">            dateTime.<span class="built_in">append</span>(pDate)</span><br><span class="line"></span><br><span class="line">            sleepTimeValue = get_time_value(pStart)</span><br><span class="line">            awakeTimeValue = get_time_value(pStop)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> sleepTimeValue &gt; <span class="number">20</span>:</span><br><span class="line">                sleepTimeValue = round(sleepTimeValue - <span class="number">24.0</span>, <span class="number">2</span>)</span><br><span class="line">            sleepTime.<span class="built_in">append</span>(sleepTimeValue)</span><br><span class="line">            awakeTime.<span class="built_in">append</span>(awakeTimeValue)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dateTime, sleepTime, awakeTime</span><br></pre></td></tr></table></figure><p>数据涵盖了 <code>2020-10-19</code> 到 <code>2020-12-06</code> 的数据，可以看到我的睡眠分布非常不均衡，不仅仅是入睡时间的不均衡，起床时间的不均衡，还包括睡眠时长的不均衡。另外，大多数的入睡时间都在12点以后，很少在12点前就睡了的，这个一方面是之前作息极度不规律，另一方面现在有时候看书容易看晚了。</p><p>所以12月的一个任务是，将整个图形向下拉一个小时，以11点为起点，7点为终点的睡眠时间，而且两条曲线应该日渐平缓，方差不要过大，让自己的睡眠真正的规律起来。</p><div id="echarts3405" style="width: 100%;height: 600px;margin: 0 auto"></div><script type="text/javascript" src="https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js"></script><script type="text/javascript" src="http://gallery.echartsjs.com/dep/echarts/map/js/china.js"></script><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('echarts3405'));  // 指定图表的配置项和数据  var dateTime = ['2020-10-19', '2020-10-20', '2020-10-21', '2020-10-22', '2020-10-23', '2020-10-24', '2020-10-25', '2020-10-26', '2020-10-27', '2020-10-28', '2020-10-29', '2020-10-30', '2020-10-31', '2020-11-01', '2020-11-01', '2020-11-03', '2020-11-17', '2020-11-19', '2020-11-20', '2020-11-21', '2020-11-22', '2020-11-23', '2020-11-24', '2020-11-25', '2020-11-26', '2020-11-27', '2020-11-28', '2020-11-29', '2020-11-30', '2020-12-01', '2020-12-02', '2020-12-03', '2020-12-04', '2020-12-05', '2020-12-06'];var sleepTime = [-0.17, 0.57, 1.13, -0.88, 0.65, 0.77, 1.62, 0.1, 0.8, 0.95, 1.12, 0.27, 0.62, -0.05, 0.0, -1.63, 0.0, -0.55, 3.12, 0.33, 1.77, -0.7, 0.28, -0.2, 1.42, 0.38, -1.0, -0.95, -0.18, 4.03, -0.55, 0.58, 0.38, 1.13, -0.08];var awakeTime = [8.78, 5.13, 8.9, 3.32, 8.38, 10.77, 9.32, 6.15, 6.92, 8.32, 9.27, 6.62, 4.18, 3.87, 0.0, 3.5300000000000002, 0.0, 7.57, 7.95, 7.08, 9.85, 7.13, 5.42, 3.52, 8.3, 8.52, 8.03, 7.8, 8.67, 7.72, 7.83, 6.82, 4.85, 9.28, 7.38];var awakeTimeBar = [];for (let i = 0; i < sleepTime.length; ++i) {    var awakeTimeValue = awakeTime[i];    if (sleepTime[i] > 0) {        awakeTimeValue = awakeTime[i] - sleepTime[i];     }    awakeTimeBar.push(awakeTimeValue);}option = {    title: {        text: '睡眠监控'     },    tooltip: {        trigger: 'axis',        formatter: function(params) {            function getHourMinute(timeValue) {                if (timeValue < 0) timeValue = 24 + timeValue;                var m = Math.floor((timeValue % 1) * 60);                m = m.toString().padStart(2, '0');                var h = Math.floor(timeValue);                return {                    hour: h,                     minute: m                }            }                        var sleepValue = params[0].data;            var awakeValue = params[1].data;            if (sleepValue > 0) {                awakeValue = sleepValue + awakeValue;            }            var sleepTime = getHourMinute(sleepValue);            var awakeTime = getHourMinute(awakeValue);            var totalTime = getHourMinute(awakeValue - sleepValue);            return params[0].name + '<br />'                + params[0].seriesName + ": " + sleepTime.hour + ":" + sleepTime.minute + '<br />'                + params[1].seriesName + ": " + awakeTime.hour + ":" + awakeTime.minute + '<br />'                +  "睡眠时长: " + totalTime.hour + ":" + totalTime.minute;        }    },    toolbox: {        feature: {            dataView: {show: true, readOnly: false},            restore: {show: true},            saveAsImage: {show: true}        }    },    legend: {        data: ['入睡时间', '起床时间', '睡眠时间']    },    xAxis: [        {            type: 'category',            data: dateTime,            axisPointer: {                type: 'shadow'            }        }    ],    yAxis: [        {            type: 'value',            axisLine: {                show: false            },            name: '时间',            axisLabel: {                formatter: function (h) {                    h = Math.floor(h);                    if (h < 0) {                        return h + 24 + ':00';                    } else {                         return h + ':00';                    }                 },                 margin: 20            }        },        {            type: 'value',            axisLine: {            show: false            },            min: 0,            max: 10,            name: '时长/小时'        }    ],    series: [        {            name: '入睡时间',            type: 'line',            data: sleepTime        },        {            name: '起床时间',            type: 'line',            data: awakeTime        },        {            type: 'bar',            stack: '总量',            data: sleepTime,            itemStyle: {                normal: {                    color: function(params) {                        if (params.data > 0) return 'rgba(0,0,0,0)';                        else return '#2F4554'                    }                }            }        },        {            type: 'bar',            stack: '总量',            data: awakeTimeBar,            itemStyle: {                normal: {                    color: function(params) {                       return '#2F4554'                    }                }            }        }    ]};  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);</script><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><h3 id="Salesforce-收购-Slack"><a href="#Salesforce-收购-Slack" class="headerlink" title="Salesforce 收购 Slack"></a>Salesforce 收购 Slack</h3><h3 id="深圳新房代持"><a href="#深圳新房代持" class="headerlink" title="深圳新房代持"></a>深圳新房代持</h3><h3 id="孟晚舟被捕两周年"><a href="#孟晚舟被捕两周年" class="headerlink" title="孟晚舟被捕两周年"></a>孟晚舟被捕两周年</h3><h3 id="Gartner-发布报告"><a href="#Gartner-发布报告" class="headerlink" title="Gartner 发布报告"></a>Gartner 发布报告</h3><h3 id="社区买菜热战正酣"><a href="#社区买菜热战正酣" class="headerlink" title="社区买菜热战正酣"></a>社区买菜热战正酣</h3><p>阿里充值十绘团</p><p>美团财报出炉</p><h3 id="虾米音乐将关闭"><a href="#虾米音乐将关闭" class="headerlink" title="虾米音乐将关闭"></a>虾米音乐将关闭</h3><h3 id="11月PMI发布"><a href="#11月PMI发布" class="headerlink" title="11月PMI发布"></a>11月PMI发布</h3><h3 id="华晨债务风波"><a href="#华晨债务风波" class="headerlink" title="华晨债务风波"></a>华晨债务风波</h3><h3 id="量子计算九章发布"><a href="#量子计算九章发布" class="headerlink" title="量子计算九章发布"></a>量子计算九章发布</h3><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;像燃烧的火球在天边下坠，这是今天旁晚的晚霞，从没拍到过这样的烟云。这里是「朝花夕拾」第二十七期 &lt;code&gt;远离他们&lt;/code&gt;，原谅我的起名无能，我只是觉得这首歌很好听。最近开始捡起了阅读，不再是刷着手机睡觉，而是看着Kindle入眠。相比之前，感觉自己的状态好了很多，也越来越觉得以前从信息流中漫无目的滑过的浅薄。远离他们，远离浮躁、远离喧嚣、远离无用的信息流，沉淀下来，做些有意义的事情（强行点题，手工狗头） &lt;/p&gt;

    &lt;div id=&quot;aplayer-hVBwvcLq&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;1447573210&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-12-05_drop.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="阅读" scheme="http://houmin.cc/tags/%E9%98%85%E8%AF%BB/"/>
    
      <category term="量子计算" scheme="http://houmin.cc/tags/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>晚安</title>
    <link href="http://houmin.cc/posts/74fda535/"/>
    <id>http://houmin.cc/posts/74fda535/</id>
    <published>2020-11-28T11:23:16.000Z</published>
    <updated>2020-12-06T16:11:55.246Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>遵守上周的约定，这周总算是定期发布了2020年「朝花夕拾」的第二十六期 <code>晚安</code>，在上期的记录中，我告诉自己要搭建起自己的睡眠数据监控系统，这周我来介绍下我是如何折腾的。封面来自今天在家拍摄的晚霞，晚安，希望大家都能够睡个好觉。</p>    <div id="aplayer-NPEtEfqx" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="439122551" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>在例行每周数据回顾之前，先来看看这周拍摄的光影，仍旧没有出门，但是天朗气清留下了美丽的晚霞，并且抓到了太阳落山的全程。</p><p><img alt="白昼越来越短，黑夜越来越长，下午四点半在家里拍摄，不一会儿太阳就开始触碰山尖" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_sunset-3.png"></p><p><img alt="知春西里一号楼十六层，这个机位也还不错，但是如果能够上楼顶就更好了，下次问问" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_sunset-2.png"></p><p><img alt="将近五点，太阳已经全部落山，整个过程就两三分钟" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_sunset-1.png"></p><p><img alt="落日的同时，人间烟火，我就在这样的小格子里面的一间" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_smoke.png"></p><p>接下来，继续每周的数据回顾，首先是 <code>RescueTime</code>：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_rescue-time.png"></p><p>可以看到，相比于上周，这周总的生产力是要提升的。这周基本实践了上周的策略，工作时间专门找独立时间来处理企业微信上面的消息，而不是一直在线。因为如果真的有什么很要紧的工作的话，打我电话啊，反正我也在企业微信上面置顶了信息。真的，要求时刻在线，时刻能够回复消息对于精力的耗费太大了，这个方法继续坚持。</p><p>因为临近月末，对于每个数据监控，现在也增加月末的数据总结，我们看看 <code>RescueTime</code> 的 11月数据：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_rescue-time-nov.png"></p><p>可以看到，<code>Distracting Time</code> 从月初相对较大的比例，到月末有了一个明显的减少。这是为什么呢？因为我的 <code>Kindle</code> 到货了 ：）开个玩笑，其实从一个多月以前开始，就一直想着要减少自己无意义刷手机的行为。我对自己的这个行为深恶痛绝，但是一直没有根治过，也许是为了放松，但是经常性的在几个App之间切换而无所得，反而有的是巨大的空虚感。其中尤其以微博、微信朋友圈等App为甚。其实也是能够从这些渠道获取一些有效信息的，但是在我看来成本太大，所以我关闭了微信朋友圈、卸载了微博，并不是永远不再使用，至少这个冬天，我想要让自己沉下心来，扎扎实实做一些记录，做一些积淀。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_rescue-time-sep.png"></p><p>如果说11月份的数据还不够明显的话，可以看看9月份的数据，在工作之余，微博、微信、B站这几个是耗时最多的应用（10月因为有段时间去旅行了，所以数据并没有那么明显）。虽然卸载了微博、关闭了朋友圈，但是像 <code>The Social Dilemma</code> 中描述的很像，这个过程有一个戒断反应，有时候你会控制不住自己去把它们找回来。每次这个时候你要想一想，那里到底有什么那么重要的东西，让你不得不去开启它们呢？现在的我仍然处于戒断反应之中，过去的一个星期重新下载了微博一次、重新开启了朋友圈5次，虽然之后都卸载关闭了，但是仍然在和自己较量中。加油吧，希望下周总结的时候这个数据能够有改进。</p><p>不刷微博微信后，你的信息源何来呢？当我问出这个问题的时候，足以反映出一个问题：你是多么的害怕和这个世界隔离。但是真的会隔离吗，我依旧会在休息空隙看微信订阅号。但是下班回家后我不会再看手机消息了，因为平时下班比较晚，到家基本上就10点了，如果像以前那样每次都刷刷手机，基本上一个小时就过去了。这看起来是一个还可以接受的时间，实际上随着自己每天心情的状态改变，加上各种杂七杂八的事情，经常会拖到凌晨十二点半，甚至有时候到一两点。现在的做法很简单，到家后直接手机关机，直接拿起 Kindle 看书，看累了就睡觉。</p><p>关于 Kindle 看书，实际上还有很多问题没有解决。因为很久没有专门的时间看书了（读研后？），现在看书也没有一个明确的门类清单，有时候也没能够很专注其中。关于这个问题，希望下周能够有一个决断。</p><p>话题扯远了，我们继续回顾数据，<code>Google Calendar</code> 的每日总结现在仍然搁置，下周得有个交代了，你拖更太久了 ：）接下来看 <code>Forest</code>，</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt="Forest - Nov 22 ~ Nov 28, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_forest.jpg"></div><div class="group-picture-column" style="width: 50%;"><img alt="Forest November" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_forest-nov.jpg"></div></div></div></div><p>健身是另一大事宜，这周跑步情况与上周相比基本持平，还在找回节奏的过程中，下周希望可以破四。回顾十一月的跑步情况，在数据监控回归之后总算维持了稳定。</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt="Running - Nov 22 ~ Nov 28, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_running.jpg"></div><div class="group-picture-column" style="width: 50%;"><img alt="Running November" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_running-nov.jpg"></div></div></div></div><hr><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><h3 id="嫦娥五号发射成功"><a href="#嫦娥五号发射成功" class="headerlink" title="嫦娥五号发射成功"></a>嫦娥五号发射成功</h3><ul><li>背景：2020年11月24日，中国的首个月球采样返回任务「嫦娥五号」发射成功</li><li>分析：这是中国的第六次探月人物，也是人类时隔44年将再次从月球带回岩石和土壤样品，上一次月球采样返回任务，还是1976年苏联的月球24号。</li></ul><h3 id="伊朗核科学家被暗杀"><a href="#伊朗核科学家被暗杀" class="headerlink" title="伊朗核科学家被暗杀"></a>伊朗核科学家被暗杀</h3><ul><li>背景：2020年11月27日，伊朗核计划负责人<strong>法赫里扎德</strong>近被暗杀身亡。据路透社报道，伊朗当天在写给联合国秘书长古特雷斯和联合国安理会的信中称，有“强烈迹象表明以色列对暗杀一名伊朗科学家负有责任”，伊朗保留自卫的权利。</li><li>跟进：西方普遍认为，这是美国和以色列主使，具体执行的是以色列情报机构<strong>摩萨德</strong></li><li>分析：这是继年初<strong>苏莱曼尼</strong>被美军无人机导弹猎杀后，伊朗核心人物遭到暗杀事件。拜登政府已经表态重返伊朗核协议，也就是说相对于特朗普政府美国的中东政策将会大幅转变。这个关头以色列暗杀<strong>法赫里扎德</strong>，再加上几天前 <strong>内塔尼亚胡</strong> <a href="https://link.zhihu.com/?target=https%3A//www.guancha.cn/internation/2020_11_28_572864.shtml" rel="external nofollow noopener noreferrer" target="_blank">访问了沙特阿拉伯</a>，舆论分析是 <strong>内塔尼亚胡</strong> 在给美国新一任政府传递信息，希望美国能够继续遏制伊朗。</li></ul><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>总算聊到了今天的主题「睡眠」，其实刚才也在社交网络戒断的时候也提到了无意义刷手机对于睡眠的影响。为了有一个更加健康的睡眠，为了自己的生活更有节奏，还是希望自己能够记录自己的睡眠数据，从而能够更好的指导自己的生活。</p><p>睡眠数据中，最最基本的维度就是每天何时入睡、何时苏醒。当然你可以自己手动记录在相应的手机App上，但是这种方法太考验人的毅力了，我们需要一种无侵入式的记录方式。那么这种情况下的解决思路就是智能穿戴设备，比如智能手环、智能手表等，通过再添加一些传感器让硬件来解放人。</p><p>令人开心的是，事实上我已经佩戴 <code>小米手环3</code> 将近两年了，而小米也有 <code>小米运动</code> App 来对数据做专门的统计。然而令人失望的是，小米运动的睡眠数据统计做的极烂，我想要看到一周的每天何时起床何时苏醒都看不到，只能够看到一些可能都不是很准的深睡时间、浅睡时间。也许这些数据以后会很有用，但是我现阶段只想看入睡和苏醒时间。</p><p>接下来我又找到了小米手机自带的 <code>健康</code> 和 <code>小米穿戴</code> 两个应用，这两个数据统计做的还不错，但是不支持 <code>小米手环3</code>，只支持小米手环4和小米手环5。雷布斯你个倒霉孩子，又想骗我买新设备，还真别说我还真动了这个念头，毕竟也不贵。可是最重要的是过去两年的数据啊。哼哼，雷布斯你难不倒我的，我在 <a href="https://www.zhihu.com/question/34255518/answer/1187679143" target="_blank" rel="external nofollow noopener noreferrer">知乎的这个回答</a> 发现了这个 <a href="https://user.huami.com/hm_account/2.0.0/index.html?v=3.7.38&amp;platform_app=com.xiaomi.hm.health#/chooseDestory" target="_blank" rel="external nofollow noopener noreferrer">链接</a> ，在这里你可以导出你手环中所有的数据。</p><p>话不多说，睡眠数据以CSV格式保存，格式如下，我的数据是从 <code>2019年11月21日</code> 开始算起的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">date,lastSyncTime,deepSleepTime,shallowSleepTime,wakeTime,start,stop</span><br><span class="line">2019-01-21,1548087470,119,372,22,1547998080,1548028860</span><br><span class="line">2019-01-22,1548173888,114,388,40,1548084300,1548116820</span><br><span class="line">2019-01-23,1548260294,120,347,45,1548174000,1548204720</span><br><span class="line">2019-01-24,1548346721,95,145,16,1548263280,1548278640</span><br></pre></td></tr></table></figure><p>其中的几个字段意义如下：</p><ul><li>lastSyncTime：上次数据同步时间</li><li><code>deepSleepTime</code>：深睡的时间</li><li><code>shallowSleepTime</code>：浅睡的时间</li><li><code>wakeTime</code>：清醒的时间</li><li><code>start</code>：每天晚上的入睡时间</li><li><code>stop</code>：每天早上起来的时间</li></ul><p>其中最关键的就是 <code>start</code> 和 <code>stop</code> 字段，有了这两个数据，我就可以基本统计出过去两年里面的睡眠模式了。<a href="http://zhangwenli.com/blog/2015/12/26/sleeping-analysis/" target="_blank" rel="external nofollow noopener noreferrer">这里</a> 是一个程序媛小姐姐在五年前（嗯很久远了）统计的睡眠数据，还用 <code>ECharts</code> 很漂亮的可视化了出来。</p><p>【今天晚上又到了睡觉的时间，明天早上补完后面的部分吧，我一定会回来的！—— 2020.11.28 22:00】</p><p>雷布斯你放心，下一款最新的小米手环我一定会支持你的。</p><p>【周日出去浪了，今天把上期朝花夕拾剩余的部分给补起来 —— 2020.11.30 17:00】</p><p>参考 <a href="http://zhangwenli.com/blog/2015/12/26/sleeping-analysis/" target="_blank" rel="external nofollow noopener noreferrer">这里</a>  的数据可视化，我简单的分析了入睡时间和起床时间，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Scatter</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"></span><br><span class="line">datafile = <span class="string">"sleep.csv"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time</span><span class="params">(timestamp)</span>:</span></span><br><span class="line">    timeStamp = float(timestamp)</span><br><span class="line">    timeArray = time.localtime(timeStamp)</span><br><span class="line">    sleepDate = time.strftime(<span class="string">"%Y-%m-%d"</span>, timeArray)</span><br><span class="line">    <span class="keyword">return</span> sleepDate, timeArray.tm_hour</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_csv</span><span class="params">(datafile)</span>:</span></span><br><span class="line">    dateTime = []</span><br><span class="line">    sleepTime = []</span><br><span class="line">    awakeTime = []</span><br><span class="line">    <span class="keyword">with</span> open(datafile, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        r = csv.DictReader(f)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> r:</span><br><span class="line">            <span class="keyword">if</span> line[<span class="string">"deepSleepTime"</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            start, stop = line[<span class="string">"start"</span>], line[<span class="string">"stop"</span>]</span><br><span class="line">            _, pStart = get_time(start)</span><br><span class="line">            pDate, pStop = get_time(stop)</span><br><span class="line">            dateTime.append(pDate)</span><br><span class="line">            sleepTime.append(pStart)</span><br><span class="line">            awakeTime.append(pStop)</span><br><span class="line">    <span class="keyword">return</span> dateTime, sleepTime, awakeTime</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dateTime, sleepTime, awakeTime = parse_csv(datafile)</span><br><span class="line">    scatter = Scatter()</span><br><span class="line">    scatter.add_xaxis(dateTime)</span><br><span class="line">    <span class="comment">#scatter.add_yaxis("入睡时间", sleepTime)</span></span><br><span class="line">    scatter.add_yaxis(<span class="string">"起床时间"</span>, awakeTime)</span><br><span class="line">    scatter.set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"睡眠时间统计"</span>))</span><br><span class="line">    scatter.render()</span><br></pre></td></tr></table></figure><p>这里的数据分析非常浅显，首先入睡时间和起床时间只是从小时维度做了分析，粒度很粗。然后也没有进一步的按周、按月来分析，没有分析每天的睡眠时间，下次有时间搞一搞吧。</p><p>首先看入睡时间，可以看到，入睡时间分布最多的就是0点，也就是凌晨的时候，其次23点和1点也很频繁。11点之前睡很少很少，居然还有21点睡的？我不记得有过这种时刻，难道是手环统计误差？</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-30_sleep-time.png"></p><p>接下来看起床时间，最广泛的分布是在早上8点-9点这个范围，9点和10点的数据也不少，最可怕的是居然有一天睡到了12点，看了下那是 <code>2019-12-21</code>，那是周六的早上，你真的挺懒的：）</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-30_awake-time.png"></p><p>检讨下自己，这个作息真的不太健康，完全暴露了你的懒惰：）下周开始，每周的睡眠数据将会更新到「记录」模块，看看自己能不能做的更好。</p><p>【2020.12.06】更新了画图脚本，得出过去两年睡眠数据可视化如下：</p><div id="echarts9639" style="width: 100%;height: 600px;margin: 0 auto"></div><script type="text/javascript" src="https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js"></script><script type="text/javascript" src="http://gallery.echartsjs.com/dep/echarts/map/js/china.js"></script><script type="text/javascript">  // 基于准备好的dom，初始化echarts实例  var myChart = echarts.init(document.getElementById('echarts9639'));  // 指定图表的配置项和数据  option = {    title: {        text: '睡眠监控'     },    tooltip: {        trigger: 'axis',        formatter: function(params) {            function getHourMinute(timeValue) {                if (timeValue < 0) timeValue = 24 + timeValue;                var m = Math.floor((timeValue % 1) * 60);                m = m.toString().padStart(2, '0');                var h = Math.floor(timeValue);                return {                    hour: h,                     minute: m                }            }                        var sleepTime = getHourMinute(params[0].data)            var awakeTime = getHourMinute(params[1].data)            return params[0].seriesName + ": " + sleepTime.hour + ":" + sleepTime.minute + '<br />'                + params[1].seriesName + ": " + awakeTime.hour + ":" + awakeTime.minute + '<br />'                +  params[2].seriesName + ": " + params[2].data + '小时';        }    },    toolbox: {        feature: {            dataView: {show: true, readOnly: false},            restore: {show: true},            saveAsImage: {show: true}        }    },    legend: {        data: ['入睡时间', '起床时间']    },    xAxis: [        {            type: 'category',            data: ['2019-01-15', '2019-01-16', '2019-01-17', '2019-01-18', '2019-01-19', '2019-01-21', '2019-01-22', '2019-01-23', '2019-01-24', '2019-01-25', '2019-01-26', '2019-01-27', '2019-01-28', '2019-01-28', '2019-01-30', '2019-01-31', '2019-01-31', '2019-02-02', '2019-02-03', '2019-02-04', '2019-02-05', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', '2019-02-09', '2019-02-10', '2019-02-11', '2019-02-13', '2019-02-14', '2019-02-15', '2019-02-16', '2019-02-17', '2019-02-18', '2019-02-19', '2019-02-20', '2019-02-21', '2019-02-22', '2019-02-23', '2019-02-24', '2019-02-25', '2019-02-26', '2019-02-27', '2019-02-28', '2019-03-01', '2019-03-02', '2019-03-03', '2019-03-04', '2019-03-05', '2019-03-06', '2019-03-07', '2019-03-08', '2019-03-09', '2019-03-10', '2019-03-11', '2019-03-12', '2019-03-13', '2019-03-14', '2019-03-15', '2019-03-16', '2019-03-17', '2019-03-18', '2019-03-19', '2019-03-20', '2019-03-21', '2019-03-22', '2019-03-23', '2019-03-24', '2019-03-25', '2019-03-26', '2019-03-27', '2019-03-28', '2019-03-29', '2019-03-30', '2019-03-31', '2019-04-01', '2019-04-02', '2019-04-02', '2019-04-04', '2019-04-04', '2019-04-05', '2019-04-07', '2019-04-08', '2019-04-09', '2019-04-10', '2019-04-11', '2019-04-12', '2019-04-13', '2019-04-14', '2019-04-15', '2019-04-15', '2019-04-17', '2019-04-18', '2019-04-19', '2019-04-20', '2019-04-21', '2019-04-21', '2019-04-23', '2019-04-24', '2019-04-25', '2019-04-26', '2019-04-27', '2019-04-28', '2019-04-28', '2019-04-30', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-05', '2019-05-07', '2019-05-07', '2019-05-09', '2019-05-09', '2019-05-10', '2019-05-11', '2019-05-13', '2019-05-14', '2019-05-15', '2019-05-16', '2019-05-16', '2019-05-18', '2019-05-19', '2019-05-20', '2019-05-21', '2019-05-22', '2019-05-23', '2019-05-24', '2019-05-24', '2019-05-26', '2019-05-27', '2019-05-27', '2019-05-29', '2019-05-29', '2019-05-31', '2019-06-01', '2019-06-02', '2019-06-03', '2019-06-04', '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-10', '2019-06-11', '2019-06-12', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16', '2019-06-17', '2019-06-18', '2019-06-19', '2019-06-19', '2019-06-21', '2019-06-21', '2019-06-22', '2019-06-24', '2019-06-25', '2019-06-26', '2019-06-26', '2019-06-28', '2019-06-29', '2019-06-30', '2019-07-01', '2019-07-02', '2019-07-03', '2019-07-04', '2019-07-05', '2019-07-06', '2019-07-07', '2019-07-08', '2019-07-08', '2019-07-09', '2019-07-11', '2019-07-12', '2019-07-13', '2019-07-14', '2019-07-15', '2019-07-16', '2019-07-17', '2019-07-18', '2019-07-18', '2019-07-20', '2019-07-30', '2019-08-01', '2019-08-02', '2019-08-03', '2019-08-04', '2019-08-04', '2019-08-06', '2019-08-07', '2019-08-08', '2019-08-09', '2019-08-10', '2019-08-10', '2019-08-12', '2019-08-13', '2019-08-14', '2019-08-15', '2019-08-15', '2019-08-17', '2019-08-18', '2019-08-19', '2019-08-19', '2019-08-21', '2019-08-22', '2019-08-22', '2019-08-24', '2019-08-25', '2019-08-26', '2019-08-27', '2019-08-27', '2019-08-29', '2019-08-30', '2019-08-31', '2019-09-01', '2019-09-02', '2019-09-02', '2019-09-04', '2019-09-04', '2019-09-05', '2019-09-07', '2019-09-08', '2019-09-09', '2019-09-09', '2019-09-11', '2019-09-12', '2019-09-13', '2019-09-14', '2019-09-15', '2019-09-16', '2019-09-17', '2019-09-17', '2019-09-19', '2019-09-19', '2019-09-21', '2019-09-22', '2019-09-23', '2019-09-24', '2019-09-24', '2019-09-26', '2019-09-26', '2019-09-28', '2019-09-29', '2019-09-30', '2019-10-01', '2019-10-01', '2019-10-02', '2019-10-03', '2019-10-04', '2019-10-14', '2019-10-15', '2019-10-17', '2019-10-18', '2019-10-19', '2019-10-20', '2019-10-21', '2019-10-22', '2019-10-23', '2019-10-24', '2019-10-25', '2019-10-26', '2019-10-27', '2019-10-28', '2019-10-29', '2019-10-30', '2019-10-31', '2019-11-01', '2019-11-02', '2019-11-03', '2019-11-04', '2019-11-05', '2019-11-06', '2019-11-07', '2019-11-07', '2019-11-09', '2019-11-10', '2019-11-11', '2019-11-11', '2019-11-13', '2019-11-14', '2019-11-15', '2019-11-16', '2019-11-17', '2019-11-18', '2019-11-19', '2019-11-20', '2019-11-21', '2019-11-22', '2019-11-23', '2019-11-24', '2019-11-25', '2019-11-26', '2019-11-27', '2019-11-28', '2019-11-29', '2019-11-30', '2019-12-01', '2019-12-02', '2019-12-03', '2019-12-04', '2019-12-05', '2019-12-06', '2019-12-07', '2019-12-08', '2019-12-09', '2019-12-10', '2019-12-11', '2019-12-11', '2019-12-13', '2019-12-14', '2019-12-15', '2019-12-16', '2019-12-17', '2019-12-18', '2019-12-19', '2019-12-20', '2019-12-21', '2019-12-22', '2019-12-22', '2019-12-24', '2019-12-25', '2019-12-26', '2019-12-27', '2019-12-28', '2019-12-29', '2019-12-29', '2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-09', '2020-01-09', '2020-01-11', '2020-01-12', '2020-01-13', '2020-01-14', '2020-01-15', '2020-01-16', '2020-01-17', '2020-01-18', '2020-01-19', '2020-01-20', '2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24', '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28', '2020-01-29', '2020-01-30', '2020-01-31', '2020-02-01', '2020-02-02', '2020-02-02', '2020-02-03', '2020-02-04', '2020-06-30', '2020-07-01', '2020-07-02', '2020-07-02', '2020-07-03', '2020-07-05', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08', '2020-07-10', '2020-07-10', '2020-07-11', '2020-07-12', '2020-07-13', '2020-07-14', '2020-07-15', '2020-07-16', '2020-07-17', '2020-07-18', '2020-07-19', '2020-07-20', '2020-07-21', '2020-07-22', '2020-07-23', '2020-07-24', '2020-07-25', '2020-07-26', '2020-07-27', '2020-07-28', '2020-07-29', '2020-07-30', '2020-07-31', '2020-08-01', '2020-08-02', '2020-08-03', '2020-08-04', '2020-08-06', '2020-08-06', '2020-08-07', '2020-08-08', '2020-08-09', '2020-08-11', '2020-08-12', '2020-08-12', '2020-08-13', '2020-08-14', '2020-08-15', '2020-08-17', '2020-08-18', '2020-08-18', '2020-08-19', '2020-08-20', '2020-08-21', '2020-08-22', '2020-08-23', '2020-08-24', '2020-08-25', '2020-08-27', '2020-08-27', '2020-08-29', '2020-08-29', '2020-08-30', '2020-08-31', '2020-09-01', '2020-09-02', '2020-09-03', '2020-09-05', '2020-09-05', '2020-09-07', '2020-09-07', '2020-09-08', '2020-09-10', '2020-09-11', '2020-09-12', '2020-09-13', '2020-09-13', '2020-09-15', '2020-09-15', '2020-09-17', '2020-09-18', '2020-09-19', '2020-09-19', '2020-09-20', '2020-09-21', '2020-09-22', '2020-09-23', '2020-09-24', '2020-09-25', '2020-09-26', '2020-09-27', '2020-09-29', '2020-09-30', '2020-09-30', '2020-10-02', '2020-10-03', '2020-10-03', '2020-10-05', '2020-10-05', '2020-10-06', '2020-10-08', '2020-10-09', '2020-10-09', '2020-10-11', '2020-10-11', '2020-10-13', '2020-10-14', '2020-10-15', '2020-10-16', '2020-10-17', '2020-10-17', '2020-10-19', '2020-10-20', '2020-10-21', '2020-10-22', '2020-10-23', '2020-10-24', '2020-10-25', '2020-10-26', '2020-10-27', '2020-10-28', '2020-10-29', '2020-10-30', '2020-10-31', '2020-11-01', '2020-11-01', '2020-11-03', '2020-11-17', '2020-11-19', '2020-11-20', '2020-11-21', '2020-11-22', '2020-11-23', '2020-11-24', '2020-11-25', '2020-11-26', '2020-11-27', '2020-11-28'],            axisPointer: {                type: 'shadow'            }        }    ],    yAxis: [        {            type: 'value',            axisLine: {                show: false            },            name: '小时',            axisLabel: {                formatter: function (h) {                    h = Math.floor(h);                    if (h < 0) {                        return h + 24 + ':00';                    } else {                         return h + ':00';                    }                 },                 margin: 20            }        }    ],    series: [        {            name: '入睡时间',            type: 'line',            data: [0.0, 0.0, 0.0, 0.0, 0.0, -0.53, -0.58, 0.33, 1.13, 0.2, -0.27, 2.82, -0.67, 0.0, 0.8, 1.98, 0.0, -2.05, 0.58, 0.07, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57, 1.9300000000000002, 0.83, 0.12, 0.37, 0.57, 0.1, 0.47, 0.33, 1.92, 1.72, 0.35, 0.67, 0.35, 0.57, 0.93, 1.07, 1.17, -0.72, 0.95, -0.32, 0.18, 0.45, 0.27, 0.0, 1.37, -0.1, 0.22, 0.52, 0.42, 0.85, 0.88, 0.32, 0.1, -0.17, -1.08, 1.83, -0.8, 0.02, -0.25, -0.07, -0.72, -0.93, 0.0, 0.72, -0.77, 0.07, 0.52, -0.35, 0.0, -1.97, 0.0, 0.0, -1.77, 1.45, 0.3, 0.17, 3.98, 0.1, 0.58, -0.67, -0.45, 0.0, 2.02, -0.3, 0.78, 0.63, 0.33, 0.0, 2.32, -0.17, 1.33, -0.9, 1.83, 0.32, 0.0, -0.48, -0.42, 0.6, -1.3, 1.08, 0.48, 0.0, 3.1, 0.0, 1.5, 0.0, 0.0, 0.0, 1.67, -0.22, 0.47, 1.37, 0.0, -0.87, -0.8, -0.47, -0.48, 0.33, 0.0, 0.33, 0.0, -1.52, 0.98, 0.0, 0.17, 0.0, 0.55, 2.2800000000000002, -0.72, -0.32, -0.15, 0.87, 1.1, 1.9300000000000002, 0.35, -3.52, 1.05, 0.53, 0.33, 0.28, 3.0, 0.5, 1.22, 0.82, 1.28, 1.87, 0.0, -0.4, 0.0, 0.0, -0.27, -1.53, 1.08, 0.0, -0.12, 1.48, 0.37, 0.35, -0.22, 0.48, -0.03, 0.53, 1.33, -0.93, 0.27, 0.0, 0.0, 0.73, 0.58, 0.7, 0.62, 0.42, 0.82, 0.68, 0.67, 0.0, 0.62, 0.0, 0.63, 0.12, 1.07, 1.48, 0.0, 0.85, 0.75, 0.62, 0.65, 0.6, 0.0, 1.03, 1.22, 0.63, 0.87, 0.0, 0.97, 1.78, 1.55, 0.0, 0.38, 0.78, 0.0, 3.12, 0.9, 0.73, 0.57, 0.0, 0.72, 0.08, 0.58, 1.1, 0.7, 0.0, -0.17, 0.0, 0.0, -2.13, 2.7800000000000002, 0.37, 0.0, 0.63, 1.43, -2.12, 0.57, 0.85, 2.27, 1.8, 0.0, -3.08, 0.0, 2.38, 1.45, 3.65, 0.58, 0.0, 0.87, 0.0, 1.65, -0.6, 0.15, 1.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.62, 2.27, 2.1, 0.28, -0.48, -1.08, 1.35, 1.3, 1.62, -0.2, 0.47, -0.1, 1.28, 0.93, 2.0, 1.25, -0.32, 0.65, 1.27, 2.03, 3.92, 1.13, 0.0, 0.75, 0.85, 0.82, 0.0, 0.87, 0.53, 1.13, 0.62, 2.58, 1.55, 0.8, 1.3, 1.3, 0.87, 0.57, 2.92, 1.37, 0.02, 0.3, 1.42, -0.37, 0.93, 2.18, 1.18, 1.38, 0.57, 0.38, -0.35, 0.8, 0.45, 1.5699999999999998, 0.82, 0.98, 0.0, 1.17, 0.5, 0.93, 0.92, 4.65, 0.23, 5.32, -0.47, 2.25, 0.67, 0.0, 0.37, 2.08, 1.83, 1.12, 0.45, -0.6, 0.0, 1.42, 4.35, -0.42, 1.43, -0.08, 0.42, 0.73, 0.75, 1.17, 0.82, 0.0, 2.67, 2.83, 0.93, 0.8, -0.28, -0.15, 0.65, 1.2, 0.17, -2.05, 0.55, 0.33, 0.48, 0.75, 1.63, -0.77, 1.87, 1.33, 1.5, 0.9, 0.97, 0.22, 0.47, 0.0, 0.0, 0.0, 0.7, -0.42, -0.1, 0.0, 0.0, 4.1, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, -0.38, -0.72, 0.0, 0.0, 0.0, 0.0, 0.27, -0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.0, -0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, -1.68, 0.0, 0.0, -0.07, 0.65, 0.32, -0.68, 0.0, 5.12, 0.0, -0.17, -0.53, 5.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.07, 2.07, 0.0, 1.13, 3.45, 0.0, -1.65, 0.0, 0.0, 0.75, 1.25, 0.0, 3.45, 0.0, 0.25, 1.22, 1.62, 0.43, 1.98, 0.0, -0.17, 0.57, 1.13, -0.88, 0.65, 0.77, 1.62, 0.1, 0.8, 0.95, 1.12, 0.27, 0.62, -0.05, 0.0, -1.63, 0.0, -0.55, 3.12, 0.33, 1.77, -0.7, 0.28, -0.2, 1.42, 0.38, -1.0]        },        {            name: '起床时间',            type: 'line',            data: [0.0, 0.0, 0.0, 0.0, 0.0, 8.02, 8.45, 8.87, 5.4, 8.72, 9.37, 9.87, 8.38, 0.0, 5.58, 6.42, 0.0, 7.63, 7.1, 8.1, 7.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.92, 8.7, 5.33, 8.27, 8.8, 8.93, 6.02, 8.15, 8.92, 7.82, 9.03, 9.03, 9.38, 6.32, 8.68, 8.78, 9.25, 7.52, 9.2, 7.6, 3.9, 8.25, 8.27, 8.5, 8.9, 8.53, 9.07, 7.93, 9.13, 5.88, 9.05, 6.6, 8.88, 9.17, 8.52, 8.98, 8.95, 4.98, 8.22, 8.83, 6.77, 9.37, 7.4, 8.27, 8.15, 9.12, 10.33, 9.03, 6.32, 0.0, 6.0, 0.0, 0.0, 7.15, 7.92, 7.72, 7.9, 8.57, 8.23, 7.58, 8.5, 8.45, 0.0, 8.07, 8.3, 8.3, 9.12, 8.3, 0.0, 8.32, 8.37, 7.85, 7.13, 8.63, 5.38, 0.0, 8.78, 4.78, 6.93, 6.93, 7.9, 9.02, 0.0, 7.97, 0.0, 7.82, 0.0, 0.0, 0.0, 8.48, 6.97, 8.22, 8.3, 0.0, 6.93, 6.93, 5.42, 7.65, 7.1, 7.38, 5.38, 0.0, 4.43, 7.62, 0.0, 8.17, 0.0, 6.92, 10.38, 9.58, 7.8, 7.92, 7.67, 8.13, 10.55, 8.87, 5.08, 4.77, 5.63, 8.02, 8.6, 8.87, 8.9, 8.95, 8.57, 8.88, 8.77, 0.0, 3.67, 0.0, 0.0, 8.23, 8.07, 7.97, 0.0, 8.53, 7.92, 8.57, 3.7199999999999998, 4.82, 8.0, 3.92, 9.17, 7.38, 8.88, 8.83, 0.0, 0.0, 8.4, 8.93, 9.0, 8.25, 8.73, 8.85, 8.72, 8.88, 0.0, 9.77, 0.0, 7.83, 8.92, 8.78, 5.02, 0.0, 6.75, 4.43, 9.13, 5.75, 8.68, 0.0, 8.43, 8.93, 6.22, 5.47, 0.0, 8.55, 6.65, 6.13, 0.0, 5.82, 5.8, 0.0, 8.65, 10.42, 8.95, 8.55, 0.0, 8.35, 8.5, 8.7, 9.02, 8.88, 0.0, 3.7, 0.0, 0.0, 8.78, 6.63, 8.05, 0.0, 8.32, 8.08, 4.5, 8.55, 7.98, 4.87, 5.68, 0.0, 0.17, 0.0, 7.32, 9.15, 8.7, 8.67, 0.0, 5.57, 0.0, 9.0, 4.72, 6.9, 4.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.92, 8.97, 6.05, 9.75, 8.48, 7.73, 5.62, 5.53, 9.67, 9.05, 5.38, 8.98, 7.43, 4.47, 8.92, 4.28, 6.08, 8.8, 5.7, 10.05, 10.03, 9.93, 0.0, 9.35, 8.93, 9.47, 0.0, 7.83, 7.93, 5.25, 8.87, 5.6, 9.87, 4.25, 6.68, 4.18, 8.95, 7.0, 5.38, 3.9, 9.52, 8.9, 10.72, 7.62, 10.82, 11.45, 9.47, 10.97, 6.85, 9.75, 8.42, 5.63, 9.2, 9.8, 10.12, 5.4, 0.0, 6.2, 5.55, 4.65, 5.75, 8.95, 9.03, 10.73, 7.02, 12.83, 6.92, 0.0, 9.48, 6.0, 10.53, 9.1, 8.72, 8.05, 0.0, 11.12, 6.82, 9.53, 5.73, 9.83, 8.43, 6.15, 8.65, 8.55, 9.0, 0.0, 10.68, 9.75, 9.45, 6.43, 6.08, 9.23, 8.38, 10.33, 4.88, 8.08, 5.87, 7.75, 10.05, 9.32, 10.97, 8.52, 10.28, 10.65, 6.47, 6.13, 9.75, 7.83, 9.83, 0.0, 0.0, 0.0, 7.53, 6.78, 4.05, 0.0, 0.0, 6.45, 0.0, 0.0, 0.0, 0.0, 3.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.17, 0.0, 0.0, 0.0, 0.0, 8.03, 7.7, 0.0, 0.0, 0.0, 0.0, 7.8, 7.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.42, 0.0, 7.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.18, 0.0, 0.9, 0.0, 0.0, 8.32, 8.18, 3.68, 4.12, 0.0, 6.67, 0.0, 8.7, 8.15, 8.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.42, 8.13, 0.0, 4.42, 5.77, 0.0, 6.87, 0.0, 0.0, 6.12, 9.52, 0.0, 9.68, 0.0, 8.58, 9.02, 5.87, 8.78, 8.85, 0.0, 8.78, 5.13, 8.9, 3.32, 8.38, 10.77, 9.32, 6.15, 6.92, 8.32, 9.27, 6.62, 4.18, 3.87, 0.0, 3.5300000000000002, 0.0, 7.57, 7.95, 7.08, 9.85, 7.13, 5.42, 3.52, 8.3, 8.52, 8.03]        }    ]};  // 使用刚指定的配置项和数据显示图表。  myChart.setOption(option);</script><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>有研究表明人睡眠存在一个生物节律，即由4-5个睡眠周期组成，每一个周期又分为5个阶段，由两个时相组成：非快速眼动睡眠相NREM（前4期）和快速眼动睡眠REM（在睡眠70-90分钟后出现）。每个周期持续90分钟左右，每晚可出现4-5个周期。</p><h3 id="非快速眼动睡眠相NREM"><a href="#非快速眼动睡眠相NREM" class="headerlink" title="非快速眼动睡眠相NREM"></a>非快速眼动睡眠相NREM</h3><p>此睡眠相主要由入眠期、浅睡和中睡期组成，可以伴有少量的深睡期。</p><h4 id="入睡期"><a href="#入睡期" class="headerlink" title="入睡期"></a>入睡期</h4><p>是睡眠的开始，昏昏欲睡的感觉就属于这一阶段。此时脑波开始变化，频率渐缓，振幅渐小。此阶段是清醒和睡眠之间的转换期，人非常容易醒来，约占睡眠总时间的10%。</p><h4 id="浅睡期和中睡期"><a href="#浅睡期和中睡期" class="headerlink" title="浅睡期和中睡期"></a>浅睡期和中睡期</h4><p>开始正式睡眠。此时脑波渐呈不规律进行，频率与振幅忽大忽小，其中偶尔会出现被称为“睡眠锭”的高频、大波幅脑波，以及被称为“K结”的低频、很大波幅脑波。此期容易觉醒，入睡困难者，常自行惊醒，约占整个睡眠期的50%。</p><h4 id="深睡期"><a href="#深睡期" class="headerlink" title="深睡期"></a>深睡期</h4><p>沉睡阶段，被试不易被叫醒。此时脑波变化很大，频率只有每秒1~2周，为<strong>慢波睡眠</strong>，但振幅增加较大，呈现变化缓慢的曲线。此期睡眠深，觉醒相当困难，在每个睡眠周期中约持续30分钟，然后进入快速眼动睡眠。</p><p>这四个阶段的睡眠均不出现眼球快速跳动现象，故统称为<strong>非快速眼动睡眠</strong>（non－rapideyemovementsleep，简称<strong>NREM</strong>。</p><p><img alt="睡眠周期" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-30_sleep-hypnogram.svg"></p><h3 id="快速动眼期REM"><a href="#快速动眼期REM" class="headerlink" title="快速动眼期REM"></a>快速动眼期REM</h3><p>这一阶段以深睡眠为主，脑波迅速改变，出现与清醒状态时的脑波相似的高频率、低波幅脑波，但其中会有特点鲜明的锯齿状波。睡眠者通常会有翻身的动作，并很容易惊醒，似乎又进入阶段1的睡眠，但实际是进入了一个被称为<strong>快速眼动睡眠</strong>（rapideyemovementsleep，简称<strong>REM</strong>）的睡眠阶段。因为，此时除了脑波的改变之外，被试的眼球会呈现快速跳动现象。如果此时将其唤醒，大部分人报告说正在做梦。因此，REM就成为睡眠第五个阶段的重要特征，也成为心理学家研究做梦的重要根据。</p><p>在整个睡眠周期中，NREM与REM有规律地交替出现，两种不同时相睡眠各出现一次为一个睡眠期。入睡后必须先经过NREM阶段，才能进入REM阶段，而人体只有在经过了REM阶段后才有真正睡过觉的感觉，体能才能得到很好的恢复。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遵守上周的约定，这周总算是定期发布了2020年「朝花夕拾」的第二十六期 &lt;code&gt;晚安&lt;/code&gt;，在上期的记录中，我告诉自己要搭建起自己的睡眠数据监控系统，这周我来介绍下我是如何折腾的。封面来自今天在家拍摄的晚霞，晚安，希望大家都能够睡个好觉。&lt;/p&gt;

    &lt;div id=&quot;aplayer-NPEtEfqx&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;439122551&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-28_sunset-2.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="sleep" scheme="http://houmin.cc/tags/sleep/"/>
    
      <category term="嫦娥五号" scheme="http://houmin.cc/tags/%E5%AB%A6%E5%A8%A5%E4%BA%94%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>【Service Mesh】Envoy 入门</title>
    <link href="http://houmin.cc/posts/7beb34d2/"/>
    <id>http://houmin.cc/posts/7beb34d2/</id>
    <published>2020-11-25T04:15:08.000Z</published>
    <updated>2020-12-15T11:25:17.458Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><code>Envoy</code> 是一款由 Lyft 开源的高性能数据和服务代理软件，使用现代 C++ 开发，提供四层和七层网络代理能力。尽管在设计之初 <code>Envoy</code>没有将性能作为最终的目标，而是更加强调模块化、易测试、易开发等特性，可它仍旧拥有足可媲美 Nginx 等经典代理软件的超高性能。在保证性能的同时，<code>Envoy</code>也提供了强大的流量治理能力和可观察性。其独创的 xDS 协议则成为了构建 Service Mesh 通用数据面 API（UPDA）的基石。</p><a id="more"></a><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img alt="Envoy Architecture" data-src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200504160047.png"></p><p>首先介绍Envoy中的一些基本概念：</p><ul><li>Downstream：下游主机，指连接到Envoy的主机，这些主机用来发送请求并接受响应。</li><li>Upstream：上游主机，指接收来自Envoy连接和请求的主机，并返回响应。</li><li>Listener：服务或程序的监听器， Envoy暴露一个或多个监听器监听下游主机的请求，当监听到请求时，通过Filter Chain把对请求的处理全部抽象为Filter， 例如ReadFilter、WriteFilter、HttpFilter等。</li><li>Cluster：服务提供集群，指Envoy连接的一组逻辑相同的上游主机。Envoy通过服务发现功能来发现集群内的成员，通过负载均衡功能将流量路由到集群的各个成员。</li><li>xDS：xDS中的x是一个代词，类似云计算里的XaaS可以指代IaaS、PaaS、SaaS等。DS为Discovery Service，即发现服务的意思。xDS包括CDS（cluster discovery service）、RDS（route discovery service）、EDS（endpoint discovery service）、ADS（aggregated discovery service），其中ADS称为聚合的发现服务，是对CDS、RDS、LDS、EDS服务的统一封装，解决CDS、RDS、LDS、EDS信息更新顺序依赖的问题，从而保证以一定的顺序同步各类配置信息。以上Endpoint、Cluster、Route的概念介绍如下：<ul><li>Endpoint：一个具体的“应用实例”，类似于Kubernetes中的一个Pod；</li><li>Cluster：可以理解“应用集群”，对应提供相同服务的一个或多个Endpoint， 类似Kubernetes中Service概念，即一个Service提供多个相同服务的Pod；</li><li>Route：当我们做金丝雀发布部署时，同一个服务会有多个版本，这时需要Route规则规定请求如何路由到其中的某个版本上。</li></ul></li></ul><p>xDS模块的功能是通过Envoy API V1（基于HTTP）或V2（基于gRPC）实现一个服务端将配置信息暴露给上游主机，等待上游主机的拉取。</p><p>Envoy正常的工作流程为Host A（下游主机）发送请求至上游主机（Host B、Host C、Host D等），Envoy通过Listener监听到有下游主机的请求，收到请求后的Envoy将所有请求流量劫持至Envoy内部，并将请求内容抽象为Filter Chains路由至某个上游主机中从而实现路由转发及负载均衡能力。</p><p>Envoy为了实现流量代理能力通常需要一个统一的配置文件来记录信息以便启动时加载，在Envoy中启动配置文件有静态配置和动态配置两种方式。静态配置是将配置信息写入文件中，启动时直接加载，动态配置通过xDS实现一个Envoy的服务端（可以理解为以API接口对外实现服务发现能力）。</p><h3 id="Network-Topology"><a href="#Network-Topology" class="headerlink" title="Network Topology"></a>Network Topology</h3><p>Envoy作为Service Mesh中的 sidecar 代理，请求可以通过 ingress 或者 egress listener 到达 envoy。</p><ul><li>Ingress Listener 负责从服务网格中其他节点接受请求，并将请求转发到本地应用。本地应用的响应之后通过 Envoy 转发到 downstream。</li><li>Egress Listener 负责从本地应用接受请求，并将请求转发到服务网格中的其他节点。</li></ul><p><img alt="Service Mesh" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-topology-service-mesh.svg"></p><p>除了服务网格外，Envoy还可以用作很多其他的请求，比如作为内部的负载均衡器：</p><p><img alt="Internal Load Balancer" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-topology-ilb.svg"></p><p>或者作为网络边缘的 <code>ingress/egress</code> 代理：</p><p><img alt="Ingress/Egress Proxy on Network Edge" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-topology-edge.svg"></p><p>在实际应用中，Envoy一般会发挥上述多种功能，一个网络请求路径中可能会通过多个Envoy：</p><p><img alt="Hybrid Envoy" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-topology-hybrid.svg"></p><p>为了可靠性和可扩充性，Envoy可能会被配置成多层拓扑的形式：</p><p><img alt="Envoy Tiered" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-topology-tiered.svg"></p><h3 id="High-Level-Architecture"><a href="#High-Level-Architecture" class="headerlink" title="High Level Architecture"></a>High Level Architecture</h3><p>Envoy中服务请求处理过程可以大致分为两个部分：</p><ul><li>Listener 子系统：处理来自 downstream 的请求。</li><li>Cluster 子系统：负责选择和配置 upstream 连接。</li></ul><p><img alt="High Level Architecture" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-architecture.svg"></p><p>Envoy采用了基于事件的线程模型：</p><ul><li>一个主线程负责server的生命周期，配置处理，统计等</li><li>多个worker线程负责处理请求。</li></ul><p>所有的线程都运行在一个基于 <a href="https://libevent.org/" target="_blank" rel="external nofollow noopener noreferrer">libevent</a> 的事件循环中，任何 downstream 的 TCP连接都会被分配一个 work 线程来处理</p><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p>Envoy 进程中运行着一系列 <code>Inbound/Outbound</code> 监听器（Listener），<code>Inbound</code> 代理入站流量，<code>Outbound</code> 代理出站流量。Listener 的核心就是过滤器链（FilterChain），链中每个过滤器都能够控制流量的处理流程。过滤器链中的过滤器分为两个类别：</p><ul><li><strong>网络过滤器</strong>（Network Filters）: 工作在 <code>L3/L4</code>，是 Envoy 网络连接处理的核心，处理的是原始字节，分为 <code>Read</code>、<code>Write</code> 和 <code>Read/Write</code> 三类。</li><li><strong>HTTP 过滤器</strong>（HTTP Filters）: 工作在 <code>L7</code>，由特殊的网络过滤器 <code>HTTP connection manager</code> 管理，专门处理 <code>HTTP1/HTTP2/gRPC</code> 请求。它将原始字节转换成 <code>HTTP</code> 格式，从而可以对 <code>HTTP</code> 协议进行精确控制。</li></ul><p>除了 <code>HTTP connection manager</code> 之外，还有一种特别的网络过滤器叫 <code>Thrift Proxy</code>。<code>Thrift</code> 是一套包含序列化功能和支持服务通信的 RPC 框架，详情参考<a href="https://zh.wikipedia.org/wiki/Thrift" target="_blank" rel="external nofollow noopener noreferrer">维基百科</a>。Thrift Proxy 管理了两个 Filter：<a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/other_protocols/thrift_filters/router_filter" target="_blank" rel="external nofollow noopener noreferrer">Router</a> 和 <a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/other_protocols/thrift_filters/rate_limit_filter" target="_blank" rel="external nofollow noopener noreferrer">Rate Limit</a>。</p><p>除了过滤器链之外，还有一种过滤器叫<strong>监听器过滤器</strong>（Listener Filters），它会在过滤器链之前执行，用于操纵连接的<strong>元数据</strong>。这样做的目的是，无需更改 Envoy 的核心代码就可以方便地集成更多功能。例如，当监听的地址协议是 <code>UDP</code> 时，就可以指定 UDP 监听器过滤器。根据上面的分类，Envoy 过滤器的架构如下图所示：</p><p><img alt="img" data-src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200504224710.png"></p><h2 id="Request-Flow"><a href="#Request-Flow" class="headerlink" title="Request Flow"></a>Request Flow</h2><h3 id="Listener-TCP-Accept"><a href="#Listener-TCP-Accept" class="headerlink" title="Listener TCP Accept"></a>Listener TCP Accept</h3><p><img alt="Listener TCP Accept" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-listeners.svg"></p><h3 id="Listener-filter-chains-and-network-filter-chain-matching"><a href="#Listener-filter-chains-and-network-filter-chain-matching" class="headerlink" title="Listener filter chains and network filter chain matching"></a>Listener filter chains and network filter chain matching</h3><p><img alt="Listener Filter Chains" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-listener-filters.svg"></p><p><img alt="../_images/lor-filter-chain-match.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-filter-chain-match.svg"></p><h3 id="TLS-transport-socket-decryption"><a href="#TLS-transport-socket-decryption" class="headerlink" title="TLS transport socket decryption"></a>TLS transport socket decryption</h3><p><img alt="../_images/lor-transport-socket.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-transport-socket.svg"></p><h3 id="Network-filter-chain-processing"><a href="#Network-filter-chain-processing" class="headerlink" title="Network filter chain processing"></a>Network filter chain processing</h3><p><img alt="../_images/lor-network-filters.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-network-filters.svg"></p><p><img alt="../_images/lor-network-read.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-network-read.svg"></p><p><img alt="../_images/lor-network-write.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-network-write.svg"></p><h3 id="HTTP-2-codec-encoding"><a href="#HTTP-2-codec-encoding" class="headerlink" title="HTTP/2 codec encoding"></a>HTTP/2 codec encoding</h3><h3 id="TLS-transport-socket-encryption"><a href="#TLS-transport-socket-encryption" class="headerlink" title="TLS transport socket encryption"></a>TLS transport socket encryption</h3><p><img alt="../_images/lor-http-filters.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-http-filters.svg"></p><p><img alt="../_images/lor-http.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-http.svg"></p><p><img alt="../_images/lor-http-decode.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-http-decode.svg"></p><p><img alt="../_images/lor-http-encode.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-http-encode.svg"></p><p><img alt="../_images/lor-route-config.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-route-config.svg"></p><h3 id="Load-Balancing"><a href="#Load-Balancing" class="headerlink" title="Load Balancing"></a>Load Balancing</h3><p><img alt="../_images/lor-lb.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-lb.svg"></p><h3 id="Response-path-and-HTTP-lifecycle"><a href="#Response-path-and-HTTP-lifecycle" class="headerlink" title="Response path and HTTP lifecycle"></a>Response path and HTTP lifecycle</h3><p><img alt="../_images/lor-client.svg" data-src="https://www.envoyproxy.io/docs/envoy/latest/_images/lor-client.svg"></p><h3 id="Post-request-processing"><a href="#Post-request-processing" class="headerlink" title="Post-request processing"></a>Post-request processing</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://developer.aliyun.com/article/606655" target="_blank" rel="external nofollow noopener noreferrer">https://developer.aliyun.com/article/606655</a></li><li><a href="https://www.cnblogs.com/popsuper1982/p/9841978.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/popsuper1982/p/9841978.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Envoy&lt;/code&gt; 是一款由 Lyft 开源的高性能数据和服务代理软件，使用现代 C++ 开发，提供四层和七层网络代理能力。尽管在设计之初 &lt;code&gt;Envoy&lt;/code&gt;没有将性能作为最终的目标，而是更加强调模块化、易测试、易开发等特性，可它仍旧拥有足可媲美 Nginx 等经典代理软件的超高性能。在保证性能的同时，&lt;code&gt;Envoy&lt;/code&gt;也提供了强大的流量治理能力和可观察性。其独创的 xDS 协议则成为了构建 Service Mesh 通用数据面 API（UPDA）的基石。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200504160047.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="网络" scheme="http://houmin.cc/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="envoy" scheme="http://houmin.cc/tags/envoy/"/>
    
  </entry>
  
  <entry>
    <title>【Service Mesh】Istio 流量控制</title>
    <link href="http://houmin.cc/posts/151719f0/"/>
    <id>http://houmin.cc/posts/151719f0/</id>
    <published>2020-11-24T08:47:28.000Z</published>
    <updated>2020-12-02T11:40:50.219Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>流量控制是指对系统流量的管控，包括了对网格入口的流量、网格出口的流量以及在网格内部微服务间相互调用流量的控制。在 <a href="../22cae0b8">Istio 入门</a> 中我们知道，Istio 架构在逻辑上分为 Control plane 和 Data plane，Control plane 负责整体管理和配置代理， Data plane 负责网格内所有微服务间的网络通信，同时还收集报告网络请求的遥测数据等。流量控制是在 Data plane 层实现。</p><a id="more"></a><p><img alt="Istio Architecture" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-arch.svg"></p><h2 id="路由和流量转移"><a href="#路由和流量转移" class="headerlink" title="路由和流量转移"></a>路由和流量转移</h2><p>Istio 为了控制服务请求，引入了服务版本（version）的概念，可以通过版本这一标签将服务进行区分。版本的设置是非常灵活的，以下是几种典型的设置方式：</p><ul><li>根据服务的迭代编号进行定义（如 v1、v2 版本）</li><li>根据部署环境进行定义（比如 dev、staging、production）</li><li>自定义的任何用于区分服务的某种标记</li></ul><p>通过版本标签，Istio 就可以定义灵活的路由规则来控制流量，上面提到的金丝雀发布这类应用场景就很容易实现了。</p><p>下图展示了使用服务版本实现路由分配的例子。服务版本定义了版本号（v1.5、v2.0-alpha）和环境（us-prod、us-staging）两种信息。服务 B 包含了 4 个 Pod，其中 3 个是部署在生产环境的 v1.5 版本，而 Pod4 是部署在预生产环境的 v2.0-alpha 版本。运维人员可以根据服务版本来指定路由规则，使 99% 的流量流向 v1.5 版本，而 1% 的流量进入 v2.0-alpha 版本。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_istio-routing.png"></p><p>除了上面介绍的服务间流量控制外，还能控制与网格边界交互的流量。可以在系统的入口和出口处部署 Sidecar 代理，让所有流入和流出的流量都由代理进行转发。负责入和出的代理就叫做入口网关和出口网关，它们把守着进入和流出网格的流量。下图展示了 Ingress 和 Egress 在请求流中的位置，有了他们俩，也就可以控制出入网格的流量了。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_istio-gateway.png"></p><p>Istio 还能设置流量策略。比如可以对连接池相关的属性进行设置，通过修改最大连接等参数，实现对请求负载的控制。还可以对负载均衡策略进行设置，在轮询、随机、最少访问等方式之间进行切换。还能设置异常探测策略，将满足异常条件的实例从负载均衡池中摘除，以保证服务的稳定性。</p><hr><p>Istio 的流量路由规则可以让您很容易的控制服务之间的流量和 API 调用。Istio 在服务层面提供了断路器，超时，重试等功能，通过这些功能可以简单地实现 A/B 测试，金丝雀发布，基于百分比的流量分割等，此外还提供了开箱即用的故障恢复功能，用于增加应用的健壮性，以应对服务故障或网络故障。这些功能都可以通过 Istio 的流量管理 API 添加流量配置来实现。</p><p>跟其他 Istio 配置一样，流量管理 API 也使用 CRD 指定。本小节主要介绍下面几个典型的流量管理 API 资源，以及这些 API 的功能和使用示例。</p><h3 id="VirtualService"><a href="#VirtualService" class="headerlink" title="VirtualService"></a>VirtualService</h3><p>VirtualService 由一组 <strong>路由规则</strong> 组成，描述了 <strong>用户请求的目标地址</strong> 到 <strong>服务网格中实际工作负载</strong> 之间的映射。在这个映射中，VirtualService提供了丰富的配置方式，可以为发送到这些 Workloads 的流量指定不同的路由规则。对应于具体的配置，用户请求的目标地址用 <code>hosts</code> 字段来表示，网格内的实际负载由每个 <code>route</code> 配置项中的 <code>destination</code> 字段指定。</p><pre class="mermaid">graph LRsubgraph VirtualServiceClientRequests -- DifferentTrafficRoutingRules --> DestinationWorkloadsHosts -- DifferentTrafficRoutingRules --> RouteDestinationend</pre><p>VirtualService 通过解耦 <strong>用户请求的目标地址</strong> 和 <strong>真实响应请求的目标工作负载</strong>，为服务提供了合适的统一抽象层，而由此演化设计的配置模型为管理这方面提供了一致的环境。对于原生 Kubernetes 而言，只有在 Ingress 处有这种路由规则的定义，对于集群内部不同Service的不同版本之间，并没有类似 VirtualService 的定义。</p><p>使用 VirtualService，可以为一个或多个主机名指定流量行为。在 VirtualService 中使用路由规则，告诉 Envoy如何发送 VirtualService 的流量到适当的目标。路由目标可以是相同服务的不同版本，或者是完全不同的服务。</p><p>一个典型的应用场景是将流量发送到被指定为服务子集的服务的不同版本。客户端将 VirtualService 视为一个单一实体，将请求发送至 VirtualService 主机，然后 Envoy 根据 VirtualService 规则把流量路由到不同的版本中。</p><p>这种方式可以方便地创建一种金丝雀的发布策略实现新版本流量的平滑比重升级。流量路由完全独立于实例部署，这意味着实现新版本服务的实例可以根据流量的负载来伸缩，完全不影响流量路由。相比之下，类似 Kubernetes 的容器调度平台仅支持基于部署中实例扩缩容比重的流量分发，那样会日趋复杂化。关于使用VirtualService实现金丝雀部署，可以参考 <a href="https://istio.io/latest/blog/2017/0.1-canary/" target="_blank" rel="external nofollow noopener noreferrer">Canary</a> 。</p><p>VirtualService 也提供了如下功能。</p><ul><li>通过单个 VirtualService 处理多个应用程序服务。例如，如果您的服务网格使用是 Kubernetes，您可以配置一个 VirtualService 来处理一个特定命名空间的所有服务。将单一的 VirtualService 映射为多个“真实”的服务特别有用，可以在不需要客户适应转换的情况下，将单体应用转换为微服务构建的复合应用系统。您的路由规则可以指定“请求到 <code>monolith.com</code> 的 URLs 跳转至 <code>microservice A</code> 中”。</li><li>和 Gateway  一起配置流量规则来控制入口和出口流量。</li></ul><p>在一些应用场景中，由于指定服务子集，需要配置 DestinationRule 来使用这些功能。在不同的对象中指定服务子集以及其他特定的目标策略可以帮助您在不同的 VirtualService 中清晰地复用这些功能。</p><p>下面的 VirtualService 根据是否来自于特定用户路由请求到不同的服务版本中（如果请求来自用户 <code>jason</code> ，则访问 <code>v2</code> 版本的 <code>reviews</code>，否则访问 <code>v3</code> 版本）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">reviews</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">reviews</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">headers:</span></span><br><span class="line">        <span class="attr">end-user:</span></span><br><span class="line">          <span class="attr">exact:</span> <span class="string">jason</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v2</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v3</span></span><br></pre></td></tr></table></figure><p>下面对这些字段依次解释：</p><h4 id="Hosts"><a href="#Hosts" class="headerlink" title="Hosts"></a>Hosts</h4><p>用来配置 Downstream 访问的可寻址地址，也就是用户请求的目标地址。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hosts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">reviews</span></span><br></pre></td></tr></table></figure><ul><li>VirtualService 主机名可以是 IP 地址、 DNS 域名、完全限定域名（FQDN)</li><li>也可以是 依赖于平台的一个简称（例如 Kubernetes 服务的短名称）</li><li>也可以使用通配符 <code>*</code>前缀，创建一组匹配所有服务的路由规则</li><li>VirtualService 的 <code>hosts</code> 实际上不必是 Istio 服务注册的一部分，它只是虚拟的目标地址。这可以为没有路由到网格内部的虚拟主机建模。</li></ul><h4 id="路由规则"><a href="#路由规则" class="headerlink" title="路由规则"></a>路由规则</h4><p><code>http</code> 字段用来配置路由规则，通常情况下配置一组路由规则，当请求到来时，自上而下依次进行匹配，直到匹配成功后跳出匹配。它可以对请求的 uri、method、authority、headers、port、queryParams 以及是否对 uri 大小写敏感等进行配置。</p><blockquote><p>除了HTTP协议，也可以使用 <code>tcp</code> 和 <code>tls</code> 片段为 <a href="https://istio.io/latest/docs/reference/config/networking/virtual-service/#TCPRoute" target="_blank" rel="external nofollow noopener noreferrer">TCP</a> 和未终止的 <a href="https://istio.io/docs/reference/config/networking/virtual-service/#TLSRoute" target="_blank" rel="external nofollow noopener noreferrer">TLS</a> 流量设置路由规则</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">headers:</span></span><br><span class="line">      <span class="attr">end-user:</span></span><br><span class="line">        <span class="attr">exact:</span> <span class="string">jason</span></span><br><span class="line">  <span class="attr">route:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">      <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">      <span class="attr">subset:</span> <span class="string">v2</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">      <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">      <span class="attr">subset:</span> <span class="string">v3</span></span><br></pre></td></tr></table></figure><p>我们推荐在每个 VirtualService 中配置一条默认「无条件的」或者基于权重的规则以确保 VirtualService 至少有一条匹配的路由。</p><h5 id="Destination"><a href="#Destination" class="headerlink" title="Destination"></a>Destination</h5><p>路由片段的 <code>destination</code> 字段指定符合匹配条件的流量目标地址。这里不像 VirtualService 的 <code>hosts</code>，Destination 的 <code>host</code> 必须是存在于 Istio 服务注册中心的实际目标地址，否则 Envoy 不知道该将请求发送到哪里。这个目标地址可以是代理的网格服务或者作为服务入口加入的非网格服务。下面的场景中我们运行在 Kubernetes 平台上，主机名是 Kubernetes 的服务名。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">route:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">    <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">    <span class="attr">subset:</span> <span class="string">v2</span></span><br></pre></td></tr></table></figure><blockquote><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">*Note for Kubernetes users*: When short names are used (e.g. "reviews" instead of "reviews.default.svc.cluster.local"), Istio will interpret the short name based on the namespace of the rule, not the service. A rule in the "default" namespace containing a host "reviews will be interpreted as "reviews.default.svc.cluster.local", irrespective of the actual namespace associated with the reviews service. To avoid potential misconfiguration, it is recommended to always use fully qualified domain names over short names.</span></span><br></pre></td></tr></table></figure></blockquote><h5 id="Match"><a href="#Match" class="headerlink" title="Match"></a>Match</h5><p>路由规则是将特定流量子集路由到特定目标地址的强大工具。可以在流量端口、<code>header</code> 字段、 URL 等内容上设置匹配条件。例如，下面的VirtualService 使用户发送流量到两个独立的服务，ratings and reviews， 就好像它们是 <code>http://bookinfo.com/</code> 这个更大的 VirtualService 的一部分。VirtualService 规则根据请求的 URL 和指向适当服务的请求匹配流量。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bookinfo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">bookinfo.com</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/reviews</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/ratings</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">ratings</span></span><br></pre></td></tr></table></figure><p>对于匹配条件，您可以使用确定的值，一条前缀、或者一条正则表达式。</p><p>您可以使用 <code>AND</code> 向同一个 <code>match</code> 块添加多个匹配条件， 或者使用 <code>OR</code> 向同一个规则添加多个 <code>match</code> 块。对于任意给定的 VirtualService ，您可以配置多条路由规则。这可以使您的路由条件在一个单独的 VirtualService 中基于业务场景的复杂度来进行相应的配置。可以在 <a href="https://istio.io/docs/reference/config/networking/virtual-service/#HTTPMatchRequest" target="_blank" rel="external nofollow noopener noreferrer">HTTPMatchRequest 参考</a>中查看匹配条件字段和他们可能的值。</p><p>再者进一步使用匹配条件，您可以使用基于“权重”百分比分发流量。这在 A/B 测试和金丝雀部署中非常有用。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">reviews</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">75</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v2</span></span><br><span class="line">      <span class="attr">weight:</span> <span class="number">25</span></span><br></pre></td></tr></table></figure><p>您也可以使用路由规则在流量上执行一些操作，例如</p><ul><li>扩展或者删除 <code>headers</code></li><li>重写 URL</li><li>为调用这个目标地址设置重试策略</li></ul><h3 id="DestinationRule"><a href="#DestinationRule" class="headerlink" title="DestinationRule"></a>DestinationRule</h3><p><code>DestinationRule</code> 是 Istio 流量路由功能的重要组成部分。一个 <code>VirtualService</code> 可以看作是如何将流量分发到给定的目标地址，然后调用 <code>DestinationRule</code> 来配置分发到该目标地址的流量。<code>DestinationRule</code> 在 <code>VirtualService</code> 的路由规则之后起作用(即在 <code>VirtualService</code> 的 <code>match</code> -&gt; <code>route</code> -&gt; <code>destination</code> 之后起作用，此时流量已经分发到真实的 <code>Service</code> 上)，应用于真实的目标地址。</p><p>特别地，可以使用 <code>DestinationRule</code> 来指定命名的服务子集，例如根据版本对服务的实例进行分组，然后通过 <code>VirtualService</code> 的路由规则中的服务子集将控制流量分发到不同服务的实例中。</p><p><code>DestinationRule</code> 允许在调用完整的目标服务或特定的服务子集(如倾向使用的负载均衡模型，TLS 安全模型或断路器)时自定义 Envoy流量策略。Istio 默认会使用轮询策略，此外 Istio 也支持如下负载均衡模型，可以在 <code>DestinationRule</code> 中使用这些模型，将请求分发到特定的服务或服务子集。</p><ul><li>Random：将请求转发到一个随机的实例上</li><li>Weighted：按照指定的百分比将请求转发到实例上</li><li>Least requests：将请求转发到具有最少请求数目的实例上</li></ul><p>下面的 <code>DestinationRule</code> 使用不同的负载均衡策略为 my-svc 目的服务配置了3个不同的 Subset</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-destination-rule</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">my-svc</span></span><br><span class="line">  <span class="attr">trafficPolicy:</span>     <span class="comment">#默认的负载均衡策略模型为随机</span></span><br><span class="line">    <span class="attr">loadBalancer:</span></span><br><span class="line">      <span class="attr">simple:</span> <span class="string">RANDOM</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v1</span>  <span class="comment">#subset1，将流量转发到具有标签 version:v1 的 deployment 对应的服务上</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v2</span>  <span class="comment">#subset2，将流量转发到具有标签 version:v2 的 deployment 对应的服务上,指定负载均衡为轮询</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">trafficPolicy:</span></span><br><span class="line">      <span class="attr">loadBalancer:</span></span><br><span class="line">        <span class="attr">simple:</span> <span class="string">ROUND_ROBIN</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v3</span>   <span class="comment">#subset3，将流量转发到具有标签 version:v3 的 deployment 对应的服务上</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v3</span></span><br></pre></td></tr></table></figure><p>每个子集由一个或多个 <code>labels</code> 定义，对应 Kubernetes 中的对象(如 <code>Pod</code> )的 key/value 对。这些标签定义在 Kubernetes 服务的 deployment 的 metadata 中，用于标识不同的版本。</p><p>除了定义子集外，<code>DestinationRule</code> 还定义了该目的地中所有子集的默认流量策略，以及仅覆盖该子集的特定策略。默认的策略定义在 <code>subset</code> 字段之上，为 <code>v1</code> 和 <code>v3</code> 子集设置了随机负载均衡策略，在 <code>v2</code> 策略中使用了轮询负载均衡。</p><h3 id="Gateway"><a href="#Gateway" class="headerlink" title="Gateway"></a>Gateway</h3><p>Gateway 用于管理进出网格的流量，指定可以进入或离开网格的流量。Gateway 配置应用于网格边缘的独立的 Envoy代理上，而不是服务负载的 Envoy 代理上。</p><p>与其他控制进入系统的流量的机制(如 Kubernetes Ingress API)不同，Istio gateway 允许利用 Istio 的流量路由的强大功能和灵活性。Istio 的 gateway 资源仅允许配置 4-6 层的负载属性，如暴露的端口，TLS 配置等等，但结合 Istio 的 <code>VirtualService</code>，就可以像管理 Istio 网格中的其他数据面流量一样管理 Gateway 的流量。</p><p>Gateway 主要用于管理 Ingress 流量，但也可以配置 Egress Gateway。通过 Egress Gateway 可以配置流量离开网格的特定节点，限制哪些服务可以访问外部网络，或通过 Egress 安全控制来提高网格的安全性。Gateway 可以用于配置为一个纯粹的内部代理。</p><p>Istio (通过 <code>istio-ingressgateway</code> 和 <code>istio-egressgateway</code> 参数)提供了一些预配置的 Gateway 代理，<code>default</code> profile 下仅会部署 Ingress Gateway。Gateway 可以通过部署文件进行部署，也可以单独部署。</p><p>下面是 <code>default</code> profile 默认安装的 Ingress</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get gw</span><br><span class="line">NAME               AGE</span><br><span class="line">bookinfo-gateway   28h</span><br></pre></td></tr></table></figure><p>可以看到该 ingress 就是一个普通的 <code>Pod</code>，该 <code>Pod</code> 仅包含一个 Istio-proxy 容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n istio-system |grep ingress</span><br><span class="line">istio-ingressgateway-64f6f9d5c6-qrnw2 1/1 Running 0 4d20h</span><br></pre></td></tr></table></figure><p>下面是一个 Gateway 的例子，用于配置外部 HTTPS 的 ingress 流量：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Gateway</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ext-host-gwy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span>              <span class="comment">#指定 gateway 配置下发的代理，如具有标签 app: my-gateway-controller 的 pod</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">my-gateway-controller</span></span><br><span class="line">  <span class="attr">servers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span>                <span class="comment">#gateway pod 暴露的端口信息</span></span><br><span class="line">      <span class="attr">number:</span> <span class="number">443</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">HTTPS</span></span><br><span class="line">    <span class="attr">hosts:</span>                <span class="comment">#外部流量</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ext-host.example.com</span></span><br><span class="line">    <span class="attr">tls:</span></span><br><span class="line">      <span class="attr">mode:</span> <span class="string">SIMPLE</span></span><br><span class="line">      <span class="attr">serverCertificate:</span> <span class="string">/tmp/tls.crt</span></span><br><span class="line">      <span class="attr">privateKey:</span> <span class="string">/tmp/tls.key</span></span><br></pre></td></tr></table></figure><p>上述 Gateway 配置允许来自 <code>ext-host.example.com</code> 流量进入网格的 443 端口，但没有指定该流量的路由。(此时流量只能进入网格，但没有指定处理该流量的服务，因此需要与 <code>VirtualService</code> 进行绑定)</p><p>为了为 Gateway 指定路由，需要通过 <code>VirtualService</code> 的 <code>Gateway</code> 字段，将 <code>Gateway</code> 绑定到一个 <code>VirtualService</code> 上，将来自 <code>ext-host.example.com</code> 流量引入一个 <code>VirtualService</code>，<code>hosts</code> 可以是通配符，表示引入匹配到的流量。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">virtual-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ext-host.example.com</span></span><br><span class="line">  <span class="attr">gateways:</span>        <span class="comment">#将 gateway "ext-host-gwy" 绑定到 virtual service "virtual-svc"上</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ext-host-gwy</span></span><br></pre></td></tr></table></figure><p>Egress Gateway 提供了对网格的出口流量进行统一管控的功能，在安装 Istio 时默认是不开启的。可以使用以下命令查看是否开启。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod -l istio=egressgateway -n istio-system</span></span><br></pre></td></tr></table></figure><p>若没有开启，使用以下命令添加。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ istioctl manifest apply --<span class="built_in">set</span> values.global.istioNamespace=istio-system \</span><br><span class="line">    --<span class="built_in">set</span> values.gateways.istio-egressgateway.enabled=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>Egress Gateway 的一个简单示例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Gateway</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">istio-egressgateway</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">istio:</span> <span class="string">egressgateway</span></span><br><span class="line">  <span class="attr">servers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span></span><br><span class="line">      <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">HTTP</span></span><br><span class="line">    <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">edition.cnn.com</span></span><br></pre></td></tr></table></figure><p>可以看出，与 Ingress Gateway 不同，Egress Gateway 使用有 <code>istio: egressgateway</code> 标签的 Pod 来代理流量，实际上这也是一个 Envoy 代理。当网格内部需要访问 <code>edition.cnn.com</code> 这个地址时，流量将会统一先转发到 Egress Gateway 上，再由 Egress Gateway 将流量转发到 <code>edition.cnn.com</code> 上。</p><h3 id="ServiceEntry"><a href="#ServiceEntry" class="headerlink" title="ServiceEntry"></a>ServiceEntry</h3><p>Istio 支持对接 Kubernetes、Consul 等多种不同的注册中心，控制平面<code>Pilot</code>启动时，会从指定的注册中心获取 <code>Service Mesh</code> 集群的服务信息和实例列表，并将这些信息进行处理和转换，然后通过 xDS 下发给对应的数据平面，保证服务之间可以互相发现并正常访问。</p><p>同时，由于这些服务和实例信息都来源于服务网格内部，Istio 无法从注册中心直接获取网格外的服务，导致不利于网格内部与外部服务之间的通信和流量管理。为此，Istio 引入 ServiceEntry 实现对外通信和管理。</p><p>使用 ServiceEntry 可以将外部的服务条目添加到 Istio 内部的服务注册表中，以便让网格中的服务能够访问并路由到这些手动指定的服务。ServiceEntry 描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可能是位于网格外部（如，web APIs），也可能是处于网格内部但不属于平台服务注册表中的条目（如，需要和 Kubernetes 服务交互的一组虚拟机服务）。</p><h4 id="ServiceEntry-示例和属性介绍"><a href="#ServiceEntry-示例和属性介绍" class="headerlink" title="ServiceEntry 示例和属性介绍"></a>ServiceEntry 示例和属性介绍</h4><p>对于网格外部的服务，下面的 ServiceEntry 示例表示网格内部的应用通过 https 访问外部的 API。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">google</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">www.google.com</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">HTTPS</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">DNS</span></span><br><span class="line">  <span class="attr">location:</span> <span class="string">MESH_EXTERNAL</span></span><br></pre></td></tr></table></figure><p>对于在网格内部但不属于平台服务注册表的服务，使用下面的示例可以将一组在非托管 VM 上运行的 MongoDB 实例添加到 Istio 的注册中心，以便可以将这些服务视为网格中的任何其他服务。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-svc-mongocluster</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">mymongodb.somedomain</span></span><br><span class="line">  <span class="attr">addresses:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">192.192</span><span class="number">.192</span><span class="number">.192</span><span class="string">/24</span> <span class="comment"># VIPs</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">27018</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mongodb</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">MONGO</span></span><br><span class="line">  <span class="attr">location:</span> <span class="string">MESH_INTERNAL</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">STATIC</span></span><br><span class="line">  <span class="attr">endpoints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">2.2</span><span class="number">.2</span><span class="number">.2</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">address:</span> <span class="number">3.3</span><span class="number">.3</span><span class="number">.3</span></span><br></pre></td></tr></table></figure><p>结合上面给出的示例，这里对 ServiceEntry 涉及的关键属性解释如下：</p><ul><li><code>hosts</code>: 表示与该 ServiceEntry 相关的主机名，可以是带有通配符前缀的 DNS 名称。</li><li><code>address</code>: 与服务相关的虚拟 IP 地址，可以是 CIDR 前缀的形式。</li><li><code>ports</code>: 和外部服务相关的端口，如果外部服务的 endpoints 是 Unix socket 地址，这里必须只有一个端口。</li><li><code>location</code>: 用于指定该服务属于网格内部（MESH_INTERNAL）还是外部（MESH_EXTERNAL）。</li><li><code>resolution</code>: 主机的服务发现模式，可以是 NONE、STATIC、DNS。</li><li><code>endpoints</code>: 与服务相关的一个或多个端点。</li><li><code>exportTo</code>: 用于控制 ServiceEntry 跨命名空间的可见性，这样就可以控制在一个命名空间下定义的资源对象是否可以被其他命名空间下的 <code>Sidecar</code>、Gateway 和 VirtualService 使用。目前支持两种选项，”.” 表示仅应用到当前命名空间，”*” 表示应用到所有命名空间。</li></ul><h4 id="使用-ServiceEntry-访问外部服务"><a href="#使用-ServiceEntry-访问外部服务" class="headerlink" title="使用 ServiceEntry 访问外部服务"></a>使用 ServiceEntry 访问外部服务</h4><p>Istio 提供了三种访问外部服务的方法：</p><ol><li>允许 <code>Sidecar</code> 将请求传递到未在网格内配置过的任何外部服务。使用这种方法时，无法监控对外部服务的访问，也不能利用 Istio 的流量控制功能。</li><li>配置 ServiceEntry 以提供对外部服务的受控访问。这是 Istio 官方推荐使用的方法。</li><li>对于特定范围的 IP，完全绕过 <code>Sidecar</code>。仅当出于性能或其他原因无法使用 <code>Sidecar</code> 配置外部访问时，才建议使用该配置方法。</li></ol><p>这里，我们重点讨论第 2 种方式，也就是使用 ServiceEntry 完成对网格外部服务的受控访问。</p><p>对于 <code>Sidecar</code> 对外部服务的处理方式，Istio 提供了两种选项:</p><ul><li><code>ALLOW_ANY</code>：默认值，表示 Istio 代理允许调用未知的外部服务。上面的第一种方法就使用了该配置项。</li><li><code>REGISTRY_ONLY</code>：Istio 代理会阻止任何没有在网格中定义的 HTTP 服务或 ServiceEntry 的主机。</li></ul><p>可以使用下面的命令查看当前所使用的模式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get configmap istio -n istio-system -o yaml | grep -o <span class="string">"mode: ALLOW_ANY"</span></span><br><span class="line">mode: ALLOW_ANY</span><br></pre></td></tr></table></figure><p>如果当前使用的是 <code>ALLOW_ANY</code> 模式，可以使用下面的命令切换为 <code>REGISTRY_ONLY</code> 模式:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get configmap istio -n istio-system -o yaml | sed <span class="string">'s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g'</span> | kubectl replace -n istio-system -f -</span><br><span class="line">configmap <span class="string">"istio"</span> replaced</span><br></pre></td></tr></table></figure><p>在 <code>REGISTRY_ONLY</code> 模式下，需要使用 ServiceEntry 才能完成对外部服务的访问。当创建如下的 ServiceEntry 时，服务网格内部的应用就可以正常访问 httpbin.org 服务了。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceEntry</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">httpbin-ext</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">httpbin.org</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">HTTP</span></span><br><span class="line">  <span class="attr">resolution:</span> <span class="string">DNS</span></span><br><span class="line">  <span class="attr">location:</span> <span class="string">MESH_EXTERNAL</span></span><br></pre></td></tr></table></figure><h4 id="管理外部流量"><a href="#管理外部流量" class="headerlink" title="管理外部流量"></a>管理外部流量</h4><p>使用 ServiceEntry 可以使网格内部服务发现并访问外部服务，除此之外，还可以对这些到外部服务的流量进行管理。结合 VirtualService 为对应的 ServiceEntry 配置外部服务访问规则，如请求超时、故障注入等，实现对指定服务的受控访问。</p><p>下面的示例就是为外部服务 httpbin.org 设置了超时时间，当请求时间超过 3s 时，请求就会直接中断，避免因外部服务访问时延过高而影响内部服务的正常运行。由于外部服务的稳定性通常无法管控和监测，这种超时机制对内部服务的正常运行具有重要意义。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">httpbin-ext</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">httpbin.org</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">timeout:</span> <span class="string">3s</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">          <span class="attr">host:</span> <span class="string">httpbin.org</span></span><br><span class="line">        <span class="attr">weight:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><p>同样的，我们也可以为 ServiceEntry 设置故障注入规则，为系统测试提供基础。下面的示例表示为所有访问 <code>httpbin.org</code> 服务的请求注入一个403错误。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"> <span class="attr">name:</span> <span class="string">httpbin-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">hosts:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="string">httpbin.org</span></span><br><span class="line"> <span class="attr">http:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">   <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">       <span class="attr">host:</span> <span class="string">httpbin.org</span></span><br><span class="line">   <span class="attr">fault:</span></span><br><span class="line">     <span class="attr">abort:</span></span><br><span class="line">       <span class="attr">percent:</span> <span class="number">100</span></span><br><span class="line">       <span class="attr">httpStatus:</span> <span class="number">403</span></span><br></pre></td></tr></table></figure><h3 id="Sidecar"><a href="#Sidecar" class="headerlink" title="Sidecar"></a>Sidecar</h3><p>在默认的情况下，Istio 中所有 Pod 中的 Envoy 代理都是可以被寻址的。然而在某些场景下，我们为了做资源隔离，希望只访问某些 Namespace 下的资源。这个时候，我们就可以使用 Sidecar配置来实现。下面是一个简单的示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Sidecar</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">bookinfo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">hosts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"./*"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"istio-system/*"</span></span><br></pre></td></tr></table></figure><p>该示例就规定了在命名空间为 bookinfo 下的所有服务仅可以访问本命名空间下的服务以及 <code>istio-system</code> 命名空间下的服务。</p><h2 id="弹性功能"><a href="#弹性功能" class="headerlink" title="弹性功能"></a>弹性功能</h2><p>除了最核心的路由和流量转移功能外，Istio 还提供了一定的弹性功能，目前支持超时、重试和熔断。</p><h3 id="Request-Timeouts"><a href="#Request-Timeouts" class="headerlink" title="Request Timeouts"></a>Request Timeouts</h3><p>如果程序请求长时间无法返回结果，则需要设置超时机制，超过设置的时间则返回错误信息。这样做既可以节约等待时消耗的资源，也可以避免由于级联错误引起的一系列问题。</p><p>设置超时的方式也有很多种，比如通过修改代码在应用程序侧设置请求超时时间，但是这样很不灵活，也容易出现遗漏的现象，而 Istio 则可以在基础设施层解决这一问题。在 Istio 里添加超时非常简单，只需要在路由配置里添加 <code>timeout</code> 这个关键字就可以实现。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ratings</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ratings</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">ratings</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">timeout:</span> <span class="string">10s</span></span><br></pre></td></tr></table></figure><h3 id="Retries"><a href="#Retries" class="headerlink" title="Retries"></a>Retries</h3><p>在网络环境不稳定的情况下，会出现暂时的网络不可达现象，这时需要重试机制，通过多次尝试来获取正确的返回信息。重试逻辑可以写业务代码中，比如 Bookinfo 应用中的<code>productpage</code>服务就存在硬编码重试，而 Istio 可以通过简单的配置来实现重试功能，让开发人员无需关注重试部分的代码实现，专心实现业务代码。在 Istio 里添加超时和重试都非常简单，只需要在路由配置里添 <code>retry</code> 这个关键字就可以实现。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ratings</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ratings</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">ratings</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">retries:</span></span><br><span class="line">      <span class="attr">attempts:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">perTryTimeout:</span> <span class="string">2s</span></span><br></pre></td></tr></table></figure><h3 id="Circuit-Breaking"><a href="#Circuit-Breaking" class="headerlink" title="Circuit Breaking"></a>Circuit Breaking</h3><p>熔断是一种非常有用的过载保护手段，可以避免服务的级联失败。在熔断器中，设置一个对服务中的单个主机调用的限制，例如并发连接的数量或对该主机调用失败的次数。一旦限制被触发，熔断器就会“跳闸”并停止连接到该主机。使用熔断模式可以快速失败而不必让客户端尝试连接到过载或有故障的主机。熔断适用于在负载均衡池中的“真实”网格目标地址，可以在 DestinationRule 中配置熔断器阈值，让配置适用于服务中的每个主机。</p><p>Istio 里面的熔断需要在自定义资源 <code>DestinationRule</code> 的 <code>TrafficPolicy</code> 里进行设置。下面的示例将 v1 子集的<code>reviews</code>服务工作负载的并发连接数限制为 100：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">reviews</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">reviews</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">trafficPolicy:</span></span><br><span class="line">      <span class="attr">connectionPool:</span></span><br><span class="line">        <span class="attr">tcp:</span></span><br><span class="line">          <span class="attr">maxConnections:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="调试能力"><a href="#调试能力" class="headerlink" title="调试能力"></a>调试能力</h2><p>Istio 还提供了对流量进行调试的能力，包括故障注入和流量镜像。对流量进行调试可以让系统具有更好的容错能力，也方便我们在问题排查时通过调试来快速定位原因所在。</p><h3 id="Fault-Injection"><a href="#Fault-Injection" class="headerlink" title="Fault Injection"></a>Fault Injection</h3><p>在一个微服务架构的系统中，为了让系统达到较高的健壮性要求，通常需要对系统做定向错误测试。比如电商中的订单系统、支付系统等若出现故障那将是非常严重的生产事故，因此必须在系统设计前期就需要考虑多样性的异常故障并对每一种异常设计完善的恢复策略或优雅的回退策略，尽全力规避类似事故的发生，使得当系统发生故障时依然可以正常运作。而在这个过程中，服务故障模拟一直以来是一个非常繁杂的工作，于是在这样的背景下就衍生出了故障注入技术手段，故障注入是用来模拟上游服务请求响应异常行为的一种手段。通过人为模拟上游服务请求的一些故障信息来检测下游服务的故障策略是否能够承受这些故障并进行自我恢复。</p><p>Istio 提供了一种无侵入式的故障注入机制，让开发测试人员在不用调整服务程序的前提下，通过配置即可完成对服务的异常模拟。Istio 1.5 仅支持网络层的故障模拟，即支持模拟上游服务的处理时长、服务异常状态、自定义响应状态码等故障信息，暂不支持对于服务主机内存、CPU 等信息故障的模拟。他们都是通过配置上游主机的 VirtualService 来实现的。当我们在 VirtualService 中配置了故障注入时，上游服务的 Envoy代理在拦截到请求之后就会做出相应的响应。</p><p>目前，Istio 提供两种类型的故障注入，abort 类型与 delay 类型。</p><ul><li><strong>abort</strong>：非必配项，配置一个 Abort 类型的对象。用来注入请求异常类故障。简单的说，就是用来模拟上游服务对请求返回指定异常码时，当前的服务是否具备处理能力。它对应于 Envoy过滤器中的 <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/filter/http/fault/v2/fault.proto#envoy-api-msg-config-filter-http-fault-v2-faultabort" target="_blank" rel="external nofollow noopener noreferrer">config.filter.http.fault.v2.FaultAbort</a> 配置项，当 VirtualService 资源应用时，Envoy将会该配置加载到过滤器中并处理接收到的流量。</li><li><strong>delay</strong>：非必配项，配置一个 Delay 类型的对象。用来注入延时类故障。通俗一点讲，就是人为模拟上游服务的响应时间，测试在高延迟的情况下，当前的服务是否具备容错容灾的能力。它对应于 Envoy过滤器中的 <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/filter/fault/v2/fault.proto#envoy-api-msg-config-filter-fault-v2-faultdelay" target="_blank" rel="external nofollow noopener noreferrer">config.filter.fault.v2.FaultDelay</a> 配置型，同样也是在应用 Istio 的 VirtualService 资源时，Envoy将该配置加入到过滤器中。</li></ul><p>实际上，Istio 的故障注入正是基于 Envoy的 config.filter.http.fault.v2.HTTPFault 过滤器实现的，它的局限性也来自于 Envoy故障注入机制的局限性。对于 Envoy的 HttpFault 的详细介绍请参考 <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v2/config/filter/http/fault/v2/fault.proto#envoy-api-msg-config-filter-http-fault-v2-httpfault" target="_blank" rel="external nofollow noopener noreferrer">Envoy 文档</a>。对比 Istio 故障注入的配置项与 Envoy故障注入的配置项，不难发现，Istio 简化了对于故障控制的手段，去掉了 Envoy中通过 HTTP header 控制故障注入的配置。</p><h4 id="HTTPFaultInjection-Abort"><a href="#HTTPFaultInjection-Abort" class="headerlink" title="HTTPFaultInjection.Abort"></a>HTTPFaultInjection.Abort</h4><ul><li><strong>httpStatus</strong>：必配项，是一个整型的值。表示注入 HTTP 请求的故障状态码。</li><li><strong>percentage</strong>：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。</li><li><strong>percent</strong>：已经废弃的一个配置，与 percentage 配置功能一样，已经被 percentage 代替。</li></ul><p>如下的配置表示对 <code>v1</code> 版本的 <code>ratings.prod.svc.cluster.local</code> 服务访问的时候进行故障注入，<code>0.1</code>表示有千分之一的请求被注入故障， <code>400</code> 表示故障为该请求的 HTTP 响应码为 <code>400</code> 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ratings-route</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ratings.prod.svc.cluster.local</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">ratings.prod.svc.cluster.local</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">fault:</span></span><br><span class="line">      <span class="attr">abort:</span></span><br><span class="line">        <span class="attr">percentage:</span></span><br><span class="line">          <span class="attr">value:</span> <span class="number">0.1</span></span><br><span class="line">        <span class="attr">httpStatus:</span> <span class="number">400</span></span><br></pre></td></tr></table></figure><h4 id="HTTPFaultInjection-Delay"><a href="#HTTPFaultInjection-Delay" class="headerlink" title="HTTPFaultInjection.Delay"></a>HTTPFaultInjection.Delay</h4><ul><li><strong>fixedDelay</strong>：必配项，表示请求响应的模拟处理时间。格式为：<code>1h/1m/1s/1ms</code>， 不能小于 <code>1ms</code>。</li><li><strong>percentage</strong>：非必配项，是一个 Percent 类型的值。表示对多少请求进行故障注入。如果不指定该配置，那么所有请求都将会被注入故障。</li><li><strong>percent</strong>：已经废弃的一个配置，与 <code>percentage</code> 配置功能一样，已经被 <code>percentage</code> 代替。</li></ul><p>如下的配置表示对 <code>v1</code> 版本的 <code>reviews.prod.svc.cluster.local</code> 服务访问的时候进行延时故障注入，<code>0.1</code> 表示有千分之一的请求被注入故障，<code>5s</code> 表示<code>reviews.prod.svc.cluster.local</code> 延时 <code>5s</code>返回。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">reviews-route</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">reviews.prod.svc.cluster.local</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">sourceLabels:</span></span><br><span class="line">        <span class="attr">env:</span> <span class="string">prod</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">reviews.prod.svc.cluster.local</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">fault:</span></span><br><span class="line">      <span class="attr">delay:</span></span><br><span class="line">        <span class="attr">percentage:</span></span><br><span class="line">          <span class="attr">value:</span> <span class="number">0.1</span></span><br><span class="line">        <span class="attr">fixedDelay:</span> <span class="string">5s</span></span><br></pre></td></tr></table></figure><h3 id="Mirroring"><a href="#Mirroring" class="headerlink" title="Mirroring"></a>Mirroring</h3><p>流量镜像（Mirroring / traffic-shadow），也叫作影子流量，就是通过复制一份请求并把它发送到镜像服务，从而实现流量的复制功能。流量镜像的主要应用场景有以下几种：最主要的就是进行<strong>线上问题排查</strong>。</p><p>一般情况下，因为系统环境，特别是数据环境、用户使用习惯等问题，我们很难在开发环境中模拟出真实的生产环境中出现的棘手问题，同时生产环境也不能记录太过详细的日志，因此很难定位到问题。有了流量镜像，我们就可以把真实的请求发送到镜像服务，再打开 debug 日志来查看详细的信息。除此以外，还可以通过它来观察生产环境的请求处理能力，比如在镜像服务进行压力测试。也可以将复制的请求信息用于数据分析。流量镜像在 Istio 里实现起来也非常简单，只需要在路由配置中通添加<code>mirror</code>关键字即可。</p><h4 id="流量镜像能够为我们带来什么"><a href="#流量镜像能够为我们带来什么" class="headerlink" title="流量镜像能够为我们带来什么"></a>流量镜像能够为我们带来什么</h4><p>很多情况下，当我们对服务做了重构，或者我们对项目做了重大优化时，怎么样保证服务是健壮的呢？在传统的服务里，我们只能通过大量的测试，模拟在各种情况下服务的响应情况。虽然也有手工测试、自动化测试、压力测试等一系列手段去检测它，但是测试本身就是一个样本化的行为，即使测试人员再完善它的测试样例，无法全面的表现出线上服务的一个真实流量形态。往往当项目发布之后，总会出现一些意外，比如你服务里收到客户使用的某些数据库不认识的特殊符号，再比如用户在本该输入日期的输入框中输入了 “—” 字样的字符，又比如用户使用乱码替换你的 token 值批量恶意攻击服务等等，这样的情况屡见不鲜。数据的多样性，复杂性决定了开发人员在开发阶段根本是无法考虑周全的。</p><p>而流量镜像的设计，让这类问题得到了最大限度的解决。流量镜像讲究的不再是使用少量样本去评估一个服务的健壮性，而是在不影响线上坏境的前提下将线上流量持续的镜像到我们的预发布坏境中去，让重构后的服务在上线之前就结结实实地接受一波真实流量的冲击与考验，让所有的风险全部暴露在上线前夕，通过不断的暴露问题，解决问题让服务在上线前夕就拥有跟线上服务一样的健壮性。由于测试坏境使用的是真实流量，所以不管从流量的多样性，真实性，还是复杂性上都将能够得以展现，同时预发布服务也将表现出其最真实的处理能力和对异常的处理能力。运用这种模式，一方面，我们将不会再跟以前一样在发布服务前夕内心始终忐忑不安，只能祈祷上线之后不会出现问题。另一方面，当大量的流量流入重构服务之后，开发过程中难以评估的性能问题也将完完整整的暴露出来，此时开发人员将会考虑它服务的性能，测试人员将会更加完善他们的测试样例。通过暴露问题，解决问题，再暴露问题，再解决问题的方式循序渐进地完善预发布服务来增加我们上线的成功率。同时也变相的促进我们开发测试人员技能水平的提高。</p><p>当然，流量镜像的作用不仅仅只是解决上面这样的场景问题，我们可以根据它的特性，解决更多的问题。比如，假如我们在上线后突然发现一个线上问题，而这个问题在测试坏境中始终不能复现。那么这个时候我们就能利用它将异常流量镜像到一个分支服务中去，然后我们可以随意在这个分支服务上进行分析调试，这里所说的分支服务，可以是原服务的只用于问题分析而不处理正式业务的副本服务，也可以是一个只收集镜像流量的组件类服务。又比如突然需要收集某个时间段某些流量的特征数据做分析，像这种临时性的需求，使用流量镜像来处理非常合适，既不影响线上服务的正常运转，也达到了收集分析的目的。</p><h4 id="流量镜像的实现原理"><a href="#流量镜像的实现原理" class="headerlink" title="流量镜像的实现原理"></a>流量镜像的实现原理</h4><p>实际上在 Istio 中，服务间的通讯都是被 Envoy代理拦截并处理的， Istio 流量镜像的设计也是基于 Envoy特性实现的。它的流量转发如下图所示。可以看到，当流量进入到<code>Service A</code>时，因为在<code>Service A</code>的 Envoy代理上配置了流量镜像规则，那么它首先会将原始流量转发到<code>v1</code>版本的 <code>Service B</code>服务子集中去 。同时也会将相同的流量复制一份，异步地发送给<code>v2</code>版本的<code>Service B</code> 服务子集中去，可以明显的看到，<code>Service A</code> 发送完镜像流量之后并不关心它的响应情况。</p><p>在很多情况下，我们需要将真实的流量数据与镜像流量数据进行收集并分析，那么当我们收集完成后应该怎样区分哪些是真实流量，哪些是镜像流量呢？ 实际上，Envoy团队早就考虑到了这样的场景，他们为了区分镜像流量与真实流量，在镜像流量中修改了请求标头中 <code>host</code> 值来标识，它的修改规则是：在原始流量请求标头中的 <code>host</code> 属性值拼接上<code>“-shadow”</code> 字样作为镜像流量的 <code>host</code> 请求标头。</p><p>为了能够更清晰的对比出原始流量与镜像流量的区别，我们使用以下的一个示例来说明：</p><p>如下图所示，我们发起一个<code>http://istio.gateway.xxxx.tech/serviceB/request/info</code>的请求，请求首先进入了<code>istio-ingressgateway</code> ，它是一个 Istio 的 Gateway 资源类型的服务，它本身就是一个 Envoy代理。在这个例子里，就是它对流量进行了镜像处理。可以看到，它将流量转发给<code>v1</code>版本<code>Service B</code>服务子集的同时也复制了一份流量发送到了<code>v2</code>版本的<code>Service B</code>服务子集中去。</p><p><img alt="concepts-traffic-shadow-request" data-src="https://www.servicemesher.com/istio-handbook/images/concepts-traffic-shadow-request.png"></p><p>在上面的请求链中，请求标头数据有什么变化呢？下图收集了它们请求标头中的所有信息，可以明显的对比出正式流量与镜像流量请求标头中<code>host</code>属性的区别（部分相同的属性值过长，这里只截取了前半段）。从图中我们可以看出，首先就是host属性值的不同，而区别就是多了一个<code>“-shadow”</code>的后缀。再者发现<code>x-forwarded-for</code>属性也不相同，<code>x-forwarded-for</code>协议头的格式是：<code>x-forwarded-for: client1, proxy1, proxy2</code>， 当流量经过 Envoy代理时这个协议头将会把代理服务的 IP 添加进去。实例中<code>10.10.2.151</code>是我们云主机的 IP，而<code>10.10.2.121</code>是<code>isito-ingressgateway</code>所对应<code>Pod</code>的 IP 。从这里也能看到，镜像流量是由<code>istio-ingressgatway</code>发起的。除了这两个请求标头的不同，其他配置项是完全一样的。</p><p><img alt="concepts-traffic-shadow-header" data-src="https://www.servicemesher.com/istio-handbook/images/concepts-traffic-shadow-header.png"></p><h4 id="流量镜像的配置"><a href="#流量镜像的配置" class="headerlink" title="流量镜像的配置"></a>流量镜像的配置</h4><p>上面我们介绍了流量镜像的原理及使用场景，接下来我们再介绍下流量的镜像如何配置才能生效。在 Istio 架构里，镜像流量是借助于 VirtualService 这个资源中的 <code>HTTPRoute</code> 配置项的<code>mirror</code>与<code>mirrorPercent</code>这两项子配置项来实现的，这两个配置项的定义也是非常的简单。</p><ul><li><strong>mirror</strong>：配置一个 Destination 类型的对象，这里就是我们镜像流量转发的服务地址。具体的 <strong>VirtualService</strong> 配置与<strong>DestinationRule</strong> 对象配置属性请参考相关介绍页。</li><li><strong>mirrorPercent</strong>：配置一个数值，这个配置项用来指定有多少的原始流量将被转发到镜像流量服务中去，它的有效值为<code>0~100</code>，如果配置成<code>0</code>则表示不发送镜像流量。</li></ul><p>下面的例子就是我们在示例中使用到的<code>Service B</code>的镜像流量配置，其中，<code>mirror.host</code>配置项是配置一个域名或者在Istio 注册表中注册过的服务名称，可以看到，该配置指定了镜像流量需要发送的目标服务地址为<code>serviceB</code>。<code>mirror.subset</code>配置项配置一个<code>Service B</code>服务的服务子集名称 ，指定了要将镜像流量镜像到<code>v2</code>版本的<code>Service B</code>服务子集中去。<code>mirror_percent</code>配置将<code>100%</code>的真实流量进行镜像发送。所以下面的配置整体表示当流量到来时，将请求转发到<code>v1</code>版本的<code>service B</code>服务子集中，再以镜像的方式发送到<code>v2</code>版本的<code>service B</code>服务上一份，并将真实流量全部镜像。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VirtualService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serviceB</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hosts:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">istio.gateway.xxxx.tech</span></span><br><span class="line">  <span class="attr">gateways:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ingressgateway.istio-system.svc.cluster.local</span></span><br><span class="line">  <span class="attr">http:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">match:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">uri:</span></span><br><span class="line">        <span class="attr">prefix:</span> <span class="string">/serviceB</span></span><br><span class="line">    <span class="attr">rewrite:</span></span><br><span class="line">      <span class="attr">uri:</span> <span class="string">/</span></span><br><span class="line">    <span class="attr">route:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">destination:</span></span><br><span class="line">        <span class="attr">host:</span> <span class="string">serviceB</span></span><br><span class="line">        <span class="attr">subset:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">mirror:</span></span><br><span class="line">      <span class="attr">host:</span> <span class="string">serviceB</span></span><br><span class="line">      <span class="attr">subset:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">mirror_percent:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><p><code>service B</code> 服务对应的 DestinationRule 配置如下 ：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.istio.io/v1alpha3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DestinationRule</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serviceB</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">serviceB</span></span><br><span class="line">  <span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v2</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v2</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">v1</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">v1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;流量控制是指对系统流量的管控，包括了对网格入口的流量、网格出口的流量以及在网格内部微服务间相互调用流量的控制。在 &lt;a href=&quot;../22cae0b8&quot;&gt;Istio 入门&lt;/a&gt; 中我们知道，Istio 架构在逻辑上分为 Control plane 和 Data plane，Control plane 负责整体管理和配置代理， Data plane 负责网格内所有微服务间的网络通信，同时还收集报告网络请求的遥测数据等。流量控制是在 Data plane 层实现。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-bookinfo.svg" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="网络" scheme="http://houmin.cc/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="service mesh" scheme="http://houmin.cc/tags/service-mesh/"/>
    
      <category term="envoy" scheme="http://houmin.cc/tags/envoy/"/>
    
      <category term="istio" scheme="http://houmin.cc/tags/istio/"/>
    
  </entry>
  
  <entry>
    <title>【Service Mesh】Istio 入门</title>
    <link href="http://houmin.cc/posts/22cae0b8/"/>
    <id>http://houmin.cc/posts/22cae0b8/</id>
    <published>2020-11-23T02:44:08.000Z</published>
    <updated>2020-11-27T02:46:29.502Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Istio 是一个完全开源的服务网格，以透明的方式构建在现有的分布式应用中。它也是一个平台，拥有可以集成任何日志、遥测和策略系统的 API 接口。Istio 多样化的特性使你能够成功且高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。</p><a id="more"></a><h2 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h2><h3 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h3><p>微服务应用最大的痛点就是处理服务间的通信，而这一问题的核心其实就是流量管理。首先我们来看看传统的微服务应用在没有 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#service-mesh" target="_blank" rel="external nofollow noopener noreferrer">Service Mesh</a> 介入的情况下，是如何完成诸如金丝雀发布这样的路由功能的。我们假设不借助任何现成的第三方框架，一个最简单的实现方法，就是在服务间添加一个负载均衡（比如 Nginx）做代理，通过修改配置的权重来分配流量。这种方式使得对流量的管理和基础设施绑定在了一起，难以维护。</p><p>而使用 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 就可以轻松的实现各种维度的流量控制。下图是典型的金丝雀发布策略：根据权重把 5% 的流量路由给新版本，如果服务正常，再逐渐转移更多的流量到新版本。</p><p><a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 中的流量控制功能主要分为三个方面：</p><ul><li>请求路由和流量转移</li><li>弹性功能，包括熔断、超时、重试</li><li>调试能力，包括故障注入和流量镜像</li></ul><p>关于流量控制的更多内容，参考 <a href="../151719f0">Istio流量控制</a></p><h3 id="安全管理"><a href="#安全管理" class="headerlink" title="安全管理"></a>安全管理</h3><p>安全对于微服务这样的分布式系统来说至关重要。与单体应用在进程内进行通信不同，网络成为了服务间通信的纽带，这使得它对安全有了更迫切的需求。比如为了抵御外来攻击，我们需要对流量进行加密；为保证服务间通信的可靠性，需要使用mTLS的方式进行交互；为控制不同身份的访问，需要设置不同粒度的授权策略。作为一个服务网格，<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 提供了一整套完整的安全解决方案。它可以以透明的方式，为我们的微服务应用添加安全策略。</p><p><a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 中的安全架构是由多个组件协同完成的。Citadel 是负责安全的主要组件，用于密钥和证书的管理；<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#pilot" target="_blank" rel="external nofollow noopener noreferrer">Pilot</a> 会将安全策略配置分发给 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#envoy" target="_blank" rel="external nofollow noopener noreferrer">Envoy</a> 代理；<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#envoy" target="_blank" rel="external nofollow noopener noreferrer">Envoy</a> 执行安全策略来实现访问控制。下图展示了 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 的安全架构和运作流程。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-secure-arch.svg"></p><p>关于安全管理的更多内容，参考 <a href="../">Istio安全管理</a></p><h3 id="可观测性"><a href="#可观测性" class="headerlink" title="可观测性"></a>可观测性</h3><p>面对复杂的应用环境和不断扩展的业务需求，即使再完备的测试也难以覆盖所有场景，无法保证服务不会出现故障。正因为如此，才需要“可观察性”来对服务的运行时状态进行监控、上报、分析，以提高服务可靠性。具有可观察性的系统，可以在服务出现故障时大大降低问题定位的难度，甚至可以在出现问题之前及时发现问题以降低风险。具体来说，可观察性可以：</p><ul><li>及时反馈异常或者风险使得开发人员可以及时关注、修复和解决问题（告警）；</li><li>出现问题时，能够帮助快速定位问题根源并解决问题，以减少服务损失（减损）；</li><li>收集并分析数据，以帮助开发人员不断调整和改善服务（持续优化）。</li></ul><p>而在微服务治理之中，随着服务数量大大增加，服务拓扑不断复杂化，可观察性更是至关重要。<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 自然也不可能缺少对可观察性的支持。它会为所有的服务间通信生成详细的遥测数据，使得网格中每个服务请求都可以被观察和跟踪。开发人员可以凭此定位故障，维护和优化相关服务。而且，这一特性的引入无需侵入被观察的服务。</p><p><a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 一共提供了三种不同类型的数据从不同的角度支撑起其可观察性：</p><ul><li>指标（Metrics）</li><li>日志（Access Logs）</li><li>分布式追踪（Distributed Traces）</li></ul><p>关于可观测行的更多内容，参考 <a href="../">Istio可观测性</a></p><h2 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h2><p>Istio的架构由<strong>控制平面</strong>和<strong>数据平面</strong>两个部分组成。</p><ul><li>数据平面：由整个网格内的sidecar代理组成，每个sidecar代理会接管流入和流出服务的流量，并配合控制平面完成流量控制等方面的内容。</li><li>控制平面：负责控制和管理数据平面的sidecar代理，完成配置的分发、服务发现和授权鉴权等功能。</li></ul><p>控制平面是 Istio 在原有服务网格产品上，首次提出的架构，实现了对于数据平面的统一管理。</p><p><img alt="Istio Arch" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-arch.svg"></p><h3 id="控制平面"><a href="#控制平面" class="headerlink" title="控制平面"></a>控制平面</h3><h4 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h4><p><code>Pilot</code> 组件的主要功能是将路由规则等配置信息转换为 sidecar 可以识别的信息，并下发给数据平面。可以把它简单的理解为是一个<strong>配置分发器</strong>（dispatcher），并辅助 sidecar 完成流量控制相关的功能。它管理sidecar代理之间的路由流量规则，并配置故障恢复功能，如超时、重试和熔断。</p><p><img alt="Istio Pilot Arch" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-pilot-arch.svg"></p><p>上图显示了Pilot的基本架构，它主要由以下几个部分组成：</p><h5 id="Abstract-Model"><a href="#Abstract-Model" class="headerlink" title="Abstract Model"></a>Abstract Model</h5><p>为了实现对不同服务注册中心 （Kubernetes、consul） 的支持，<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#pilot" target="_blank" rel="external nofollow noopener noreferrer">Pilot</a> 需要对不同的输入来源的数据有一个统一的存储格式，也就是抽象模型。抽象模型中定义的关键成员包括 HostName（Service名称）、Ports（Service端口）、Address（Service ClusterIP）、Resolution （负载均衡策略） 等。</p><h5 id="Platform-Adapters"><a href="#Platform-Adapters" class="headerlink" title="Platform Adapters"></a>Platform Adapters</h5><p>借助平台适配器 Pilot 可以实现服务注册中心数据到抽象模型之间的数据转换。例如 Pilot 中的 Kubernetes 适配器通过 Kubernetes API 服务器得到 Kubernetes 中 Service 和 Pod 的相关信息，然后翻译为抽象模型提供给 Pilot 使用。通过平台适配器模式，Pilot 还可以从 Consul 等平台中获取服务信息，还可以开发适配器将其他提供服务发现的组件集成到 Pilot 中。</p><h5 id="xDS-API"><a href="#xDS-API" class="headerlink" title="xDS API"></a>xDS API</h5><p>Pilot 使用了一套起源于 Envoy 项目的标准数据面 API 来将服务信息和流量规则下发到数据面的 sidecar 中。这套标准数据面 API，也叫 xDS。Sidecar 通过 xDS API 可以动态获取 Listener （监听器）、Route （路由）、<a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#cluster" target="_blank" rel="external nofollow noopener noreferrer">Cluster</a> （集群）及 Endpoint （集群成员）配置：</p><ul><li>LDS，Listener 发现服务：Listener 监听器控制 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#sidecar" target="_blank" rel="external nofollow noopener noreferrer">sidecar</a> 启动端口监听（目前只支持 TCP 协议），并配置 L3/L4 层过滤器，当网络连接达到后，配置好的网络过滤器堆栈开始处理后续事件。</li><li>RDS，Router 发现服务：用于 HTTP 连接管理过滤器动态获取路由配置，路由配置包含 HTTP 头部修改（增加、删除 HTTP 头部键值），virtual hosts （虚拟主机），以及 virtual hosts 定义的各个路由条目。</li><li>CDS，Cluster发现服务：用于动态获取 Cluster 信息。</li><li>EDS，Endpoint 发现服务：用于动态维护端点信息，端点信息中还包括负载均衡权重、金丝雀状态等，基于这些信息，Sidecar 可以做出智能的负载均衡决策。</li></ul><h5 id="User-API"><a href="#User-API" class="headerlink" title="User API"></a>User API</h5><p>Pilot 还定义了一套用户 API， 用户 API 提供了面向业务的高层抽象，可以被运维人员理解和使用。</p><p>运维人员使用该 API 定义流量规则并下发到 Pilot，这些规则被 Pilot 翻译成数据面的配置，再通过标准数据面 API 分发到 sidecar 实例，可以在运行期对微服务的流量进行控制和调整。</p><p>通过运用不同的流量规则，可以对网格中微服务进行精细化的流量控制，如按版本分流、断路器、故障注入、灰度发布等。</p><p>关于 Pilot 的具体实现，可以参考 <a href="../">Istio Pilot 模块分析</a></p><h4 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h4><p><code>Citadel</code> 是 Istio 中专门负责安全的组件，内置有身份和证书管理功能，可以实现较为强大的授权和认证等操作，在1.5 版本之后取消了独立进程，作为一个模块被整合在 istiod 中。</p><p>总体来说，Istio 在安全架构方面主要包括以下内容：</p><ul><li>证书签发机构（CA）负责密钥和证书管理</li><li>API 服务器将安全配置分发给数据平面</li><li>客户端、服务端通过代理安全通信</li><li>Envoy 代理管理遥测和审计</li></ul><p>Istio 的身份标识模型使用一级服务标识来确定请求的来源，它可以灵活的标识终端用户、工作负载等。在平台层面，Istio 可以使用类似于服务名称来标识身份，或直接使用平台提供的服务标识。比如 Kubernetes 的 ServiceAccount，AWS IAM 用户、角色账户等。</p><p>在身份和证书管理方面，Istio 使用 X.509 证书，并支持密钥和证书的自动轮换。从 1.1 版本开始，Istio 开始支持安全发现服务器（SDS），随着不断的完善和增强，1.5 版本 SDS 已经成为默认开启的组件。Citadel 以前有两个功能：将证书以 Secret 的方式挂载到命名空间里；通过 SDS gRPC 接口与 nodeagent（已废弃）通信。目前 Citadel 只需要完成与 SDS 相关的工作，其他功能被移动到了 istiod 中。</p><p>关于Citadel的更多内容，参考 <a href="../">Istio安全管理</a></p><h4 id="Galley"><a href="#Galley" class="headerlink" title="Galley"></a>Galley</h4><p><code>Galley</code> 是 Istio 1.1 版本中新增加的组件，其目的是将 <code>Pilot</code> 和底层平台（如 Kubernetes）进行解耦。它分担了原本 <code>Pilot</code> 的一部分功能，主要负责配置的验证、提取和处理等功能。</p><h3 id="数据平面"><a href="#数据平面" class="headerlink" title="数据平面"></a>数据平面</h3><p>Istio 数据平面核心是以 sidecar 模式运行的智能代理。Sidecar 模式将数据平面核心组件部署到单独的流程或容器中，以提供隔离和封装。Sidecar 应用与父应用程序共享相同的生命周期，与父应用程序一起创建和退出。Sidecar 应用附加到父应用程序，并为应用程序提供额外的特性支持。</p><p>如下图所示，数据平面的 sidecar 代理可以调节和控制微服务之间所有的网络通信，每个服务 Pod 启动时会伴随启动 <code>istio-init</code> 和 proxy 容器。 </p><ul><li><code>istio-init</code> 容器主要功能是初始化 Pod 网络和对 Pod设置 iptable 规则，设置完成后自动结束。</li><li>Proxy 容器会启动两个服务：<code>istio-agent</code> 以及网络代理组件<ul><li><code>istio-agent</code>  的作用是同步管理数据，启动并管理网络代理服务进程，上报遥测数据</li><li>网络代理组件则根据管理策略完成流量管控、生成遥测数据。</li></ul></li></ul><p>数据平面真正触及到对网络数据包的相关操作，是上层控制平面策略的具体执行者。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-data-plane-arch.png"></p><p>Envoy 是 Istio 中默认的数据平面 Sidecar 代理，关于 Sidecar 是如何实现自动注入和流量劫持，以及Sidecar的流量路由机制如何实现，更多可参考 <a href="../">Envoy系列文章</a> 。</p><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><p>这里介绍在 Kubernetes 环境下安装 Istio，在开始之前，你需要有一个 Kubernetes 运行环境。</p><p>从 Istio v1.7 版本开始，Istio官方推荐使用 istioctl 安装。下面是安装步骤：</p><ul><li>在 <a href="https://github.com/istio/istio/releases" target="_blank" rel="external nofollow noopener noreferrer">Istio release</a> 页面下载与操作系统匹配的安装包，并将其解压。这里可以直接用Istio提供的脚本：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">$ curl -L https://raw.githubusercontent.com/istio/istio/release-1.7/release/downloadIstioCandidate.sh | sh -</span><br><span class="line">$  [root@VM-1-28-centos istio]<span class="comment"># ls </span></span><br><span class="line">istio-1.7.0  istio-1.7.0-linux-amd64.tar.gz</span><br><span class="line">$ [root@VM-1-28-centos istio]<span class="built_in">cd</span> istio-1.7.0</span><br><span class="line">$ [root@VM-1-28-centos istio-1.7.0]<span class="comment"># tree -L 2</span></span><br><span class="line">.</span><br><span class="line">├── bin</span><br><span class="line">│   └── istioctl</span><br><span class="line">├── LICENSE</span><br><span class="line">├── manifests</span><br><span class="line">│   ├── charts</span><br><span class="line">│   ├── deploy</span><br><span class="line">│   ├── examples</span><br><span class="line">│   └── profiles</span><br><span class="line">├── manifest.yaml</span><br><span class="line">├── README.md</span><br><span class="line">├── samples</span><br><span class="line">│   ├── addons</span><br><span class="line">│   ├── bookinfo</span><br><span class="line">│   ├── certs</span><br><span class="line">│   ├── cross-network-gateway</span><br><span class="line">│   ├── custom-bootstrap</span><br><span class="line">│   ├── external</span><br><span class="line">│   ├── fortio</span><br><span class="line">│   ├── health-check</span><br><span class="line">│   ├── helloworld</span><br><span class="line">│   ├── httpbin</span><br><span class="line">│   ├── https</span><br><span class="line">│   ├── kubernetes-blog</span><br><span class="line">│   ├── operator</span><br><span class="line">│   ├── rawvm</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── security</span><br><span class="line">│   ├── sleep</span><br><span class="line">│   ├── tcp-echo</span><br><span class="line">│   └── websockets</span><br><span class="line">└── tools</span><br><span class="line">    ├── certs</span><br><span class="line">    ├── convert_RbacConfig_to_ClusterRbacConfig.sh</span><br><span class="line">    ├── dump_kubernetes.sh</span><br><span class="line">    ├── _istioctl</span><br><span class="line">    └── istioctl.bash</span><br><span class="line"></span><br><span class="line">27 directories, 9 files</span><br></pre></td></tr></table></figure><p>安装目录内容： </p><div class="table-container"><table><thead><tr><th>目录</th><th>包含内容</th></tr></thead><tbody><tr><td><code>bin</code></td><td>包含 istioctl 的客户端文件</td></tr><tr><td><code>manifests</code></td><td>包含 各种部署的 manifests</td></tr><tr><td><code>samples</code></td><td>包含示例应用程序</td></tr><tr><td><code>tools</code></td><td>包含用于性能测试和在本地机器上进行测试的脚本</td></tr></tbody></table></div><ul><li>将<code>istioctl</code>客户端路径加入 <code>$PATH</code> 中，从而可以使用 istioctl 命令行工具</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:$(<span class="built_in">pwd</span>)/bin</span><br></pre></td></tr></table></figure><ul><li>安装 <code>demo</code> 配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ istioctl install --<span class="built_in">set</span> profile=demo</span><br><span class="line">✔ Istio core installed</span><br><span class="line">✔ Istiod installed</span><br><span class="line">✔ Egress gateways installed</span><br><span class="line">✔ Ingress gateways installed</span><br><span class="line">✔ Installation complete</span><br></pre></td></tr></table></figure><ul><li>添加一个Namespace Label，使得之后在部署你的应用的时候，istio会自动注入Envoy sidecar 代理</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label namespace default istio-injection=enabled</span><br></pre></td></tr></table></figure><h3 id="部署-Bookinfo"><a href="#部署-Bookinfo" class="headerlink" title="部署 Bookinfo"></a>部署 Bookinfo</h3><p>Bookinfo 是 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 社区官方推荐的示例应用之一。它可以用来演示多种 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 的特性，并且它是一个异构的微服务应用。该应用由四个单独的微服务构成。 这个应用模仿了在线书店，可以展示书店中书籍的信息。例如页面上会显示一本书的描述，书籍的细节（ ISBN、页数等），以及关于这本书的一些评论。</p><p>Bookinfo 应用分为四个单独的微服务， 这些服务对 <a href="https://www.servicemesher.com/istio-handbook/GLOSSARY.html#istio" target="_blank" rel="external nofollow noopener noreferrer">Istio</a> 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个不同语言编写的服务构成，并且其中有一个应用会包含多个版本。</p><ul><li><code>productpage</code> 会调用 <code>details</code> 和 <code>reviews</code> 两个微服务，用来生成页面。</li><li><code>details</code> 中包含了书籍的信息。</li><li><code>reviews</code> 中包含了书籍相关的评论。它还会调用 <code>ratings</code> 微服务。</li><li><code>ratings</code> 中包含了由书籍评价组成的评级信息。</li></ul><p><code>reviews</code> 微服务有 3 个版本，可用来展示各服务之间的不同的调用链路：</p><ul><li>v1 版本不会调用 <code>ratings</code> 服务。</li><li>v2 版本会调用 <code>ratings</code> 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。</li><li>v3 版本会调用 <code>ratings</code> 服务，并使用 1 到 5 个红色星形图标来显示评分信息。</li></ul><p>下图展示了这个应用的端到端架构：</p><p><img alt="Bookinfo Application without Istio" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-bookinfo-noistio.svg"></p><ul><li>部署示例应用程序</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span><br><span class="line">service/details created</span><br><span class="line">serviceaccount/bookinfo-details unchanged</span><br><span class="line">deployment.apps/details-v1 created</span><br><span class="line">service/ratings created</span><br><span class="line">serviceaccount/bookinfo-ratings unchanged</span><br><span class="line">deployment.apps/ratings-v1 created</span><br><span class="line">service/reviews created</span><br><span class="line">serviceaccount/bookinfo-reviews unchanged</span><br><span class="line">deployment.apps/reviews-v1 created</span><br><span class="line">deployment.apps/reviews-v2 created</span><br><span class="line">deployment.apps/reviews-v3 created</span><br><span class="line">service/productpage created</span><br><span class="line">serviceaccount/bookinfo-productpage unchanged</span><br><span class="line">deployment.apps/productpage-v1 created</span><br></pre></td></tr></table></figure><ul><li>之后应用起来，当每个Pod状态变为Ready的时候，sidecar也部署成功。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc</span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">details       ClusterIP   172.18.252.45    &lt;none&gt;        9080/TCP   97s</span><br><span class="line">kubernetes    ClusterIP   172.18.252.1     &lt;none&gt;        443/TCP    51d</span><br><span class="line">productpage   ClusterIP   172.18.253.238   &lt;none&gt;        9080/TCP   97s</span><br><span class="line">ratings       ClusterIP   172.18.254.131   &lt;none&gt;        9080/TCP   97s</span><br><span class="line">reviews       ClusterIP   172.18.255.63    &lt;none&gt;        9080/TCP   97s</span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">details-v1-5974b67c8-z67st        2/2     Running   0          2m8s</span><br><span class="line">productpage-v1-797898bc54-frzdz   2/2     Running   0          2m8s</span><br><span class="line">ratings-v1-c6cdf8d98-xmhz8        2/2     Running   0          2m8s</span><br><span class="line">reviews-v1-8bdc65f7b-mjktx        2/2     Running   0          2m8s</span><br><span class="line">reviews-v2-868d77d678-4dzmn       2/2     Running   0          2m8s</span><br><span class="line">reviews-v3-6c9b646cb4-5tp9q       2/2     Running   0          2m8s</span><br></pre></td></tr></table></figure><ul><li>查看应用是否成功运行，通过给productpage发送请求，查看其返回</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> <span class="string">"<span class="variable">$(kubectl get pod -l app=ratings -o jsonpath='&#123;.items[0].metadata.name&#125;')</span>"</span> -c ratings -- curl -s productpage:9080/productpage | grep -o <span class="string">"&lt;title&gt;.*&lt;/title&gt;"</span></span><br><span class="line">&lt;title&gt;Simple Bookstore App&lt;/title&gt;</span><br></pre></td></tr></table></figure><h3 id="集群外部访问应用"><a href="#集群外部访问应用" class="headerlink" title="集群外部访问应用"></a>集群外部访问应用</h3><p>到现在，Bookinfo 应用已经成功部署，我们在集群内部也已经可以访问，但是在集群外部还不能够访问。为了使得外部能够访问应用程序，我们需要创建一个<a href="https://istio.io/latest/docs/concepts/traffic-management/#gateways" target="_blank" rel="external nofollow noopener noreferrer">Istio Ingress Gateway</a>。</p><ul><li>将应用于istio gateway关联</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml</span><br><span class="line">gateway.networking.istio.io/bookinfo-gateway created</span><br><span class="line">virtualservice.networking.istio.io/bookinfo created</span><br></pre></td></tr></table></figure><ul><li>确保配置上没有问题</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ istioctl analyze</span><br><span class="line">✔ No validation issues found when analyzing namespace: default.</span><br></pre></td></tr></table></figure><ul><li>确定Ingress的IP和Ports</li></ul><p>通过下面的命令来设置 <code>INGRESS_HOST</code> 和 <code>INGRESS_PORT</code>环境变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc istio-ingressgateway -n istio-system</span><br><span class="line">NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                                                                      AGE</span><br><span class="line">istio-ingressgateway   LoadBalancer   172.18.252.12   49.233.242.233   15021:32663/TCP,80:31968/TCP,443:31588/TCP,31400:32002/TCP,15443:30652/TCP   18m</span><br></pre></td></tr></table></figure><p>这里显示 <code>EXTERNAL_IP</code> 已经变设置，表明当前环境下有一个可以使用的外部负载均衡器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">'&#123;.status.loadBalancer.ingress[0].ip&#125;'</span>)</span><br><span class="line">$ <span class="built_in">export</span> INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">'&#123;.spec.ports[?(@.name=="http2")].port&#125;'</span>)</span><br><span class="line">$ <span class="built_in">export</span> SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">'&#123;.spec.ports[?(@.name=="https")].port&#125;'</span>)</span><br></pre></td></tr></table></figure><ul><li>设定GATEWAY_URL</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> GATEWAY_URL=<span class="variable">$INGRESS_HOST</span>:<span class="variable">$INGRESS_PORT</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$GATEWAY_URL</span></span><br><span class="line">49.233.242.233:80</span><br></pre></td></tr></table></figure><ul><li>确认外部访问是否成功：在浏览器直接访问 <code>http://&lt;GATE_WAYURL&gt;/productpage</code> 来访问Bookinfo应用</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-external-access.png"></p><h3 id="查看Dashboard"><a href="#查看Dashboard" class="headerlink" title="查看Dashboard"></a>查看Dashboard</h3><p>Istio集成了 <a href="https://istio.io/latest/docs/ops/integrations/" target="_blank" rel="external nofollow noopener noreferrer">一些</a> 遥测应用，他们可以帮助你对你的服务网格有直观的认识、展示网格的拓扑、分析网格的健康状态</p><ul><li>安装Kiali </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f samples/addons</span><br><span class="line">$ <span class="keyword">while</span> ! kubectl <span class="built_in">wait</span> --<span class="keyword">for</span>=condition=available --timeout=600s deployment/kiali -n istio-system; <span class="keyword">do</span> sleep 1; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li>访问Kiali</li></ul><p>官方教程指示使用 <code>istioctl dashboard kiali</code> 命令来打开浏览器访问 Kiali服务，但是我的 Kubernetes 集群在服务器上，这样显然不行，不要将 Kiali 服务暴露给外部。因为之前集群已经安装了 Traefik ，所以可以使用 Ingress来暴露。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kiali</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">istio-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/kiali</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">kiali</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">20001</span></span><br></pre></td></tr></table></figure><p>在命令行创建Ingress，打开浏览器访问 <code>http://&lt;NodeIP&gt;:&lt;TraefikWebNodePort&gt;/kiali</code> 即可访问Kiali</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_traefik-kiali.png"></p><p>在左侧导航栏点击Graph，选择default的命名空间，可以看到 <code>Bookinfo</code> 应用中各个服务间的关系：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-kiali.png"></p><p>到此为止，你的Istio和相关的服务已经在集群中完好的部署，关于其具体功能演示，参照 <a href="../151719f0">Istio流量控制</a>。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://istio.io/latest/docs/setup/getting-started" target="_blank" rel="external nofollow noopener noreferrer">https://istio.io/latest/docs/setup/getting-started</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Istio 是一个完全开源的服务网格，以透明的方式构建在现有的分布式应用中。它也是一个平台，拥有可以集成任何日志、遥测和策略系统的 API 接口。Istio 多样化的特性使你能够成功且高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="service mesh" scheme="http://houmin.cc/tags/service-mesh/"/>
    
      <category term="istio" scheme="http://houmin.cc/tags/istio/"/>
    
  </entry>
  
  <entry>
    <title>【Service Mesh】开篇</title>
    <link href="http://houmin.cc/posts/ac3e3d15/"/>
    <id>http://houmin.cc/posts/ac3e3d15/</id>
    <published>2020-11-22T06:24:34.000Z</published>
    <updated>2020-11-27T02:46:55.389Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p>Service Mesh 是一个<strong>基础设施层</strong>，用于处理<strong>服务到服务间</strong>的网络通信。<strong>云原生应用</strong>有着复杂的服务拓扑，Service Mesh负责在这些<strong>网络拓扑中实现请求的可靠传递</strong>。在实践中，Service Mesh通常实现为一组轻量级的<strong>网络代理</strong>，它们与应用程序部署在一起，但是<strong>对应用保持透明</strong>。</p></blockquote><p>本文作为 「Service Mesh」系列开篇，将理清 Service Mesh 的前世今生，通过对其概念与原理的理解，开始上手 Service Mesh的工作。与此同时，我们也会讨论 Service Mesh 在业界当前的应用现状，探讨其落地的难点与痛点。</p><a id="more"></a><h2 id="历史演进"><a href="#历史演进" class="headerlink" title="历史演进"></a>历史演进</h2><p>随着行业需求的推动，互联网服务从最早的仅有少数几台的大型服务器演变到成百上千的小型服务，服务架构也从最早期的单体式（Monolithic）到分布式（Distributed），再到微服务（Microservices）、容器化（Containerization）、容器编排（Container Orchestration），最后到服务网格（Service Mesh）、无服务器（Serverless）。</p><p>总结分布式系统的演进过程，我们可以看到一种通用的发展规律：</p><ul><li>首先是对每种情况提出临时解决方案</li><li>然后是更复杂的解决方案，类似于 library 以实现统一复用</li><li>随着对问题有更多的了解，开始将这些解决方案落实到 platform</li></ul><p>接下来我们会回顾从早期TCP/IP协议栈的广泛应用，到微服务时代从容器编排到服务网格的演进过程，并再次体会上述规律。</p><h3 id="计算机网络系统的演进"><a href="#计算机网络系统的演进" class="headerlink" title="计算机网络系统的演进"></a>计算机网络系统的演进</h3><p>从多台计算机开始通信以来，服务间通信是应用最为广泛的模式。以下图为例，ServiceA 和 ServiceB 可以是我们提供应用的服务端与客户端。在开发者开发这些服务的时候，需要借助底层的网络硬件和协议进行通信。这张图只是一个简化的师徒，省略了在代码操作的数据和通过线路发送接收的电信号之间转换的很多层。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-svc2svc.png"></p><p>更加具体一点，把底层的网络协议栈加入，我们会看到下图：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-svc2svc-stack.png"></p><p>从上世纪50年代起，上述的模型就一直在使用。最开始，由于计算机系统规模相对较小，每个节点之间的链路协议都是经过专门设计和维护的。随着计算机规模的迅速扩大，很多个小的网络系统开始连接起来。在这个过程中，不同主机间如何找到彼此，跨网络间如何路由转发，如何实现流量控制等问题，成了摆在网络系统设计人员面前亟需解决的难题。</p><p>为了实现各个网络节点的路由转发，屏蔽链路层协议，人们发明了IP网络协议。然而，IP网络协议还不能够解决流量控制的问题。这里的流量控制，值得是防止一台服务器发送过多的数据包，超出下游服务器的处理能力。在最开始，编写网络服务和应用程序的开发者来负责处理上述流量控制的问题。这就意味着在编写应用程序过程中，网络处理的逻辑和应用自身的业务逻辑被耦合在一起，如下图所示。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-flow-control.png"></p><p>然而，这种每个开发人员都要去考虑流量处理等传输层的问题太过复杂，程序开发的成本太高。随着技术的快速发展，流量处理和其他网络问题相关的解决方案被整合到网络协议栈，TCP/IP席卷了世界，成为互联网事实上的协议标准。流量控制等网络问题的代码仍在，但是你不再需要自己去开发与维护这段代码，而是直接调用系统提供的网络协议栈。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-tcp.png"></p><h3 id="微服务架构的演进"><a href="#微服务架构的演进" class="headerlink" title="微服务架构的演进"></a>微服务架构的演进</h3><p>确定于上世界80年代的TCP/IP网络协议栈和通用的网络模型对于互联网的发展发挥了巨大的作用，极大了促进了互联网应用的繁荣。网络应用的功能逐渐复杂起来，人们把所有的组件都集中在一个应用当中，这即是<code>单体应用 Monolithic</code>。单体应用基于相同技术栈开发、访问共享的数据库、共同部署运维和扩容。同时，组件之间的通信也趋于频繁和耦合，所有的交互都是以函数调用的形式来实现。</p><p>然而，随着互联网的迅猛发展，网络应用中需要添加越来越多的功能，应用的复杂度不断提升，参与软件开发的协作人数也越来越多，单体应用开始爆发出其固有局限性。在这种背景下，微服务的思潮降临，让软件开发重新变得小而美：</p><ul><li>单⼀职责：拆分后的单个微服务，通常只负责单个高内聚自闭环功能，因此很易于开发、理解和维护。</li><li>架构灵活：不同微服务应用之间在技术选型层面几乎是独立的，可以⾃由选择最适合的技术栈。</li><li>部署隔离：相比巨无霸单体应用，单个微服务应用的代码和产物体积大大减少，更容易持续集成和快速部署；同时，通过进程级别的隔离，也不再像单体应用一样只能同生共死，故障隔离效果显著提升。</li><li>独⽴扩展：单体应用时代，某个模块如果存在资源瓶颈（e.g. CPU/内存），只能跟随整个应用一起扩容，白白浪费很多资源。微服务化后，扩展的粒度细化到了微服务级别，可以更精确地按需独立扩展。</li></ul><p>然而，微服务也不是银弹，在微服务落地的过程中，也产生了很多的问题，其中主要的问题就是服务间通信：</p><ul><li><p><strong>如何找到服务的提供⽅？</strong></p><p>微服务通讯必须走远程过程调用（HTTP/REST本质上也属于RPC），当其中一个应用需要消费另一个应用的服务时，无法再像单体应用一样通过简单的进程内机制（e.g. Spring的依赖注入）就能获取到服务实例；你甚至都不知道有没有这个服务方。</p></li><li><p><strong>如何保证远程调⽤的可靠性?</strong></p><p>既然是RPC，那必然要走IP网络，而我们都知道网络（相比计算和存储）是软件世界里最不可靠的东西。虽然有TCP这种可靠传输协议，但频繁丢包、交换机故障甚至电缆被挖断也常有发生；即使网络是好的，如果对方机器宕机了，或者进程负载过高不响应呢？</p></li><li><p><strong>如何降低服务调⽤的延迟？</strong></p><p>网络不只是不可靠，还有延迟的问题。虽然相同系统内的微服务应用通常都部署在一起，同机房内调用延迟很小；但对于较复杂的业务链路，很可能一次业务访问就会包括数十次RPC调用，累积起来的延迟就很可观了。</p></li><li><p><strong>如何保证服务调⽤的安全性？</strong></p><p>网络不只是不可靠和有延迟，还是不安全的。互联网时代，你永远不知道屏幕对面坐的是人还是狗；同样，微服务间通讯时，如果直接走裸的通讯协议，你也永远不知道对端是否真的就是自己人，或者传输的机密信息是否有被中间人偷听。</p></li></ul><h4 id="服务通信：耦合业务逻辑"><a href="#服务通信：耦合业务逻辑" class="headerlink" title="服务通信：耦合业务逻辑"></a>服务通信：耦合业务逻辑</h4><p>就像历史总是会重演，为了解决上述微服务引入的问题，最早需要工程师独立去完成对应的服务，在业务逻辑中实现下列逻辑：</p><ul><li>服务发现（Service Discovery）：解决“我想调用你，如何找到你”的问题。</li><li>服务熔断（Circuit Breaker）：缓解服务之间依赖的不可靠问题。</li><li>负载均衡（Load Balancing）：通过均匀分配流量，让请求处理更加及时。</li><li>安全通讯：包括协议加密（TLS）、身份认证（证书/签名）、访问鉴权（RBAC）等</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-micro-service.png"></p><p>然而，随着分布式程度的增加，这些服务的复杂度也越来越高，一些问题不得不考虑：</p><ul><li>重复造轮子：需要编写和维护⼤量非功能性代码，如何集中精力专注业务创新?</li><li>与业务耦合：服务通讯逻辑与业务代码逻辑混在一起，动不动还会遇到点匪夷所思的分布式bug。</li></ul><h4 id="服务通信：独立Library"><a href="#服务通信：独立Library" class="headerlink" title="服务通信：独立Library"></a>服务通信：独立Library</h4><p>为了解决重复造轮子的问题，集成了服务通信中各种问题的Library开始变得十分流行，包括 Apache Dubbo（手动置顶）、Spring Cloud、Netflix OSS、gRPC 等等。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-micro-service-lib.png"></p><p>这些可复用的类库和框架，确确实实带来了质量和效率上的大幅提升，但是也存在着下列问题：</p><ul><li>并非完全透明：程序员们仍然需要正确理解和使⽤这些库，上手成本和出错概率依然很高。</li><li>限制技术选择：使用这些技术后，应用很容易就会被对应的语⾔和框架强绑定（vendor-lock）。</li><li>维护成本高：库版本升级，需要牵连应⽤一起重新构建和部署；麻烦不说，还要祈祷别出故障。</li></ul><h4 id="服务通信：Sidecar"><a href="#服务通信：Sidecar" class="headerlink" title="服务通信：Sidecar"></a>服务通信：Sidecar</h4><p>像网络协议栈发展的过程一样，将大规模分布式服务所需要的功能剥离出来集成到底层平台是一个众望所归的选择。人们通过应用层的协议(例如HTTP)写出了很多复杂的应用程序和服务，甚至不用考虑TCP是如何控制数据包在网络上传输的。这就是我们微服务所需要的，从事服务开发的工程师们可以专注于业务逻辑的开发，避免浪费时间去编写服务基础设施代码或者管理这些库和框架。</p><p>在这个想法下，我们可以得到类似于如下的图：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-protocol.png"></p><p>不幸的是，更改协议栈来增加微服务的功能不是一个可行的方案，许多开发者是通过一组代理来实现此功能。这里的设计思想是<strong>服务不需要和下游服务直连，所有的流量都通过该代理透明的来实现对应的功能</strong>。这里的透明代理，通过一种叫做 <code>Sidecar</code> 的模式来运行，Sidecar将上述类库和框架要干的事情从应用中彻底剥离了出来，并统一下沉到了基础设施层，这其中的典型代表就是 Linkerd 和 Envoy。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-sidecar.png"></p><h4 id="服务通信：Service-Mesh"><a href="#服务通信：Service-Mesh" class="headerlink" title="服务通信：Service Mesh"></a>服务通信：Service Mesh</h4><p>在这种模型中，每个服务都会有一个配套的代理SideCar。考虑到服务之间的通信仅仅通过SideCar代理，我们最终得到如下的部署图：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-data.png"></p><p>Buoyant的CEO William Morgan ，发现了各个SideCar代理之间互联组成了一个网状网络，<strong>2017初，William为这个网状的平台起了一个<a href="https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/" target="_blank" rel="external nofollow noopener noreferrer">“Service Mesh”的定义</a></strong>。</p><blockquote><p>Service Mesh是一个用于服务和服务之间通信的专用基础设施层。它负责服务请求能够在复杂的服务拓扑(组成了云原生应用)中可靠的进行投递。在实践中，Serivce Mesh的典型实现是作为轻量级网络代理阵列，部署在应用程序旁边，不需要业务进程感知到。</p></blockquote><p>William关于Service Mesh的定义中，最有说服力的一点是，他不再将SideCar代理视为一个独立组件，而是承认了<strong>它们组成的网络像它们自身一样是有价值的</strong></p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-data2.png"></p><p>随着很多公司将它们的微服务部署到更复杂的系统运行环境中，例如Kubernetes和Mesos，人们开始使用这些平台提供的工具来实现合适的Serivce Mesh的想法。它们将独立的SideCar代理从独立的工作环境中转移到一个适当的，有集中的控制面。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-control.png"></p><p>看下我们的鸟瞰图，服务之间的流量仍然是通过SideCar代理来进行转发，但是控制平面知道每个SideCar实例。控制平面能够让代理实现例如访问控制，指标收集等需要协作完成的事情。Istio是这个模型的典型实现。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh-control2.png"> </p><h2 id="主流实现"><a href="#主流实现" class="headerlink" title="主流实现"></a>主流实现</h2><p>Service Mesh 的主流实现包括：</p><ul><li>Linkerd：背后公司是Buoyant，开发语⾔使用Scala，2016年1⽉15日初次发布，2017年1⽉23日加入CNCF。</li><li>Envoy：背后公司是Lyft，开发语言使用C++ 11，2016年9月13日初次发布，2017年9⽉14日加⼊CNCF。</li><li>Istio：背后公司是Google和IBM，开发语言使用Go，2017年5⽉月10日初次发布。</li><li>Conduit：背后公司也是Buoyant，开发语言使用Rust和Go，2017年12月5日初次发布，现在已经加入了 <code>Linkerd</code> 项目。</li></ul><h3 id="Linkerd"><a href="#Linkerd" class="headerlink" title="Linkerd"></a>Linkerd</h3><p>现在（2020.09.08） <code>Linkerd</code> 已经发展到 2.8 版本，由控制面和数据面组成，详情可以参考 <a href="https://linkerd.io/2/reference/architecture/" target="_blank" rel="external nofollow noopener noreferrer">这里</a></p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_linkerd-control-plane.png"></p><h3 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h3><p>Envoy是一个高性能的Service Mesh软件，现在主要被用于数据面作为 Sidecar 代理，详情可以参考 <a href="../7beb34d2/">这里</a></p><p><img alt data-src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200504160047.png"></p><h3 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h3><p>Istio是第二代 Service Mesh，第一次提出控制面的概念，详情可以参考 <a href="../22cae0b8/">这里</a></p><p><img alt="Istio Arch" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-10_istio-arch.svg"></p><h3 id="NginMesh"><a href="#NginMesh" class="headerlink" title="NginMesh"></a>NginMesh</h3><p>Service Mesh 最基础的功能毕竟是 sidecar proxy. 提到 proxy 怎么能够少了 nginx? 我想nginx自己也是这么想的吧 毫不意外，nginx也推出了其 service mesh 的开源实现：nginMesh.</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_nginx-sidecar.png"></p><p>不过，与 William Morgan 的死磕策略不同，nginMesh 从一开始就没有想过要做一套完整的第二代Service Mesh 开源方案，而是直接宣布兼容Istio, 作为Istio的 sidecar proxy. 由于 nginx 在反向代理方面广泛的使用，以及运维技术的相对成熟，nginMesh在sidecar proxy领域应该会有一席之地。</p><h2 id="对比Kubernetes原生架构"><a href="#对比Kubernetes原生架构" class="headerlink" title="对比Kubernetes原生架构"></a>对比Kubernetes原生架构</h2><h3 id="Kube-proxy-vs-Sidecar"><a href="#Kube-proxy-vs-Sidecar" class="headerlink" title="Kube-proxy vs Sidecar"></a>Kube-proxy vs Sidecar</h3><p>下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_k8s-vs-service-mesh.png"></p><ul><li>Kubernetes 集群的每个节点都部署了一个 <code>kube-proxy</code> 组件，该组件会与 Kubernetes API Server 通信，获取集群中的 <code>Service</code> 信息，然后设置 iptables 规则，直接将对某个 <code>Service</code> 的请求发送到对应的 Endpoint（属于同一组 <code>Service</code> 的 <code>Pod</code>）上。</li><li>Kube-proxy 实现了流量在 Kubernetes <code>Service</code> 多个 <code>Pod</code> 实例间的负载均衡，但是如何对这些 <code>Service</code> 间的流量做细粒度的控制，比如按照百分比划分流量到不同的应用版本（这些应用都属于同一个 <code>Service</code>，但位于不同的 deployment 上），做金丝雀发布（灰度发布）和蓝绿发布？Kubernetes 社区给出了 <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments" target="_blank" rel="external nofollow noopener noreferrer">使用 Deployment 做金丝雀发布的方法</a>，该方法本质上就是通过修改 <code>Pod</code> 的 label 来将不同的 <code>Pod</code> 划归到 Deployment 的 <code>Service</code> 上。</li></ul><p><code>kube-proxy</code> 的设置都是全局生效的，无法对每个服务做细粒度的控制，而 <code>Service Mesh</code> 通过 <code>Sidecar</code> proxy 的方式将 Kubernetes 中对流量的控制从 <code>Service</code> 一层抽离出来，可以做更多的扩展。</p><h3 id="Ingress-vs-Gateway"><a href="#Ingress-vs-Gateway" class="headerlink" title="Ingress vs Gateway"></a>Ingress vs Gateway</h3><p> <code>kube-proxy</code> 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 <code>Pod</code> 位于 CNI 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 Ingress 这个资源对象，它由位于 Kubernetes 边缘节点（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理 <strong>南北向流量</strong>，Ingress 必须对接各种 Ingress Controller 才能使用，比如 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="external nofollow noopener noreferrer">nginx ingress controller</a>、<a href="https://traefik.io/" target="_blank" rel="external nofollow noopener noreferrer">traefik</a>。</p><ul><li>Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 <code>Service</code>、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。</li><li>要想直接路由南北向的流量，只能使用 <code>Service</code> 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。</li><li>有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 <code>Service</code> 来暴露，Ingress 本身是不支持的，例如 <a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/" target="_blank" rel="external nofollow noopener noreferrer">nginx ingress controller</a>，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。</li></ul><p><code>Istio</code> Gateway 的功能与 Kubernetes Ingress 类似，都是负责集群的南北向流量。<code>Istio</code> <code>Gateway</code> 描述的负载均衡器用于承载进出网格边缘的连接。该规范中描述了一系列开放端口和这些端口所使用的协议、负载均衡的 SNI 配置等内容。Gateway 是一种 CRD 扩展，它同时复用了 <code>Sidecar</code> proxy 的能力，详细配置请参考 <a href="https://istio.io/docs/reference/config/networking/gateway/" target="_blank" rel="external nofollow noopener noreferrer">Istio 官网</a>。</p><h2 id="落地问题"><a href="#落地问题" class="headerlink" title="落地问题"></a>落地问题</h2><p>服务网格的出现带来的变革：</p><p>第一，<strong>微服务治理与业务逻辑的解耦</strong>。服务网格把 SDK 中的<strong>大部分</strong>能力从应用中剥离出来，拆解为独立进程，以 Sidecar 的模式进行部署。服务网格通过将服务通信及相关管控功能从业务程序中分离并下沉到基础设施层，使其和业务系统完全解耦，使开发人员更加专注于业务本身。</p><blockquote><p>注意，这里提到了一个词“大部分”，SDK 中往往还需要保留<strong>协议编解码</strong>的逻辑，甚至在某些场景下还需要一个轻量级的 SDK 来实现细粒度的治理与监控策略。例如，要想实现方法级别的调用链追踪，服务网格则需要业务应用实现 trace ID 的传递，而这部分实现逻辑也可以通过轻量级的 SDK 实现。因此，从代码层面来讲，服务网格并非是零侵入的。</p></blockquote><p>第二，<strong>异构系统的统一治理</strong>。随着新技术的发展和人员更替，在同一家公司中往往会出现不同语言、不同框架的应用和服务，为了能够统一管控这些服务，以往的做法是为每种语言、每种框架都开发一套完整的 SDK，维护成本非常之高，而且给公司的中间件团队带来了很大的挑战。有了服务网格之后，通过将主体的服务治理能力下沉到基础设施，多语言的支持就轻松很多了。只需要提供一个非常轻量级的 SDK，甚至很多情况下都不需要一个单独的 SDK，就可以方便地实现多语言、多协议的统一流量管控、监控等需求。</p><p>此外，服务网格相对于传统微服务框架，还拥有三大技术优势：</p><ul><li>可观察性。因为服务网格是一个专用的基础设施层，所有的服务间通信都要通过它，所以它在技术堆栈中处于独特的位置，以便在服务调用级别上提供统一的遥测指标。这意味着，所有服务都被监控为“黑盒”。服务网格捕获诸如来源、目的地、协议、URL、状态码、延迟、持续时间等线路数据。这本质上等同于 web 服务器日志可以提供的数据，但是服务网格可以为所有服务捕获这些数据，而不仅仅是单个服务的 web 层。需要指出的是，收集数据仅仅是解决微服务应用程序中可观察性问题的一部分。存储与分析这些数据则需要额外能力的机制的补充，然后作用于警报或实例自动伸缩等。</li><li>流量控制。通过 <code>Service Mesh</code>，可以为服务提供智能路由（蓝绿部署、金丝雀发布、A/B test）、超时重试、熔断、故障注入、流量镜像等各种控制能力。而以上这些往往是传统微服务框架不具备，但是对系统来说至关重要的功能。例如，服务网格承载了微服务之间的通信流量，因此可以在网格中通过规则进行故障注入，模拟部分微服务出现故障的情况，对整个应用的健壮性进行测试。由于服务网格的设计目的是有效地将来源请求调用连接到其最优目标服务实例，所以这些流量控制特性是“面向目的地的”。这正是服务网格流量控制能力的一大特点。</li><li>安全。在某种程度上，单体架构应用受其单地址空间的保护。然而，一旦单体架构应用被分解为多个微服务，网络就会成为一个重要的攻击面。更多的服务意味着更多的网络流量，这对黑客来说意味着更多的机会来攻击信息流。而服务网格恰恰提供了保护网络调用的能力和基础设施。服务网格的安全相关的好处主要体现在以下三个核心领域：服务的认证、服务间通讯的加密、安全相关策略的强制执行。</li></ul><p>服务网格带来了巨大变革并且拥有其强大的技术优势，被称为第二代“微服务架构”。然而就像之前说的软件开发没有银弹，传统微服务架构有许多痛点，而服务网格也不例外，也有它的局限性。</p><ul><li>增加了复杂度。服务网格将 <code>Sidecar</code> 代理和其它组件引入到已经很复杂的分布式环境中，会极大地增加整体链路和操作运维的复杂性。</li><li>运维人员需要更专业。在容器编排器（如 Kubernetes）上添加 <code>Istio</code> 之类的服务网格，通常需要运维人员成为这两种技术的专家，以便充分使用二者的功能以及定位环境中遇到的问题。</li><li>延迟。从链路层面来讲，服务网格是一种侵入性的、复杂的技术，可以为系统调用增加显著的延迟。这个延迟是毫秒级别的，但是在特殊业务场景下，这个延迟可能也是难以容忍的。</li><li>平台的适配。服务网格的侵入性迫使开发人员和运维人员适应高度自治的平台并遵守平台的规则。</li></ul><h2 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h2><p>展望未来，Kubernetes 正在爆炸式发展，它已经成为企业绿地应用的容器编排的首选。如果说 Kubernetes 已经彻底赢得了市场，并且基于 Kubernetes 的应用程序的规模和复杂性持续增加，那么就会有一个临界点，而服务网格则将是有效管理这些应用程序所必需的。随着服务网格技术的持续发展，其实现产品（如 <code>Istio</code>）的架构与功能的不断优化，服务网格将完全取代传统微服务架构，成为大小企业微服务化和上云改造的首选架构。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="external nofollow noopener noreferrer">https://philcalcado.com/2017/08/03/pattern_service_mesh.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Service Mesh 是一个&lt;strong&gt;基础设施层&lt;/strong&gt;，用于处理&lt;strong&gt;服务到服务间&lt;/strong&gt;的网络通信。&lt;strong&gt;云原生应用&lt;/strong&gt;有着复杂的服务拓扑，Service Mesh负责在这些&lt;strong&gt;网络拓扑中实现请求的可靠传递&lt;/strong&gt;。在实践中，Service Mesh通常实现为一组轻量级的&lt;strong&gt;网络代理&lt;/strong&gt;，它们与应用程序部署在一起，但是&lt;strong&gt;对应用保持透明&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文作为 「Service Mesh」系列开篇，将理清 Service Mesh 的前世今生，通过对其概念与原理的理解，开始上手 Service Mesh的工作。与此同时，我们也会讨论 Service Mesh 在业界当前的应用现状，探讨其落地的难点与痛点。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-09-08_service-mesh.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="service" scheme="http://houmin.cc/tags/service/"/>
    
      <category term="service mesh" scheme="http://houmin.cc/tags/service-mesh/"/>
    
      <category term="sidecar" scheme="http://houmin.cc/tags/sidecar/"/>
    
  </entry>
  
  <entry>
    <title>めぐる季节</title>
    <link href="http://houmin.cc/posts/b0b2b640/"/>
    <id>http://houmin.cc/posts/b0b2b640/</id>
    <published>2020-11-21T11:23:16.000Z</published>
    <updated>2020-11-26T13:36:14.191Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>经过立冬后的一夜冬雨，北京城满地黄叶堆积，下班路上被冻进口袋的双手真真切地告诉我：冬天真的来了。经过一周的工作，今年冬天的初雪如约而至，这里是2020年「朝花夕拾」第二十五期 <code>めぐる季节</code>，在季节转换间我们继续。</p>    <div id="aplayer-oAuhnNXp" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="445063" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>又是一年的初雪，本来想着今天去城内好好拍照的，想着在家吃完火锅再去，结果吃完雪已经停了，「小雪」真的名不虚传。很可惜，二十四节气的小雪这次搁置，下次这种机会一定要提前准备，有备无患。毕竟，你在北京呆的时间也没有多少时间了，要抓住每一次机会。</p><p>按照惯例，我们继续每周的数据总结，首先是 <code>RescueTime</code>：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_rescue-time.png"></p><ul><li>精力分散的时间里面，依然是企业微信消耗的时间占比最多。大家都知道工作时间被碎片化是一件非常难受的事情，好不容易进入某项工作的 <code>context</code>，又被企业微信/钉钉等工具打扰，上下文切换耗时太长。相比之下，还是更喜欢邮件一点，因为大家也不期待你能够及时回复，在一段时间内你可以专注你自己的事情。想到一个改进措施，每天花固定的时间来解决需要企业微信来进行沟通相关的事情，比如<code>11:30</code>、<code>5:30</code> 那会，所有要沟通的事情统一解决，下周看看效果。</li><li>相比于上周，这个周末的分散时间好多了，因为周末在学习GPU相关的内容，这也导致这篇「朝花夕拾」晚了四天</li><li>漫无目的刷手机的情况还存在，在改善中，手机知乎就不应该存在，知乎是碰到问题去搜索的，现在推送质量太差，平时阅读可以靠微信公众号和RSS订阅，这周入手了Kindle，是天然的RSS阅读器</li></ul><p>谷歌日历的每天总结还是没跟上，IFFT在印象笔记上的每日总结推送还在继续，下周加上这个维度的总结监测。接下来是Forest专注时间观察，现在对于 <code>Forest</code> 的使用还是太随意化了，每次要专注的时候有时候会忘记种树，还有的时候会去用手机。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_forest.jpg"></p><p>接下来是跑步数据，相比上周的只有一次跑步，这周一周三次跑步算得上一种进步了，但是真正的跑步习惯还没形成，跑步的时候也算不得轻松自在一路坚持下来，下周要继续。毕竟体脂还是有点高的，坚持减肥感受一下自己能够瘦到什么地步。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_running.jpg"></p><p>睡眠数据还没有进入统计，争取下周前能够把这套数据系统搭建起来，不论是购买新的硬件还是基于当前硬件挖数据。</p><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><h3 id="区域全面经济合作协定"><a href="#区域全面经济合作协定" class="headerlink" title="区域全面经济合作协定"></a>区域全面经济合作协定</h3><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-15-rcep.jpg"></p><h3 id="华为出售荣耀"><a href="#华为出售荣耀" class="headerlink" title="华为出售荣耀"></a>华为出售荣耀</h3><h3 id="信用债"><a href="#信用债" class="headerlink" title="信用债"></a>信用债</h3><h3 id="蛋壳公寓暴雷"><a href="#蛋壳公寓暴雷" class="headerlink" title="蛋壳公寓暴雷"></a>蛋壳公寓暴雷</h3><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过立冬后的一夜冬雨，北京城满地黄叶堆积，下班路上被冻进口袋的双手真真切地告诉我：冬天真的来了。经过一周的工作，今年冬天的初雪如约而至，这里是2020年「朝花夕拾」第二十五期 &lt;code&gt;めぐる季节&lt;/code&gt;，在季节转换间我们继续。&lt;/p&gt;

    &lt;div id=&quot;aplayer-oAuhnNXp&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;445063&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_snow.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="小雪" scheme="http://houmin.cc/tags/%E5%B0%8F%E9%9B%AA/"/>
    
      <category term="RECP" scheme="http://houmin.cc/tags/RECP/"/>
    
  </entry>
  
  <entry>
    <title>【Kubernetes】GPU 共享</title>
    <link href="http://houmin.cc/posts/cf391335/"/>
    <id>http://houmin.cc/posts/cf391335/</id>
    <published>2020-11-18T03:11:07.000Z</published>
    <updated>2020-12-08T02:24:52.471Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>原生的 k8s 基于 <code>Device Plugin</code> 和 <code>Extended Resource</code> 机制实现了在容器中使用GPU，但是只支持GPU的独占使用，不允许在Pod间共享GPU，这大大降低了对集群中GPU的利用率。为了在集群层面共享GPU，我们需要实现GPU资源的隔离与调度，本文将依次介绍阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 与腾讯的 <a href="https://github.com/tkestack/gpu-manager" target="_blank" rel="external nofollow noopener noreferrer">GPUManager</a>，分析其实现机制。</p><a id="more"></a><h2 id="阿里GPUShare"><a href="#阿里GPUShare" class="headerlink" title="阿里GPUShare"></a>阿里GPUShare</h2><p>阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 基于 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="external nofollow noopener noreferrer">Nvidia Docker2</a> 和他们的 <a href="https://docs.google.com/document/d/1ZgKH_K4SEfdiE_OfxQ836s4yQWxZfSjS288Tq9YIWCA/edit#heading=h.r88v2xgacqr" target="_blank" rel="external nofollow noopener noreferrer">gpu sharing design</a> 设计而实现的，为了使用阿里的GPUShare，首先需要配置Node上的 Docker Runtime 并安装 <code>NVIDIA Docker 2</code>，具体过程可以参考 <a href="../574111db">在Docker中使用GPU</a>。</p><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><h4 id="假设条件"><a href="#假设条件" class="headerlink" title="假设条件"></a>假设条件</h4><ul><li>尽管GPU可以从 CUDA Cores 和 GPU Memory 两个维度来衡量GPU的能力，<strong>在推理的场景，我们可以假定CUDA core的数量和GPU  Memory的大小是成比例的</strong></li><li>在模型开发和推理的场景下，<strong>用户申请的GPU资源不超过1个GPU，也就是说 resource limit 是 一个GPU</strong></li><li>每个Node上所有卡的GPU Memory相同，这样可以通过 <code>gpuTotalMemory</code> 和 <code>gpuTotalCount</code> 算出Node上每张卡的GPU Memory</li></ul><h4 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h4><ul><li><p>设计里定义了两种 <code>Extended Resource</code>：</p><ul><li><code>aliyun.com/gpu-mem</code>： 单位从 <code>number of GPUs</code> 变更为 <code>amount of GPU memory in MiB</code>，如果一个Node有多个GPU设备，这里计算的是总的GPU Memory</li><li><code>aliyun.com/gpu-count</code>：对应于Node上的GPU 设备的数目</li></ul></li><li>基于k8s原生的Scheduler Extender、Extended Resource、DevicePlugin机制来实现</li><li>这个方案只实现GPU的共享，不实现算力和显存的隔离，如果想实现隔离，在阿里云可以搭配 <a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">cGPU</a> 一起使用</li></ul><h4 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h4><p>下图是整个设计的核心组件：</p><ul><li>GPU Share Scheduler Extender：基于k8s scheduler extender机制，作用于调度过程的<code>Filter</code>和<code>Bind</code>阶段，用于决定某个Node上的一个GPU设备是否可以提供足够的GPU Memory，并将GPU分配的结果记录到Pod Spec 的 Annotation中</li><li>GPU Share Device Plugin：基于k8s device plugin机制，根据GPU Share Scheduler Extender记录在Pod Spec的Annotation，实现GPU 设备的 Allocation。</li></ul><p><img alt="GPU Share Design" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share.jpg"></p><h3 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h3><h4 id="设备资源报告"><a href="#设备资源报告" class="headerlink" title="设备资源报告"></a>设备资源报告</h4><p><code>GPU Share Device Plugin</code> 基于 <code>nvml</code> 库来查询每个Node上GPU设备的数目和每个GPU设备的GPU Memory。</p><p>这些资源状况被通过 <code>ListAndWatch()</code> 汇报给 Kubelet，然后 kubelet 会上报给 APIServer，这时候执行 <code>kubectl get node</code> 可以看到在 <code>status</code> 看到相关的<code>Extended Resource</code>字段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Node</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.4</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">gpushare:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podCIDR:</span> <span class="number">172.16</span><span class="number">.1</span><span class="number">.0</span><span class="string">/26</span></span><br><span class="line">  <span class="attr">podCIDRs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.16</span><span class="number">.1</span><span class="number">.0</span><span class="string">/26</span></span><br><span class="line">  <span class="attr">providerID:</span> <span class="string">qcloud:///800002/ins-hsmsc4x9</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">allocatable:</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-count:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-mem:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">5926m</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">"47438316671"</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">54222084Ki</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-count:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="attr">aliyun.com/gpu-mem:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"6"</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">51473868Ki</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">57448708Ki</span></span><br><span class="line">    <span class="string">...</span></span><br></pre></td></tr></table></figure><h4 id="调度插件扩展"><a href="#调度插件扩展" class="headerlink" title="调度插件扩展"></a>调度插件扩展</h4><p>用户申请GPU的时候，在 Extended Resource 中只填写 <code>gpu-mem</code>，下面部署一个单机版的Tensorflow：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">tensorflow/tensorflow:2.2.1-gpu-py3-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">4</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">            <span class="attr">aliyun.com/gpu-mem:</span> <span class="number">3</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">jupyter-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8888</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br></pre></td></tr></table></figure><h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><p>当kube-scheduler运行完所有的Filter函数后，就会调用 <code>GPU Share Extender</code> 的 Filter 函数。在原生的过滤中，kube-scheduler会计算是否有足够的Extended Resource（算的是总共的GPU Memory），但是不能知道是否某个GPU设备有足够的资源，这时候就需要调度器插件来实现。以下图为例：</p><ul><li>用户申请了8138MiB的GPU Memory，对于原生调度器，N1节点只剩下  (16276 * 2 - 16276 - 12207 = 4069) 的GPU资源，不满足 Extended Resource可用的条件，N1节点被过滤掉</li><li>接下来的N2节点和N3节点剩余的总的资源数都有8138MiB，那么该选择哪一个呢</li><li>在 <code>GPU Share Extender</code> 的过滤中，他需要找到有单个GPU能够满足用户申请的资源，当检查到N2节点的时候，发现虽然总的GPU Memory有8138MiB，但是每个GPU设备都只剩4096MiB了，不能满足单设备8138的需求，所以N2被过滤掉</li><li>扫描到N3节点，发现GPU0满足8138MiB的需求，符合要求</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-filter.jpg"></p><blockquote><p><strong>这里有一个问题：当一个Node上有多张卡的时候，Scheduler Extender是如何知道每张卡当前可用的Capacity的呢？</strong></p></blockquote><p>我们看一下Extender在 Filter 阶段执行的函数，对于要创建的Pod，当前Node检查自己拥有的所有可用GPU，一旦有一个GPU的可用显存大于申请的显存，那么当前Node是可以被调度的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// check if the pod can be allocated on the node</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">Assume</span><span class="params">(pod *v1.Pod)</span> <span class="params">(allocatable <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line">allocatable = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">n.rwmu.RLock()</span><br><span class="line"><span class="keyword">defer</span> n.rwmu.RUnlock()</span><br><span class="line"></span><br><span class="line">availableGPUs := n.getAvailableGPUs()</span><br><span class="line">reqGPU := <span class="keyword">uint</span>(utils.GetGPUMemoryFromPodResource(pod))</span><br><span class="line">log.Printf(<span class="string">"debug: AvailableGPUs: %v in node %s"</span>, availableGPUs, n.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(availableGPUs) &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">for</span> devID := <span class="number">0</span>; devID &lt; <span class="built_in">len</span>(n.devs); devID++ &#123;</span><br><span class="line">availableGPU, ok := availableGPUs[devID]</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="keyword">if</span> availableGPU &gt;= reqGPU &#123;</span><br><span class="line">allocatable = <span class="literal">true</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> allocatable</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来的一个问题是，每个Node可用的GPU显存是如何得到的呢？我们进入到 <code>getAvailableGPUs</code> 继续看：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getAvailableGPUs</span><span class="params">()</span> <span class="params">(availableGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">allGPUs := n.getAllGPUs()</span><br><span class="line">usedGPUs := n.getUsedGPUs()</span><br><span class="line">unhealthyGPUs := n.getUnhealthyGPUs()</span><br><span class="line">availableGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> id, totalGPUMem := <span class="keyword">range</span> allGPUs &#123;</span><br><span class="line"><span class="keyword">if</span> usedGPUMem, found := usedGPUs[id]; found &#123;</span><br><span class="line">availableGPUs[id] = totalGPUMem - usedGPUMem</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(<span class="string">"info: available GPU list %v before removing unhealty GPUs"</span>, availableGPUs)</span><br><span class="line"><span class="keyword">for</span> id, _ := <span class="keyword">range</span> unhealthyGPUs &#123;</span><br><span class="line">log.Printf(<span class="string">"info: delete dev %d from availble GPU list"</span>, id)</span><br><span class="line"><span class="built_in">delete</span>(availableGPUs, id)</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(<span class="string">"info: available GPU list %v after removing unhealty GPUs"</span>, availableGPUs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> availableGPUs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里可以看到，<code>Scheduler Extender</code> 内部维护了当前Node上所有的GPU显存状态和已经用了的GPU显存状态信息：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// device index: gpu memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getUsedGPUs</span><span class="params">()</span> <span class="params">(usedGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">usedGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> n.devs &#123;</span><br><span class="line">usedGPUs[dev.idx] = dev.GetUsedGPUMemory()</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(<span class="string">"info: getUsedGPUs: %v in node %s, and devs %v"</span>, usedGPUs, n.name, n.devs)</span><br><span class="line"><span class="keyword">return</span> usedGPUs</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// device index: gpu memory</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">getAllGPUs</span><span class="params">()</span> <span class="params">(allGPUs <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">allGPUs = <span class="keyword">map</span>[<span class="keyword">int</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> n.devs &#123;</span><br><span class="line">allGPUs[dev.idx] = dev.totalGPUMem</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(<span class="string">"info: getAllGPUs: %v in node %s, and dev %v"</span>, allGPUs, n.name, n.devs)</span><br><span class="line"><span class="keyword">return</span> allGPUs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于 <code>GetUsedGPUMemory</code>，是<code>Scheduler Extender</code> 内部维护的 <code>DeviceInfo</code> 所记录的，这里的 <code>d.podMap</code> 会在每次Extender执行 <code>Bind</code> 的时候，将对应的Pod添加到对应的Node上的 <code>DeviceInfo</code>中：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(d *DeviceInfo)</span> <span class="title">GetUsedGPUMemory</span><span class="params">()</span> <span class="params">(gpuMem <span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">log.Printf(<span class="string">"debug: GetUsedGPUMemory() podMap %v, and its address is %p"</span>, d.podMap, d)</span><br><span class="line">d.rwmu.RLock()</span><br><span class="line"><span class="keyword">defer</span> d.rwmu.RUnlock()</span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> d.podMap &#123;</span><br><span class="line"><span class="keyword">if</span> pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed &#123;</span><br><span class="line">log.Printf(<span class="string">"debug: skip the pod %s in ns %s due to its status is %s"</span>, pod.Name, pod.Namespace, pod.Status.Phase)</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// gpuMem += utils.GetGPUMemoryFromPodEnv(pod)</span></span><br><span class="line">gpuMem += utils.GetGPUMemoryFromPodAnnotation(pod)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> gpuMem</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再总结总结，本质上是 <code>Scheduler Extender</code> 维护了一个 <code>devs</code> 这么一个数据结构，使得它可以知道当前Node上每个GPU设备的显存状态。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// NodeInfo is node level aggregated information.</span></span><br><span class="line"><span class="keyword">type</span> NodeInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">name           <span class="keyword">string</span></span><br><span class="line">node           *v1.Node</span><br><span class="line">devs           <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo</span><br><span class="line">gpuCount       <span class="keyword">int</span></span><br><span class="line">gpuTotalMemory <span class="keyword">int</span></span><br><span class="line">rwmu           *sync.RWMutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么问题来了，我们通过ApiServer，只能知道对应Node上的 <code>gpuCount</code> 和 <code>gpuTotalMemory</code>，而不知道每张卡各自的显存的。这个 <code>devs</code> 是怎么初始化得到每张卡的显存信息呢的呢？继续看代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create Node Level</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewNodeInfo</span><span class="params">(node *v1.Node)</span> *<span class="title">NodeInfo</span></span> &#123;</span><br><span class="line">log.Printf(<span class="string">"debug: NewNodeInfo() creates nodeInfo for %s"</span>, node.Name)</span><br><span class="line"></span><br><span class="line">devMap := <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; utils.GetGPUCountInNode(node); i++ &#123;</span><br><span class="line">devMap[i] = newDeviceInfo(i, <span class="keyword">uint</span>(utils.GetTotalGPUMemory(node)/utils.GetGPUCountInNode(node)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(devMap) == <span class="number">0</span> &#123;</span><br><span class="line">log.Printf(<span class="string">"warn: node %s with nodeinfo %v has no devices"</span>,</span><br><span class="line">node.Name,</span><br><span class="line">node)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &amp;NodeInfo&#123;</span><br><span class="line">name:           node.Name,</span><br><span class="line">node:           node,</span><br><span class="line">devs:           devMap,</span><br><span class="line">gpuCount:       utils.GetGPUCountInNode(node),</span><br><span class="line">gpuTotalMemory: utils.GetTotalGPUMemory(node),</span><br><span class="line">rwmu:           <span class="built_in">new</span>(sync.RWMutex),</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，<strong>这里在初始化的时候，默认设定每张GPU卡的显存大小一样，通过平均得到每张卡的心存信息。</strong></p><h5 id="Bind"><a href="#Bind" class="headerlink" title="Bind"></a>Bind</h5><ul><li>当调度器发现有Node符合要求，这时候会把Pod和Node Bind到一起，<code>GPU Share Extender</code> 需要做两件事情：<ul><li>根据 <code>binpack</code> 原则找到Node上对应的GPU设备，并将 GPU Device ID记录到 Pod的 Annotation中 <code>ALIYUN_GPU_ID</code>。他也会将Pod使用的GPU Memory记录到Pod Annotation中：<code>ALIYUN_COM_GPU_MEM_POD</code> 和 <code>ALIYUN_COM_GPU_MEM_ASSUME_TIME</code></li><li>Bind the Node and Pod with kubernetes API</li></ul></li><li>如果没有找到合适的Node符合要求，那么就不会做Bind操作</li></ul><p>以下图为例，N1中有4个GPU，其中GPU0（12207），GPU1（8138）、GPU2（4069）和GPU3（16276）, GPU2因为资源不够被过滤掉，剩下的3个GPU根据 Binpack 原则，我们选用GPU1（图里面 Annotation错了，不是0，而是1）</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share-bind.jpg"></p><p>我们看一看在找GPU设备的时候是如何操作的，可以看到这里通过 <code>candidateGPUMemory &gt; availableGPU</code> 这里实现了 <code>binpack</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// allocate the GPU ID to the pod</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(n *NodeInfo)</span> <span class="title">allocateGPUID</span><span class="params">(pod *v1.Pod)</span> <span class="params">(candidateDevID <span class="keyword">int</span>, found <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">reqGPU := <span class="keyword">uint</span>(<span class="number">0</span>)</span><br><span class="line">found = <span class="literal">false</span></span><br><span class="line">candidateDevID = <span class="number">-1</span></span><br><span class="line">candidateGPUMemory := <span class="keyword">uint</span>(<span class="number">0</span>)</span><br><span class="line">availableGPUs := n.getAvailableGPUs()</span><br><span class="line"></span><br><span class="line">reqGPU = <span class="keyword">uint</span>(utils.GetGPUMemoryFromPodResource(pod))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> reqGPU &gt; <span class="keyword">uint</span>(<span class="number">0</span>) &#123;</span><br><span class="line">log.Printf(<span class="string">"info: reqGPU for pod %s in ns %s: %d"</span>, pod.Name, pod.Namespace, reqGPU)</span><br><span class="line">log.Printf(<span class="string">"info: AvailableGPUs: %v in node %s"</span>, availableGPUs, n.name)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(availableGPUs) &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">for</span> devID := <span class="number">0</span>; devID &lt; <span class="built_in">len</span>(n.devs); devID++ &#123;</span><br><span class="line">availableGPU, ok := availableGPUs[devID]</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="keyword">if</span> availableGPU &gt;= reqGPU &#123;</span><br><span class="line"><span class="keyword">if</span> candidateDevID == <span class="number">-1</span> || candidateGPUMemory &gt; availableGPU &#123;</span><br><span class="line">candidateDevID = devID</span><br><span class="line">candidateGPUMemory = availableGPU</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">found = <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line">log.Printf(<span class="string">"info: Find candidate dev id %d for pod %s in ns %s successfully."</span>,</span><br><span class="line">candidateDevID,</span><br><span class="line">pod.Name,</span><br><span class="line">pod.Namespace)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">log.Printf(<span class="string">"warn: Failed to find available GPUs %d for the pod %s in the namespace %s"</span>,</span><br><span class="line">reqGPU,</span><br><span class="line">pod.Name,</span><br><span class="line">pod.Namespace)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> candidateDevID, found</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Kubelet创建Pod"><a href="#Kubelet创建Pod" class="headerlink" title="Kubelet创建Pod"></a>Kubelet创建Pod</h4><p>接下来由Kubelet在创建container前调用 <code>GPU Share Device Plugin</code> 的 <code>Allocate</code> 函数，参数是申请的GPU Memory的数量。</p><p>Pod运行成功后，执行 <code>kubectl get pod</code> 可以看到：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_ASSIGNED:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_ASSUME_TIME:</span> <span class="string">"1606125285243248618"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_DEV:</span> <span class="string">"22"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_IDX:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">ALIYUN_COM_GPU_MEM_POD:</span> <span class="string">"3"</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure><ul><li><p>Device Plugin 从 k8s apiserver 拿到所有Pending的Pod中属于GPU Share的Pod，并且按照 AssumedTimestamp排序</p></li><li><p>选择符合Allocation传入的GPU Memory的Pod，如果有多个，选择最早的那个Pod</p></li><li><p>标记 <code>ALIYUN_COM_GPU_MEM_ASSIGNED</code> 为 True</p></li><li><p>把 DeviceID 作为下NVIDIA_VISIBLE_DEVICES环境变量告诉 Nvidia Docker2，并且创建容器</p></li></ul><p><img alt data-src="https://github.com/AliyunContainerService/gpushare-scheduler-extender/raw/master/docs/designs/sequence.jpg"></p><blockquote><p><strong>这里问题是device plugin的allocate接口参数是什么，是否包含pod信息，是否包含pod annotation？</strong></p></blockquote><p>查看 Device Plugin 的代码，这一个申请的GPU Memory的数量让我很疑惑，为何要这么算？</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, req := <span class="keyword">range</span> reqs.ContainerRequests &#123;</span><br><span class="line">podReqGPU += <span class="keyword">uint</span>(<span class="built_in">len</span>(req.DevicesIDs))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>继续看 <code>Device Plugin</code> 的 <code>DeviceIDs</code> 是如何生成的。这里调用了 <code>nvml library</code> 可以探测到本Node上拥有的GPU有多少个，每个显存是多少。接下来 <code>Device Plugin</code> 会创建一系列的 <code>FakeDeviceID</code>，并将这个DeviceIDs返回给 Kubelet，这就解释了为什么要通过上面的方法计算申请的 GPU Memory，这里的Memory以MiB为单位。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getDevices</span><span class="params">()</span> <span class="params">([]*pluginapi.Device, <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">uint</span>)</span></span> &#123;</span><br><span class="line">n, err := nvml.GetDeviceCount()</span><br><span class="line">check(err)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> devs []*pluginapi.Device</span><br><span class="line">realDevNames := <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">uint</span>&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">uint</span>(<span class="number">0</span>); i &lt; n; i++ &#123;</span><br><span class="line">d, err := nvml.NewDevice(i)</span><br><span class="line">check(err)</span><br><span class="line"><span class="comment">// realDevNames = append(realDevNames, d.UUID)</span></span><br><span class="line"><span class="keyword">var</span> id <span class="keyword">uint</span></span><br><span class="line">log.Infof(<span class="string">"Deivce %s's Path is %s"</span>, d.UUID, d.Path)</span><br><span class="line">_, err = fmt.Sscanf(d.Path, <span class="string">"/dev/nvidia%d"</span>, &amp;id)</span><br><span class="line">check(err)</span><br><span class="line">realDevNames[d.UUID] = id</span><br><span class="line"><span class="comment">// var KiB uint64 = 1024</span></span><br><span class="line">log.Infof(<span class="string">"# device Memory: %d"</span>, <span class="keyword">uint</span>(*d.Memory))</span><br><span class="line"><span class="keyword">if</span> getGPUMemory() == <span class="keyword">uint</span>(<span class="number">0</span>) &#123;</span><br><span class="line">setGPUMemory(<span class="keyword">uint</span>(*d.Memory))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> j := <span class="keyword">uint</span>(<span class="number">0</span>); j &lt; getGPUMemory(); j++ &#123;</span><br><span class="line">fakeID := generateFakeDeviceID(d.UUID, j)</span><br><span class="line"><span class="keyword">if</span> j == <span class="number">0</span> &#123;</span><br><span class="line">log.Infoln(<span class="string">"# Add first device ID: "</span> + fakeID)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> j == getGPUMemory()<span class="number">-1</span> &#123;</span><br><span class="line">log.Infoln(<span class="string">"# Add last device ID: "</span> + fakeID)</span><br><span class="line">&#125;</span><br><span class="line">devs = <span class="built_in">append</span>(devs, &amp;pluginapi.Device&#123;</span><br><span class="line">ID:     fakeID,</span><br><span class="line">Health: pluginapi.Healthy,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> devs, realDevNames</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看一下 <code>Device Plugin</code> 是如何找到对应的Pod的，可以看到一旦碰到有Pod申请的GPU显存与Kubelet传入的显存大小一致，那么则找到对应的Pod了。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pods, err := getCandidatePods()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">   log.Infof(<span class="string">"invalid allocation requst: Failed to find candidate pods due to %v"</span>, err)</span><br><span class="line">   <span class="keyword">return</span> buildErrResponse(reqs, podReqGPU), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">   <span class="keyword">if</span> getGPUMemoryFromPodResource(pod) == podReqGPU &#123;</span><br><span class="line">      log.Infof(<span class="string">"Found Assumed GPU shared Pod %s in ns %s with GPU Memory %d"</span>,</span><br><span class="line">         pod.Name,</span><br><span class="line">         pod.Namespace,</span><br><span class="line">         podReqGPU)</span><br><span class="line">      assumePod = pod</span><br><span class="line">      found = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的 <code>getCandidatePods</code>就是List所有Pending的Pod中 Assume Memory的，并且按照时间排序：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pick up the gpushare pod with assigned status is false, and</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getCandidatePods</span><span class="params">()</span> <span class="params">([]*v1.Pod, error)</span></span> &#123;</span><br><span class="line">candidatePods := []*v1.Pod&#123;&#125;</span><br><span class="line">allPods, err := getPendingPodsInNode()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> candidatePods, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> allPods &#123;</span><br><span class="line">current := pod</span><br><span class="line"><span class="keyword">if</span> isGPUMemoryAssumedPod(&amp;current) &#123;</span><br><span class="line">candidatePods = <span class="built_in">append</span>(candidatePods, &amp;current)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"><span class="keyword">return</span> makePodOrderdByAge(candidatePods), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>那么这里有一个问题：如果在同一个Node有两个Pod <poda, podb>，都申请了相同的GPU显存大小，比如3G，那么kubelet是在创建容器的时候，是如何保证两个Pod不混淆的呢？混淆会有问题吗，kubelet建Pod的时候到底是怎么搞的？是谁触发了kubelet创建容器？</poda,></strong></p></blockquote><hr><h2 id="腾讯GPUManager"><a href="#腾讯GPUManager" class="headerlink" title="腾讯GPUManager"></a>腾讯GPUManager</h2><p>GPU Manager 提供一个 All-in-One 的 GPU 管理器，基于 Kubernetes DevicePlugin 插件系统实现，该管理器提供了分配并共享 GPU、GPU 指标查询、容器运行前的 GPU 相关设备准备等功能，支持用户在 Kubernetes 集群中使用 GPU 设备。</p><ul><li><strong>拓扑分配</strong>：提供基于 GPU 拓扑分配功能，当用户分配超过1张 GPU 卡的应用，可以选择拓扑连接最快的方式分配 GPU 设备。</li><li><strong>GPU 共享</strong>：允许用户提交小于1张卡资源的任务，并提供 QoS 保证。</li><li><strong>应用 GPU 指标的查询</strong>：用户可以访问主机端口（默认为 5678）的 <code>/metrics</code> 路径，可以为 Prometheus 提供 GPU 指标的收集功能，访问 <code>/usage</code> 路径可以进行可读性的容器状况查询。</li></ul><h3 id="架构设计-1"><a href="#架构设计-1" class="headerlink" title="架构设计"></a>架构设计</h3><h4 id="设计原则-1"><a href="#设计原则-1" class="headerlink" title="设计原则"></a>设计原则</h4><ul><li><p>设计里定义了两种 <code>Extended Resource</code>：</p><ul><li><code>tencent.com/vcuda-core</code> ： <code>vcuda-core</code>对应的是使用率，单张卡有100个core</li><li><code>tencent.com/vcuda-memory</code> ：<code>vcuda-memory</code> 是显存，每个单位是256MB的显存</li><li>如果申请的资源为50%利用率，7680MB显存，<code>tencent.com/vcuda-core</code> 填写50，<code>tencent.com/vcuda-memory</code> 填写成30</li><li>同样支持原来的独占卡的方式，只需要在core的地方填写100的整数倍，memory值填写大于0的任意值</li></ul></li><li>基于k8s原生的Scheduler Extender、Extended Resource、DevicePlugin机制来实现</li><li>这个方案同时实现GPU的共享与算力和显存的隔离，类似于阿里云 <a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">cGPU</a> 加上GPUShare 一起使用</li></ul><h4 id="核心组件-1"><a href="#核心组件-1" class="headerlink" title="核心组件"></a>核心组件</h4><p>GaiaGPU的实现主要分为两个部分：Kubernetes 部分 和 vCUDA 部分</p><ul><li>Kubernetes部分基于 Kubernetes 的 Extended Resources、Device Plugin 和 Scheduler Extender机制，实现了下面两个项目<ul><li><a href="https://github.com/tkestack/gpu-manager" target="_blank" rel="external nofollow noopener noreferrer">GPU Manager </a>：实现为一个 Device Plugin，与 NVIDIA 的 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="external nofollow noopener noreferrer">k8s-device-plugin</a> 相比，不需要额外配置 <code>nvidia-docker2</code>，使用的是原生的 <code>runc</code></li><li><a href="https://github.com/tkestack/gpu-admission" target="_blank" rel="external nofollow noopener noreferrer">GPU Admission</a>：实现为一个Scheduler Extender，注意这里的Extender在论文中没有提到，下图中的GPU Scheduler实现的是topology的选卡，属于现在GPU Manager项目的一部分，与这里的调度器插件无关</li></ul></li><li>vCUDA 部分通过 <a href="https://github.com/tkestack/vcuda-controller" target="_blank" rel="external nofollow noopener noreferrer">vcuda-controller</a> 来实现，作为 NVIDIA 的 CUDA 库的封装</li></ul><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-gpu-manager.png"></p><h3 id="具体过程-1"><a href="#具体过程-1" class="headerlink" title="具体过程"></a>具体过程</h3><h4 id="设备资源上报"><a href="#设备资源上报" class="headerlink" title="设备资源上报"></a>设备资源上报</h4><ul><li>与阿里的 <a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="external nofollow noopener noreferrer">GPUShare</a> 一样，GPU Manager 在 <code>ListAndWatch</code> 返回给Kubelet的也不是实际的GPU设备，而是 <code>a list of vGPUs</code>，</li><li>GPU被虚拟化为两个资源维度，memory 和 computing resource<ul><li>memory：以256M内存作为单位，每个memory unit叫做 <code>vmemory</code> device</li><li>computing resource：将一个物理GPU划分为100个 <code>vprocessor</code> devices，每个 <code>vprocessor</code> 占有 1%的GPU利用率</li></ul></li><li>用户申请具有GPU的Pod资源Manifest如下：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">vcuda</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">vcuda-test</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">['/usr/local/nvidia/bin/nvidia-smi']</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="number">50</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-core:</span> <span class="number">50</span></span><br><span class="line">        <span class="attr">tencent.com/vcuda-memory:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png"></p><p> 下面看具体代码，首先是向 <code>kubelet</code> 注册：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *managerImpl)</span> <span class="title">RegisterToKubelet</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">socketFile := filepath.Join(m.config.DevicePluginPath, types.KubeletSocket)</span><br><span class="line">dialOptions := []grpc.DialOption&#123;grpc.WithInsecure(), grpc.WithDialer(utils.UnixDial), grpc.WithBlock(), grpc.WithTimeout(time.Second * <span class="number">5</span>)&#125;</span><br><span class="line"></span><br><span class="line">conn, err := grpc.Dial(socketFile, dialOptions...)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line">client := pluginapi.NewRegistrationClient(conn)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, srv := <span class="keyword">range</span> m.bundleServer &#123;</span><br><span class="line">req := &amp;pluginapi.RegisterRequest&#123;</span><br><span class="line">Version:      pluginapi.Version,</span><br><span class="line">Endpoint:     path.Base(srv.SocketName()),</span><br><span class="line">ResourceName: srv.ResourceName(),</span><br><span class="line">Options:      &amp;pluginapi.DevicePluginOptions&#123;PreStartRequired: <span class="literal">true</span>&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">glog.V(<span class="number">2</span>).Infof(<span class="string">"Register to kubelet with endpoint %s"</span>, req.Endpoint)</span><br><span class="line">_, err = client.Register(context.Background(), req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里有一个 <code>m.bundleServer</code>，分别是 <code>vcore</code> 和 <code>vmemory</code> 的 gRPC Server。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *managerImpl)</span> <span class="title">setupGRPCService</span><span class="params">()</span></span> &#123;</span><br><span class="line">vcoreServer := newVcoreServer(m)</span><br><span class="line">vmemoryServer := newVmemoryServer(m)</span><br><span class="line"></span><br><span class="line">m.bundleServer[types.VCoreAnnotation] = vcoreServer</span><br><span class="line">m.bundleServer[types.VMemoryAnnotation] = vmemoryServer</span><br><span class="line"></span><br><span class="line">displayapi.RegisterGPUDisplayServer(m.srv, m)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来看 <code>ListAndWatch</code> 的实现，对于两种资源，它会去检查 <code>capacity()</code>里面包含对应 <code>resourceName</code> 的：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ListAndWatchWithResourceName send devices for request resource back to server</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">ListAndWatchWithResourceName</span><span class="params">(resourceName <span class="keyword">string</span>, e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">devs := <span class="built_in">make</span>([]*pluginapi.Device, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> ta.capacity() &#123;</span><br><span class="line"><span class="keyword">if</span> strings.HasPrefix(dev.ID, resourceName) &#123;</span><br><span class="line">devs = <span class="built_in">append</span>(devs, dev)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: devs&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// We don't send unhealthy state</span></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">time.Sleep(time.Second)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">glog.V(<span class="number">2</span>).Infof(<span class="string">"ListAndWatch %s exit"</span>, resourceName)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么这里的 <code>ta.capicity()</code> 是如何得到的呢？这里维护了一个拓扑树，树根是物理的Host，树叶是物理的GPU。这里根据树叶上GPU的数目和总的显存大小，构建了 <code>vcore</code> 设备 和 <code>vmemory</code> 设备，命名以各自的资源名为前缀。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">capacity</span><span class="params">()</span> <span class="params">(devs []*pluginapi.Device)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">gpuDevices, memoryDevices []*pluginapi.Device</span><br><span class="line">totalMemory               <span class="keyword">int64</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes := ta.tree.Leaves()</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">totalMemory += <span class="keyword">int64</span>(nodes[i].Meta.TotalMemory)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">totalCores := <span class="built_in">len</span>(nodes) * nvtree.HundredCore</span><br><span class="line">gpuDevices = <span class="built_in">make</span>([]*pluginapi.Device, totalCores)</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; totalCores; i++ &#123;</span><br><span class="line">gpuDevices[i] = &amp;pluginapi.Device&#123;</span><br><span class="line">ID:     fmt.Sprintf(<span class="string">"%s-%d"</span>, types.VCoreAnnotation, i),</span><br><span class="line">Health: pluginapi.Healthy,</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">totalMemoryBlocks := totalMemory / types.MemoryBlockSize</span><br><span class="line">memoryDevices = <span class="built_in">make</span>([]*pluginapi.Device, totalMemoryBlocks)</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">int64</span>(<span class="number">0</span>); i &lt; totalMemoryBlocks; i++ &#123;</span><br><span class="line">memoryDevices[i] = &amp;pluginapi.Device&#123;</span><br><span class="line">ID:     fmt.Sprintf(<span class="string">"%s-%d-%d"</span>, types.VMemoryAnnotation, types.MemoryBlockSize, i),</span><br><span class="line">Health: pluginapi.Healthy,</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">devs = <span class="built_in">append</span>(devs, gpuDevices...)</span><br><span class="line">devs = <span class="built_in">append</span>(devs, memoryDevices...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="调度插件扩展-1"><a href="#调度插件扩展-1" class="headerlink" title="调度插件扩展"></a>调度插件扩展</h4><h5 id="细粒度Quota准入"><a href="#细粒度Quota准入" class="headerlink" title="细粒度Quota准入"></a>细粒度Quota准入</h5><p><code>GPU Quota Admission</code> 作为调度器插件，实现了更细粒度的quota调度准入维度。用户通过配置一个 <code>ConfigMap</code>，对每个 <code>Namespace</code>可用的GPU卡的配额做规划，同时也定义了资源池，这样在调度的时候就可以实现按照资源池及GPU型号进行策略调度。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"A"</span>: &#123;</span><br><span class="line">    <span class="attr">"pool"</span>: [<span class="string">"public"</span>], <span class="comment">// Pods in namespace 'A' could use pool 'public'</span></span><br><span class="line">    <span class="attr">"quota"</span>: &#123;</span><br><span class="line">      <span class="attr">"M40"</span>: <span class="number">2</span>,</span><br><span class="line">      <span class="attr">"P100"</span>: <span class="number">3</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"B"</span>: &#123;</span><br><span class="line">    <span class="attr">"pool"</span>: [ <span class="string">"wx"</span> ], <span class="comment">// Pods in namespace 'B' could use pool 'wx'</span></span><br><span class="line">    <span class="attr">"quota"</span>: &#123;</span><br><span class="line">      <span class="attr">"M40"</span>: <span class="number">8</span>,</span><br><span class="line">      <span class="attr">"P100"</span>: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体在调度的时候，对每一个Pod，根据Namespace可以筛选出一系列含有GPU的Pods，然后当前Namespace下，对于某种GPU Model（比如P100），计算已经使用了的GPU大小，根据 <code>ConfigMap</code> 定义的配额，找到没超出。通过这个，得到所有没超出Quota的Models。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> NamespaceQuota <span class="keyword">struct</span> &#123;</span><br><span class="line">Quota <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span> <span class="string">`json:"quota"`</span></span><br><span class="line">Pool []<span class="keyword">string</span> <span class="string">`json:"pool"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gpuFilter *GPUFilter)</span> <span class="title">filterGPUModel</span><span class="params">(pod *corev1.Pod, namespaceQuota NamespaceQuota)</span> <span class="params">([]<span class="keyword">string</span>, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> filteredGPUModels []<span class="keyword">string</span></span><br><span class="line"><span class="keyword">for</span> gpuModel, limit := <span class="keyword">range</span> namespaceQuota.Quota &#123;</span><br><span class="line">limit = limit * VirtualGPUTimes</span><br><span class="line">  nodeSelector, err := metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">MatchLabels: <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>&#123;gpuFilter.conf.GPUModelLabel: gpuModel&#125;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">pods, err := gpuFilter.listPodsOnNodes(nodeSelector, pod.Namespace)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">gpuUsed := calculateGPUUsage(<span class="built_in">append</span>(pods, pod))</span><br><span class="line"><span class="keyword">if</span> gpuUsed &lt;= limit &#123;</span><br><span class="line">filteredGPUModels = <span class="built_in">append</span>(filteredGPUModels, gpuModel)</span><br><span class="line">&#125;</span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">"Pods in namespace %s will use %d %s GPU cards after adding this pod, quota is %d"</span>,</span><br><span class="line">pod.Namespace, gpuUsed, gpuModel, limit)</span><br><span class="line">&#125;</span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">"These GPU models could be used by pod %s: %+v"</span>, pod.Name, filteredGPUModels)</span><br><span class="line"><span class="keyword">return</span> filteredGPUModels, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来在 Filter阶段，根据上面的可用 <code>GPU Models</code> 和定义的 <code>Quota Pool</code>，</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(gpuFilter *GPUFilter)</span> <span class="title">filterNodes</span><span class="params">(nodes []corev1.Node, gpuModels, pools []<span class="keyword">string</span>)</span> <span class="params">(filteredNodes []corev1.Node, failedNodesMap schedulerapi.FailedNodesMap, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> gpuModelSelector, poolSelector labels.Selector</span><br><span class="line"></span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">"Filter nodes with gpuModels(%+v) and pools(%+v)"</span>, gpuModels, pools)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(gpuModels) != <span class="number">0</span> &#123;</span><br><span class="line">gpuModelSelector, err = metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">MatchExpressions: []metav1.LabelSelectorRequirement&#123;&#123;</span><br><span class="line">Key:      gpuFilter.conf.GPUModelLabel,</span><br><span class="line">Operator: metav1.LabelSelectorOpIn,</span><br><span class="line">Values:   gpuModels,</span><br><span class="line">&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">gpuModelSelector = labels.Nothing()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// If pool is empty, it means that pod could use every pool, it is OK to leave it as a empty selector.</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pools) != <span class="number">0</span> &#123;</span><br><span class="line">poolSelector, err = metav1.LabelSelectorAsSelector(&amp;metav1.LabelSelector&#123;</span><br><span class="line">MatchExpressions: []metav1.LabelSelectorRequirement&#123;&#123;</span><br><span class="line">Key:      gpuFilter.conf.GPUPoolLabel,</span><br><span class="line">Operator: metav1.LabelSelectorOpIn,</span><br><span class="line">Values:   pools,</span><br><span class="line">&#125;&#125;&#125;)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">poolSelector = labels.Everything()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">failedNodesMap = schedulerapi.FailedNodesMap&#123;&#125;</span><br><span class="line"><span class="keyword">for</span> _, node := <span class="keyword">range</span> nodes &#123;</span><br><span class="line"><span class="keyword">if</span> gpuModelSelector.Matches(labels.Set(node.Labels)) &amp;&amp; poolSelector.Matches(labels.Set(node.Labels)) &#123;</span><br><span class="line">filteredNodes = <span class="built_in">append</span>(filteredNodes, node)</span><br><span class="line">glog.V(<span class="number">5</span>).Infof(<span class="string">"Add %s to filteredNodes"</span>, node.Name)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">failedNodesMap[node.Name] = <span class="string">"ExceedsGPUQuota"</span></span><br><span class="line">glog.V(<span class="number">5</span>).Infof(<span class="string">"Add %s to failedNodesMap"</span>, node.Name)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> filteredNodes, failedNodesMap, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到这一步，也就是实现了细粒度的Quota调度准入控制。</p><h5 id="避免GPU碎片化"><a href="#避免GPU碎片化" class="headerlink" title="避免GPU碎片化"></a>避免GPU碎片化</h5><p>为此我们增加了GPU predicate controller来尽可能的降低系统默认调度策略带来的碎片化问题。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-19_gpu-manager-predicate.png"></p><p>我们看看它是如何实现的，首先在 <code>deviceFilter</code>的入口里面，拿到当前Node上存在的所有Pod：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pods, err := gpuFilter.ListPodsOnNode(node)</span><br><span class="line">...</span><br><span class="line">nodeInfo := device.NewNodeInfo(node, pods)</span><br><span class="line">alloc := algorithm.NewAllocator(nodeInfo)</span><br><span class="line">newPod, err := alloc.Allocate(pod)</span><br></pre></td></tr></table></figure><p>接下来构建一个 <code>NodeInfo</code> 结构体，里面包含有当前Node的所有信息，这里记录了Node上所有的GPU显存和GPU设备数目。这个是通过Node Status里面两个扩展资源计算出来的。<strong>GPU Manager 方案也是认为每台机器上的GPU的不同卡的显存大小是相同的，这样可以算出每张卡的显存大小</strong>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> NodeInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">name        <span class="keyword">string</span></span><br><span class="line">node        *v1.Node</span><br><span class="line">devs        <span class="keyword">map</span>[<span class="keyword">int</span>]*DeviceInfo</span><br><span class="line">deviceCount <span class="keyword">int</span></span><br><span class="line">totalMemory <span class="keyword">uint</span></span><br><span class="line">usedCore    <span class="keyword">uint</span></span><br><span class="line">usedMemory  <span class="keyword">uint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>NodeInfo</code> 里面还有一个 <code>DeviceInfo</code> 的map，用于记录每张卡的使用情况。这里在初始化这个 <code>NodeInfo</code> 数据结构的时候也会根据传入的 <code>pods</code> 信息更新 <code>DeviceInfo</code> 的设备使用情况。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> DeviceInfo <span class="keyword">struct</span> &#123;</span><br><span class="line">id          <span class="keyword">int</span></span><br><span class="line">totalMemory <span class="keyword">uint</span></span><br><span class="line">usedMemory  <span class="keyword">uint</span></span><br><span class="line">usedCore    <span class="keyword">uint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来就是每个 <code>Allocate</code> 函数的实现，对于Pod里面的每一个容器，都会分配得到一个 <code>devIDs</code> 列表，然后得到对Pod打上Annotation：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *allocator)</span> <span class="title">Allocate</span><span class="params">(pod *v1.Pod)</span> <span class="params">(*v1.Pod, error)</span></span> &#123;</span><br><span class="line">newPod := pod.DeepCopy()</span><br><span class="line"><span class="keyword">for</span> i, c := <span class="keyword">range</span> newPod.Spec.Containers &#123;</span><br><span class="line"><span class="keyword">if</span> !util.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line">devIDs := []<span class="keyword">string</span>&#123;&#125;</span><br><span class="line">devs, err := alloc.AllocateOne(&amp;c)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Infof(<span class="string">"failed to allocate for pod %s(%s)"</span>, newPod.Name, c.Name)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> devs &#123;</span><br><span class="line">devIDs = <span class="built_in">append</span>(devIDs, strconv.Itoa(dev.GetID()))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> newPod.Annotations == <span class="literal">nil</span> &#123;</span><br><span class="line">newPod.Annotations = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line">newPod.Annotations[util.PredicateGPUIndexPrefix+strconv.Itoa(i)] = strings.Join(devIDs, <span class="string">","</span>)</span><br><span class="line">&#125;</span><br><span class="line">newPod.Annotations[util.GPUAssigned] = <span class="string">"false"</span></span><br><span class="line">newPod.Annotations[util.PredicateTimeAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, time.Now().UnixNano())</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> newPod, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来的问题就是，这里的 <code>AllocateOne</code> 是如何实现的呢？对于每个容器，根据其申请的GPU资源，可以分为GPU是共享模式还是独占模式，然后调用 <code>Evaluate</code>去得到 <code>devs</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *allocator)</span> <span class="title">AllocateOne</span><span class="params">(container *v1.Container)</span> <span class="params">([]*device.DeviceInfo, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">devs           []*device.DeviceInfo</span><br><span class="line">sharedMode     <span class="keyword">bool</span></span><br><span class="line">vcore, vmemory <span class="keyword">uint</span></span><br><span class="line">)</span><br><span class="line">node := alloc.nodeInfo.GetNode()</span><br><span class="line">nodeTotalMemory := util.GetCapacityOfNode(node, util.VMemoryAnnotation)</span><br><span class="line">deviceCount := util.GetGPUDeviceCountOfNode(node)</span><br><span class="line">deviceTotalMemory := <span class="keyword">uint</span>(nodeTotalMemory / deviceCount)</span><br><span class="line">needCores := util.GetGPUResourceOfContainer(container, util.VCoreAnnotation)</span><br><span class="line">needMemory := util.GetGPUResourceOfContainer(container, util.VMemoryAnnotation)</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> needCores &lt; util.HundredCore:</span><br><span class="line">eval := NewShareMode(alloc.nodeInfo)</span><br><span class="line">devs = eval.Evaluate(needCores, needMemory)</span><br><span class="line">sharedMode = <span class="literal">true</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">eval := NewExclusiveMode(alloc.nodeInfo)</span><br><span class="line">devs = eval.Evaluate(needCores, needMemory)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(devs) == <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"failed to allocate for container %s"</span>, container.Name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> sharedMode &#123;</span><br><span class="line">vcore = needCores</span><br><span class="line">vmemory = needMemory</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">vcore = util.HundredCore</span><br><span class="line">vmemory = deviceTotalMemory</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> devs &#123;</span><br><span class="line">err := alloc.nodeInfo.AddUsedResources(dev.GetID(), vcore, vmemory)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Infof(<span class="string">"failed to update used resource for node %s dev %d due to %v"</span>, node.Name, dev.GetID(), err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> devs, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以共享模式为例，这里拿到当前Node的所有 <code>Device</code>，分别根据最少可用的<code>cores</code>和可用的<code>memory</code>来排序，如果有满足用户需要的设备，则加入到 <code>devs</code> 里面，最后将这个 <code>list</code> 返回给用户。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(al *shareMode)</span> <span class="title">Evaluate</span><span class="params">(cores <span class="keyword">uint</span>, memory <span class="keyword">uint</span>)</span> []*<span class="title">device</span>.<span class="title">DeviceInfo</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">devs        []*device.DeviceInfo</span><br><span class="line">deviceCount = al.node.GetDeviceCount()</span><br><span class="line">tmpStore    = <span class="built_in">make</span>([]*device.DeviceInfo, deviceCount)</span><br><span class="line">sorter      = shareModeSort(device.ByAllocatableCores, device.ByAllocatableMemory, device.ByID)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; deviceCount; i++ &#123;</span><br><span class="line">tmpStore[i] = al.node.GetDeviceMap()[i]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sorter.Sort(tmpStore)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> tmpStore &#123;</span><br><span class="line"><span class="keyword">if</span> dev.AllocatableCores() &gt;= cores &amp;&amp; dev.AllocatableMemory() &gt;= memory &#123;</span><br><span class="line">glog.V(<span class="number">4</span>).Infof(<span class="string">"Pick up %d , cores: %d, memory: %d"</span>, dev.GetID(), dev.AllocatableCores(), dev.AllocatableMemory())</span><br><span class="line">devs = <span class="built_in">append</span>(devs, dev)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> devs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这里在调度过程中，选择最先满足的那个，一旦满足则跳出选择。这是因为这里的 <code>devs</code> 已经按照最少可用的资源来匹配了，通过这种方式可以减少碎片化。</p><h4 id="Kubelet创建Pod-1"><a href="#Kubelet创建Pod-1" class="headerlink" title="Kubelet创建Pod"></a>Kubelet创建Pod</h4><p>用户创建Pod之后，经过调度找到对应的Node，这时候Kubelet向DevicePlugin执行Allocate函数。因为Kubelet看到的是虚拟的Devices，这里需要有一个从虚拟Device到实际GPU Device的映射，这里就是上图中GPU Manager做的事情，然后发送一个Request给GPU Scheduler，根据拓扑关系选择最合适的GPU，然后GPU Manager将 AllocateResponse返回给Kubelet。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-device-plugin.png"></p><p>我们先看 <code>Allocate</code> 的实现，这段代码比较长，但是实现的逻辑也不难：</p><ul><li>Allocate传入的参数是 <code>deviceIDs</code> 这样里一个List，<strong>里面只有 <code>vcore</code> 这种设备</strong> （代码是这样的，需要进一步看一看 kubelet）</li><li>Pod可能有多个Container，这里每次只处理一个容器<ul><li>如果还有未处理的Pod，先解决未处理Pod中的容器</li><li>否则从当前Node上的Pod遍历，选择与用户申请的 <code>vcore</code> 相同的容器</li></ul></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ta *NvidiaTopoAllocator)</span> <span class="title">Allocate</span><span class="params">(_ context.Context, reqs *pluginapi.AllocateRequest)</span> <span class="params">(*pluginapi.AllocateResponse, error)</span></span> &#123;</span><br><span class="line">ta.Lock()</span><br><span class="line"><span class="keyword">defer</span> ta.Unlock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">reqCount           <span class="keyword">uint</span></span><br><span class="line">candidatePod       *v1.Pod</span><br><span class="line">candidateContainer *v1.Container</span><br><span class="line">found              <span class="keyword">bool</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(reqs.ContainerRequests) &lt; <span class="number">1</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"empty container request"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// k8s send allocate request for one container at a time</span></span><br><span class="line">req := reqs.ContainerRequests[<span class="number">0</span>]</span><br><span class="line">resps := &amp;pluginapi.AllocateResponse&#123;&#125;</span><br><span class="line">reqCount = <span class="keyword">uint</span>(<span class="built_in">len</span>(req.DevicesIDs))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ta.unfinishedPod != <span class="literal">nil</span> &#123;</span><br><span class="line">candidatePod = ta.unfinishedPod</span><br><span class="line">cache := ta.allocatedPod.GetCache(<span class="keyword">string</span>(candidatePod.UID))</span><br><span class="line"><span class="keyword">for</span> i, c := <span class="keyword">range</span> candidatePod.Spec.Containers &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := cache[c.Name]; ok &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> !utils.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> reqCount != utils.GetGPUResourceOfContainer(&amp;candidatePod.Spec.Containers[i], types.VCoreAnnotation) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">&#125;</span><br><span class="line">candidateContainer = &amp;candidatePod.Spec.Containers[i]</span><br><span class="line">found = <span class="literal">true</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">pods, err := getCandidatePods(ta.k8sClient, ta.config.Hostname)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"Failed to find candidate pods due to %v"</span>, err)</span><br><span class="line">glog.Infof(msg)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> i, c := <span class="keyword">range</span> pod.Spec.Containers &#123;</span><br><span class="line"><span class="keyword">if</span> !utils.IsGPURequiredContainer(&amp;c) &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line">podCache := ta.allocatedPod.GetCache(<span class="keyword">string</span>(pod.UID))</span><br><span class="line"><span class="keyword">if</span> podCache != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := podCache[c.Name]; ok &#123;</span><br><span class="line">glog.Infof(<span class="string">"container %s of pod %s has been allocate, continue to next"</span>, c.Name, pod.UID)</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> utils.GetGPUResourceOfContainer(&amp;pod.Spec.Containers[i], types.VCoreAnnotation) == reqCount &#123;</span><br><span class="line">glog.Infof(<span class="string">"Found candidate Pod %s(%s) with device count %d"</span>, pod.UID, c.Name, reqCount)</span><br><span class="line">candidatePod = pod</span><br><span class="line">candidateContainer = &amp;pod.Spec.Containers[i]</span><br><span class="line">found = <span class="literal">true</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>找到这样的一个容器之后，拿到容器申请的 <code>vmemory</code>，每一个虚拟的 <code>vmemory</code> 作为一个设备加入到 <code>req.DevicesIDs</code> 中，继续调用 <code>allocateOne</code>:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line"><span class="comment">// get vmemory info from container spec</span></span><br><span class="line">vmemory := utils.GetGPUResourceOfContainer(candidateContainer, types.VMemoryAnnotation)</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="keyword">int</span>(vmemory); i++ &#123;</span><br><span class="line">req.DevicesIDs = <span class="built_in">append</span>(req.DevicesIDs, types.VMemoryAnnotation)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp, err := ta.allocateOne(candidatePod, candidateContainer, req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">glog.Errorf(err.Error())</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">resps.ContainerResponses = <span class="built_in">append</span>(resps.ContainerResponses, resp)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">msg := fmt.Sprintf(<span class="string">"candidate pod not found for request %v, allocation failed"</span>, reqs)</span><br><span class="line">glog.Infof(msg)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(msg)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> resps, ni</span><br></pre></td></tr></table></figure><p>具体的 <code>Allocate</code> 实现在 <code>allocateOne</code> 里面，根据Pod计算出其申请的 <code>needCores</code> 和 <code>needMemory</code> 之后，根据三种情况有不同的分配策略。注意这里还是在拓扑树上面操作，拓扑树树根是物理的Host，树叶是物理的GPU</p><ul><li>申请的资源超过一张卡，这时候分配的策略是尽可能减少卡之间的通信开销</li><li>申请的资源等于一张卡，这时候的分配策略是尽可能减少拓扑树里面产生没有兄弟节点的叶节点</li><li>申请的资源小于一张卡，这时候的分配策略是尽可能减少卡资源的碎片化</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> needCores &gt; nvtree.HundredCore:</span><br><span class="line">eval, ok := ta.evaluators[<span class="string">"link"</span>]</span><br><span class="line"><span class="comment">// 这种场景下needCores must be multiple of nvtree.HundredCore</span></span><br><span class="line">nodes = eval.Evaluate(needCores, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">case</span> needCores == nvtree.HundredCore:</span><br><span class="line">eval, ok := ta.evaluators[<span class="string">"fragment"</span>]</span><br><span class="line">nodes = eval.Evaluate(needCores, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="comment">// evaluate in share mode</span></span><br><span class="line">shareMode = <span class="literal">true</span></span><br><span class="line">eval, ok := ta.evaluators[<span class="string">"share"</span>]</span><br><span class="line">nodes = eval.Evaluate(needCores, needMemory)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>这里的 <code>Evaluate</code> 返回的是 <code>NvidiaNode</code> 这样的 GPU 节点，通过这个结构可以构建一个拓扑树：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//NvidiaNode represents a node of Nvidia GPU</span></span><br><span class="line"><span class="keyword">type</span> NvidiaNode <span class="keyword">struct</span> &#123;</span><br><span class="line">Meta            DeviceMeta</span><br><span class="line">AllocatableMeta SchedulerCache</span><br><span class="line"></span><br><span class="line">Parent   *NvidiaNode</span><br><span class="line">Children []*NvidiaNode</span><br><span class="line">Mask     <span class="keyword">uint32</span></span><br><span class="line"></span><br><span class="line">pendingReset <span class="keyword">bool</span></span><br><span class="line">vchildren    <span class="keyword">map</span>[<span class="keyword">int</span>]*NvidiaNode</span><br><span class="line">ntype        nvml.GpuTopologyLevel</span><br><span class="line">tree         *NvidiaTree</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于这里具体的分配算法此处就不再详述了，抓住主脉络。</p><p>接下来构建 <code>pluginapi.ContainerAllocateResponse</code>，这里会分别设置环境变量，挂载的目录，找到的设备，以及<code>Annotation</code>：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ctntResp := &amp;pluginapi.ContainerAllocateResponse&#123;</span><br><span class="line">Envs:        <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>),</span><br><span class="line">Mounts:      <span class="built_in">make</span>([]*pluginapi.Mount, <span class="number">0</span>),</span><br><span class="line">Devices:     <span class="built_in">make</span>([]*pluginapi.DeviceSpec, <span class="number">0</span>),</span><br><span class="line">Annotations: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先是 <code>Devices</code> 字段：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">allocatedDevices := sets.NewString()</span><br><span class="line">deviceList := <span class="built_in">make</span>([]<span class="keyword">string</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> _, n := <span class="keyword">range</span> nodes &#123;</span><br><span class="line">name := n.MinorName()</span><br><span class="line">glog.V(<span class="number">2</span>).Infof(<span class="string">"Allocate %s for %s(%s), Meta (%d:%d)"</span>, name, pod.UID, container.Name, n.Meta.ID, n.Meta.MinorID)</span><br><span class="line"></span><br><span class="line">ctntResp.Annotations[types.VCoreAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, needCores)</span><br><span class="line">ctntResp.Annotations[types.VMemoryAnnotation] = fmt.Sprintf(<span class="string">"%d"</span>, needMemory)</span><br><span class="line"></span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">ContainerPath: name,</span><br><span class="line">HostPath:      name,</span><br><span class="line">Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line">deviceList = <span class="built_in">append</span>(deviceList, n.Meta.UUID)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> !allocated &#123;</span><br><span class="line">ta.tree.MarkOccupied(n, needCores, needMemory)</span><br><span class="line">&#125;</span><br><span class="line">allocatedDevices.Insert(name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里还有一些控制设备：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Append control device</span></span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">ContainerPath: types.NvidiaCtlDevice,</span><br><span class="line">HostPath:      types.NvidiaCtlDevice,</span><br><span class="line">Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">ContainerPath: types.NvidiaUVMDevice,</span><br><span class="line">HostPath:      types.NvidiaUVMDevice,</span><br><span class="line">Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Append default device</span></span><br><span class="line"><span class="keyword">if</span> cfg, found := ta.extraConfig[<span class="string">"default"</span>]; found &#123;</span><br><span class="line"><span class="keyword">for</span> _, dev := <span class="keyword">range</span> cfg.Devices &#123;</span><br><span class="line">ctntResp.Devices = <span class="built_in">append</span>(ctntResp.Devices, &amp;pluginapi.DeviceSpec&#123;</span><br><span class="line">ContainerPath: dev,</span><br><span class="line">HostPath:      dev,</span><br><span class="line">Permissions:   <span class="string">"rwm"</span>,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着是 <code>Annotations</code> 字段：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ctntResp.Annotations[types.VDeviceAnnotation] = vDeviceAnnotationStr(nodes)</span><br><span class="line"><span class="keyword">if</span> !allocated &#123;</span><br><span class="line">ta.allocatedPod.Insert(<span class="keyword">string</span>(pod.UID), container.Name, &amp;cache.Info&#123;</span><br><span class="line">Devices: allocatedDevices.UnsortedList(),</span><br><span class="line">Cores:   needCores,</span><br><span class="line">Memory:  needMemory,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后是 <code>Envs</code> 字段</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LD_LIBRARY_PATH</span></span><br><span class="line">ctntResp.Envs[<span class="string">"LD_LIBRARY_PATH"</span>] = <span class="string">"/usr/local/nvidia/lib64"</span></span><br><span class="line"><span class="keyword">for</span> _, env := <span class="keyword">range</span> container.Env &#123;</span><br><span class="line"><span class="keyword">if</span> env.Name == <span class="string">"compat32"</span> &amp;&amp; strings.ToLower(env.Value) == <span class="string">"true"</span> &#123;</span><br><span class="line">ctntResp.Envs[<span class="string">"LD_LIBRARY_PATH"</span>] = <span class="string">"/usr/local/nvidia/lib"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NVIDIA_VISIBLE_DEVICES</span></span><br><span class="line">ctntResp.Envs[<span class="string">"NVIDIA_VISIBLE_DEVICES"</span>] = strings.Join(deviceList, <span class="string">","</span>)</span><br></pre></td></tr></table></figure><p>最后是 <code>Mounts</code> 字段，这里给GPU容器配置一个volume挂载点来提供CUDA Library以及配置环境变量<code>LD_LIBRARY_PATH</code> 告诉应用哪里去找到 <code>CUDA Library</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> shareMode &#123;</span><br><span class="line">ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">ContainerPath: <span class="string">"/usr/local/nvidia"</span>,</span><br><span class="line">HostPath:      types.DriverLibraryPath,</span><br><span class="line">ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">&#125;)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">ContainerPath: <span class="string">"/usr/local/nvidia"</span>,</span><br><span class="line">HostPath:      types.DriverOriginLibraryPath,</span><br><span class="line">ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ctntResp.Mounts = <span class="built_in">append</span>(ctntResp.Mounts, &amp;pluginapi.Mount&#123;</span><br><span class="line">ContainerPath: types.VCUDA_MOUNTPOINT,</span><br><span class="line">HostPath:      filepath.Join(ta.config.VirtualManagerPath, <span class="keyword">string</span>(pod.UID)),</span><br><span class="line">ReadOnly:      <span class="literal">true</span>,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="vGPU-Manager"><a href="#vGPU-Manager" class="headerlink" title="vGPU Manager"></a>vGPU Manager</h4><p><code>vGPU Manager</code> 作为 <code>GPU Manager</code> 这个 <code>DaemonSet</code> 的一部分，负责下发容器配置和监控容器分配的vGPU。上一步在拓扑分配器确定好每个容器的资源配置之后，<code>vGPU Manager</code> 负责为每个容器在 host 上创建一个独立的目录，这个目录以容器的名称命名，并且会被包括在 <code>AllocateResponse</code> 中返回给 kubelet，对就是上面那段代码做的事情。</p><p><code>vGPU Manager</code> 会维护一个使用了GPU的并且仍然活着的容器列表，还会去周期性的检查他们。一旦有容器挂掉，就会将这个容器移出列表并且删去目录。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//                Host                     |                Container</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//  .-----------.                          |</span></span><br><span class="line"><span class="comment">//  | allocator |----------.               |             ___________</span></span><br><span class="line"><span class="comment">//  '-----------'   PodUID |               |             \          \</span></span><br><span class="line"><span class="comment">//                         v               |              ) User App )--------.</span></span><br><span class="line"><span class="comment">//                .-----------------.      |             /__________/         |</span></span><br><span class="line"><span class="comment">//     .----------| virtual-manager |      |                                  |</span></span><br><span class="line"><span class="comment">//     |          '-----------------'      |                                  |</span></span><br><span class="line"><span class="comment">// $VirtualManagerPath/PodUID              |                                  |</span></span><br><span class="line"><span class="comment">//     |                                   |       read /proc/self/cgroup     |</span></span><br><span class="line"><span class="comment">//     |  .------------------.             |       to get PodUID, ContainerID |</span></span><br><span class="line"><span class="comment">//     '-&gt;| create directory |------.      |                                  |</span></span><br><span class="line"><span class="comment">//        '------------------'      |      |                                  |</span></span><br><span class="line"><span class="comment">//                                  |      |                                  |</span></span><br><span class="line"><span class="comment">//                 .----------------'      |       .----------------------.   |</span></span><br><span class="line"><span class="comment">//                 |                       |       | fork call gpu-client |&lt;--'</span></span><br><span class="line"><span class="comment">//                 |                       |       '----------------------'</span></span><br><span class="line"><span class="comment">//                 v                       |                   |</span></span><br><span class="line"><span class="comment">//    .------------------------.           |                   |</span></span><br><span class="line"><span class="comment">//   ( wait for client register )&lt;-------PodUID, ContainerID---'</span></span><br><span class="line"><span class="comment">//    '------------------------'           |</span></span><br><span class="line"><span class="comment">//                 |                       |</span></span><br><span class="line"><span class="comment">//                 v                       |</span></span><br><span class="line"><span class="comment">//   .--------------------------.          |</span></span><br><span class="line"><span class="comment">//   | locate pod and container |          |</span></span><br><span class="line"><span class="comment">//   '--------------------------'          |</span></span><br><span class="line"><span class="comment">//                 |                       |</span></span><br><span class="line"><span class="comment">//                 v                       |</span></span><br><span class="line"><span class="comment">//   .---------------------------.         |</span></span><br><span class="line"><span class="comment">//   | write down configure and  |         |</span></span><br><span class="line"><span class="comment">//   | pid file with containerID |         |</span></span><br><span class="line"><span class="comment">//   | as name                   |         |</span></span><br><span class="line"><span class="comment">//   '---------------------------'         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         |</span></span><br><span class="line"><span class="comment">//                                         v</span></span><br></pre></td></tr></table></figure><h3 id="vGPU-Library"><a href="#vGPU-Library" class="headerlink" title="vGPU Library"></a>vGPU Library</h3><p>论文中的 <code>vGPU Library</code>，具体实现为 <a href="https://github.com/tkestack/vcuda-controller" target="_blank" rel="external nofollow noopener noreferrer">vcuda-controller</a> ，它运行在容器中用于管理部署在容器中的GPU资源。这个 <code>vGPU Library</code> 本质上就是自己封装了 <code>CUDA Library</code>，劫持了 <code>memory-related</code> API 和 <code>computing-related</code> API，下表显示了劫持的API。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_gaia-vcuda.png"></p><p><code>vCUDA</code> 在调用相应API时检查：</p><ul><li>对于显存，一旦该任务申请显存后占用的显存大小大于config中的设置，就报错。</li><li>对于计算资源，存在硬隔离和软隔离两种方式<ul><li>共同点是当任务使用的GPU SM利用率超出资源上限，则暂缓下发API调用。</li><li>不同点是如果有资源空闲，软隔离允许任务超过设置，动态计算资源上限。而硬隔离则不允许超出设置量。</li></ul></li></ul><p>这里对于其具体实现按下不表。</p><p>一个令人疑惑的问题是，在GPU Manager中，用户的容器是如何能够使用这个动态库的呢？具体有两个问题：</p><ul><li>这个库从哪里来？<ul><li><code>GPU Manager</code> 作为 <code>DaemonSet</code> 会在其Image中将我们自定义的库打包进去，然后挂载到Node上的一个目录。</li></ul></li><li>容器中的应用是如何感知到的？<ul><li>这里主要是通过在创建容器的时候，设置 <code>LD_LIBRARY_PATH</code> ，将其指向这个自定义的动态库的地址。</li></ul></li></ul><h3 id="资源监控统计"><a href="#资源监控统计" class="headerlink" title="资源监控统计"></a>资源监控统计</h3><p>这部分代码还没有看。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/designs/designs.md" target="_blank" rel="external nofollow noopener noreferrer">阿里GPUShare设计文档</a></li><li><a href="https://www.alibabacloud.com/help/zh/doc-detail/163994.htm" target="_blank" rel="external nofollow noopener noreferrer">阿里共享调度使用文档</a></li><li><a href="https://ieeexplore.ieee.org/document/8672318" target="_blank" rel="external nofollow noopener noreferrer">Gaia GPUManager论文</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原生的 k8s 基于 &lt;code&gt;Device Plugin&lt;/code&gt; 和 &lt;code&gt;Extended Resource&lt;/code&gt; 机制实现了在容器中使用GPU，但是只支持GPU的独占使用，不允许在Pod间共享GPU，这大大降低了对集群中GPU的利用率。为了在集群层面共享GPU，我们需要实现GPU资源的隔离与调度，本文将依次介绍阿里的 &lt;a href=&quot;https://github.com/AliyunContainerService/gpushare-scheduler-extender&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;GPUShare&lt;/a&gt; 与腾讯的 &lt;a href=&quot;https://github.com/tkestack/gpu-manager&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;GPUManager&lt;/a&gt;，分析其实现机制。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_aliyun-gpu-share.jpg" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="k8s" scheme="http://houmin.cc/tags/k8s/"/>
    
      <category term="GPU" scheme="http://houmin.cc/tags/GPU/"/>
    
      <category term="资源隔离" scheme="http://houmin.cc/tags/%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB/"/>
    
      <category term="scheduler extender" scheme="http://houmin.cc/tags/scheduler-extender/"/>
    
      <category term="device plugin" scheme="http://houmin.cc/tags/device-plugin/"/>
    
  </entry>
  
  <entry>
    <title>【Kubernetes】在Docker中使用GPU</title>
    <link href="http://houmin.cc/posts/574111db/"/>
    <id>http://houmin.cc/posts/574111db/</id>
    <published>2020-11-17T12:45:00.000Z</published>
    <updated>2020-11-23T05:29:01.884Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>我们在 <a href="https://houmin.cc/posts/5004f8e5/">GPU 与 CUDA 编程入门</a> 这篇博客中初步介绍了如何Linux上使用GPU的方法，随着容器和k8s的迅猛发展，人们对于在容器中使用GPU的需求越发强烈。本文将基于前文，继续介绍如何在容器中使用GPU，进一步地，介绍在Kubernetes中如何调度GPU，并以Tensorflow为例，介绍如何基于Docker搭建部署了GPU的深度学习开发环境。</p><a id="more"></a><h2 id="NVIDIA-Container-Toolkit"><a href="#NVIDIA-Container-Toolkit" class="headerlink" title="NVIDIA Container Toolkit"></a>NVIDIA Container Toolkit</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>容器最早是用于无缝部署基于CPU的应用，它们对于硬件和平台是无感知的，但是显然这种使用场景对于GPU并不适用。对于不同的GPU，需要机器安装不同的硬件驱动，这极大限制了在容器中使用GPU。为了解决这个问题，最早的一种使用方法是在容器中完全重新安装一次NVIDIA驱动，然后将在容器启动的时候将GPU以字符设备 <code>/dev/nvidia0</code> 的方式传递给容器。然而这种方法要求容器中安装的驱动版本与Host上的驱动版本完全一致，同一个Docker Image不能在各个机器上复用，这极大的限制了容器的扩展性。</p><p>为了解决上述问题，容器必须对于 NVIDIA 驱动是无感知的，基于此 NVIDIA 推出了 <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA Container Toolkit</a>：</p><p><img alt="nvidia-gpu-docker" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_nvidia-gpu-docker.png"></p><p>如上图所示， NVIDIA 将原来 CUDA 应用依赖的API环境划分为两个部分：</p><ul><li>驱动级API：由<code>libcuda.so.major.minor</code>动态库和内核module提供支持，图中表示为CUDA Driver<ul><li>驱动级API属于底层API，每当NVIDIA公司释放出某一个版本的驱动时，如果你要升级主机上的驱动，那么内核模块和<code>libcuda.so.major.minor</code>这2个文件就必须同时升级到同一个版本，这样原有的程序才能正常工作,</li><li>不同版本的驱动不能同时存在于宿主机上</li></ul></li><li>非驱动级API：由动态库<code>libcublas.so</code>等用户空间级别的API组成，图中表示为CUDA Toolkit<ul><li>非驱动级API的版本号是以Toolkit自身的版本号来管理, 比如cuda-10，cuda-11</li><li>不同版本的Toolkit可以同时运行在相同的宿主机上</li><li>非驱动级API算是对驱动级API的一种更高级的封装,最终还是要调用驱动级API来实现功能</li></ul></li></ul><p>为了让使用GPU的容器更具可扩展性，关于非驱动级的API被 NVIDIA 打包进了  <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA Container Toolkit</a>，因此在容器中使用GPU之前，每个机器需要先安装好NVIDIA驱动，之后配置好 NVIDIA Container Toolkit之后，就可以在容器中方便使用GPU了。</p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>NVIDIA 的容器工具包本质是使用一个<code>nvidia-runc</code>的方式来提供GPU容器的创建, 在用户创建出来的OCI spec上补上几个hook函数，来达到GPU设备运行的准备工作。具体包括以下几个组件，从上到下展示如图：</p><ul><li><code>nvidia-docker2</code></li><li><code>nvidia-container-runtime</code></li><li><code>nvidia-container-toolkit</code></li><li><code>libnvidia-container</code></li></ul><p><img alt data-src="https://docs.nvidia.com/datacenter/cloud-native/_images/nvidia-docker-arch.png"></p><p>下面对这几个组件依次介绍：</p><h4 id="libnvidia-container"><a href="#libnvidia-container" class="headerlink" title="libnvidia-container"></a><code>libnvidia-container</code></h4><p>This component provides a library and a simple CLI utility to automatically configure GNU/Linux containers leveraging NVIDIA GPUs. The implementation relies on kernel primitives and is designed to be agnostic of the container runtime.</p><p><code>libnvidia-container</code> provides a well-defined API and a wrapper CLI (called <code>nvidia-container-cli</code>) that different runtimes can invoke to inject NVIDIA GPU support into their containers.</p><h4 id="nvidia-container-toolkit"><a href="#nvidia-container-toolkit" class="headerlink" title="nvidia-container-toolkit"></a><code>nvidia-container-toolkit</code></h4><p>This component includes a script that implements the interface required by a <code>runC</code> <code>prestart</code> hook. This script is invoked by <code>runC</code> after a container has been created, but before it has been started, and is given access to the <code>config.json</code> associated with the container (e.g. this <a href="https://github.com/opencontainers/runtime-spec/blob/master/config.md#configuration-schema-example=" target="_blank" rel="external nofollow noopener noreferrer">config.json</a> ). It then takes information contained in the <code>config.json</code> and uses it to invoke the <code>libnvidia-container</code> CLI with an appropriate set of flags. One of the most important flags being which specific GPU devices should be injected into the container.</p><p>Note that the previous name of this component was <code>nvidia-container-runtime-hook</code>. <code>nvidia-container-runtime-hook</code> is now simply a symlink to <code>nvidia-container-toolkit</code> on the system.</p><h4 id="nvidia-container-runtime"><a href="#nvidia-container-runtime" class="headerlink" title="nvidia-container-runtime"></a><code>nvidia-container-runtime</code></h4><p>This component used to be a complete fork of <code>runC</code> with NVIDIA specific code injected into it. Since 2019, it is a thin wrapper around the native <code>runC</code> installed on the host system. <code>nvidia-container-runtime</code> takes a <code>runC</code> spec as input, injects the <code>nvidia-container-toolkit</code> script as a <code>prestart</code> hook into it, and then calls out to the native <code>runC</code>, passing it the modified <code>runC</code> spec with that hook set. It’s important to note that this component is not necessarily specific to docker (but it is specific to <code>runC</code>).</p><p>When the package is installed, the Docker <code>daemon.json</code> is updated to point to the binary as can be seen below:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"default-runtime"</span>: <span class="string">"nvidia"</span>,</span><br><span class="line"><span class="string">"runtimes"</span>: &#123;</span><br><span class="line">    <span class="string">"nvidia"</span>: &#123;</span><br><span class="line">        <span class="string">"path"</span>: <span class="string">"/usr/bin/nvidia-container-runtime"</span>,</span><br><span class="line">        <span class="string">"runtimeArgs"</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="nvidia-docker2"><a href="#nvidia-docker2" class="headerlink" title="nvidia-docker2"></a><code>nvidia-docker2</code></h4><p>This package is the only docker-specific package of the hierarchy. It takes the script associated with the <code>nvidia-container-runtime</code> and installs it into docker’s <code>/etc/docker/daemon.json</code> file. This then allows you to run (for example) <code>docker run --runtime=nvidia ...</code> to automatically add GPU support to your containers. It also installs a wrapper script around the native docker CLI called <code>nvidia-docker</code> which lets you invoke docker without needing to specify <code>--runtime=nvidia</code> every single time. It also lets you set an environment variable on the host (<code>NV_GPU</code>) to specify which GPUs should be injected into a container.</p><h3 id="部署验证"><a href="#部署验证" class="headerlink" title="部署验证"></a>部署验证</h3><p>这里仍然基于腾讯云的 CentOS 7机器为例演示如何在安装配置 <code>NVIDIA Container Toolkit</code>，对于更多的平台可以参考其<a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="external nofollow noopener noreferrer">官方文档</a>。</p><h4 id="安装-Docker-CE"><a href="#安装-Docker-CE" class="headerlink" title="安装 Docker CE"></a>安装 Docker CE</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://get.docker.com | sh \</span><br><span class="line">  &amp;&amp; sudo systemctl start docker \</span><br><span class="line">  &amp;&amp; sudo systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><h4 id="安装-NVIDIA-Container-Toolkit"><a href="#安装-NVIDIA-Container-Toolkit" class="headerlink" title="安装 NVIDIA Container Toolkit"></a>安装 NVIDIA Container Toolkit</h4><p>Setup the <code>stable</code> repository and the GPG key:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br></pre></td></tr></table></figure><p>Install the <code>nvidia-docker2</code> package (and dependencies) after updating the package listing:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install -y nvidia-docker2</span><br></pre></td></tr></table></figure><p>Restart the Docker daemon to complete the installation after setting the default runtime:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>At this point, a working setup can be tested by running a base CUDA container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br></pre></td></tr></table></figure><p>This should result in a console output shown below:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h4 id="配置-NVIDIA-Runtime"><a href="#配置-NVIDIA-Runtime" class="headerlink" title="配置 NVIDIA Runtime"></a>配置 NVIDIA Runtime</h4><p>To register the <code>nvidia</code> runtime, use the method below that is best suited to your environment. You might need to merge the new argument with your existing configuration. Three options are available:</p><h4 id="Systemd-drop-in-file"><a href="#Systemd-drop-in-file" class="headerlink" title="Systemd drop-in file"></a>Systemd drop-in file</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p <span class="regexp">/etc/</span>systemd<span class="regexp">/system/</span>docker.service.d</span><br></pre></td></tr></table></figure><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tee <span class="regexp">/etc/</span>systemd<span class="regexp">/system/</span>docker.service.d/override.conf &lt;&lt;EOF</span><br><span class="line">[Service]</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=<span class="regexp">/usr/</span>bin<span class="regexp">/dockerd --host=fd:/</span><span class="regexp">/ --add-runtime=nvidia=/</span>usr<span class="regexp">/bin/</span>nvidia-container-runtime</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload \</span><br><span class="line">  &amp;&amp; sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h4 id="Daemon-configuration-file"><a href="#Daemon-configuration-file" class="headerlink" title="Daemon configuration file"></a>Daemon configuration file</h4><p>The <code>nvidia</code> runtime can also be registered with Docker using the <code>daemon.json</code> configuration file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tee /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"runtimes"</span>: &#123;</span><br><span class="line">        <span class="string">"nvidia"</span>: &#123;</span><br><span class="line">            <span class="string">"path"</span>: <span class="string">"/usr/bin/nvidia-container-runtime"</span>,</span><br><span class="line">            <span class="string">"runtimeArgs"</span>: []</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pkill -SIGHUP dockerd</span><br></pre></td></tr></table></figure><p>You can optionally reconfigure the default runtime by adding the following to <code>/etc/docker/daemon.json</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"default-runtime"</span>: <span class="string">"nvidia"</span></span><br></pre></td></tr></table></figure><h4 id="Command-Line"><a href="#Command-Line" class="headerlink" title="Command Line"></a>Command Line</h4><p>Use <code>dockerd</code> to add the <code>nvidia</code> runtime:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dockerd --add-runtime=nvidia=/usr/bin/nvidia-container-runtime [...]</span><br></pre></td></tr></table></figure><h2 id="在k8s中管理GPU"><a href="#在k8s中管理GPU" class="headerlink" title="在k8s中管理GPU"></a>在k8s中管理GPU</h2><p>为了在 k8s 中管理和使用GPU，我们除了需要配置 <code>NVIDIA Container Toolkit</code>，还需要安装NVIDIA推出的 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA/k8s-device-plugin</a>，具体安装可以参考 <a href="../3f069334">我的这篇博文</a>。上面的步骤加起来显得还是有些繁琐，如果你直接使用腾讯云 TKE 的话，在集群添加装有GPU的Node时候，就会自动帮你安装配置好  <code>NVIDIA Container Toolkit</code> 和  <code>NVIDIA/k8s-device-plugin</code>，十分方便。接下来我们以Tensorflow为例，演示在 k8s 环境运行有GPU的Tensorflow。</p><h3 id="单机版Tensorflow"><a href="#单机版Tensorflow" class="headerlink" title="单机版Tensorflow"></a>单机版Tensorflow</h3><p>首先是单机版的Tensorflow，执行 <code>kubectl apply -f tensorflow.yaml</code>来运行 <code>Jupiter Notebook</code>。</p><figure class="highlight yaml"><figcaption><span>tensorflow.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">tensorflow/tensorflow:2.2.1-gpu-py3-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8888</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">4</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">jupyter-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8888</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">tensorflow</span></span><br></pre></td></tr></table></figure><p>我们看到容器很快运行起来，根据 <code>http:&lt;nodeIP&gt;:&lt;nodePort&gt;</code> 可以访问到 <code>Jupiter Notebook</code>，但是显示需要token：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_tensorflow-jupiter.png"></p><p>查看 <code>Tensorflow</code> 日志，可以获得 token：<code>aa06c9f12d80adac1a6288b97bf8030522cecc92202dbb20</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@VM-1-14-centos single]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">tensorflow-6cbc85744b-c567p   1/1     Running   0          7m37s</span><br><span class="line">[root@VM-1-14-centos single]<span class="comment"># kubectl logs tensorflow-6cbc85744b-c567p</span></span><br><span class="line"></span><br><span class="line">________                               _______________</span><br><span class="line">___  __/__________________________________  ____/__  /________      __</span><br><span class="line">__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /</span><br><span class="line">_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /</span><br><span class="line">/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WARNING: You are running this container as root, <span class="built_in">which</span> can cause new files <span class="keyword">in</span></span><br><span class="line">mounted volumes to be created as the root user on your host machine.</span><br><span class="line"></span><br><span class="line">To avoid this, run the container by specifying your user<span class="string">'s userid:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ docker run -u $(id -u):$(id -g) args...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[I 04:47:52.083 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span></span><br><span class="line"><span class="string">[I 04:47:52.315 NotebookApp] Serving notebooks from local directory: /tf</span></span><br><span class="line"><span class="string">[I 04:47:52.315 NotebookApp] Jupyter Notebook 6.1.4 is running at:</span></span><br><span class="line"><span class="string">[I 04:47:52.315 NotebookApp] http://tensorflow-6cbc85744b-c567p:8888/?token=aa06c9f12d80adac1a6288b97bf8030522cecc92202dbb20</span></span><br><span class="line"><span class="string">[I 04:47:52.315 NotebookApp]  or http://127.0.0.1:8888/?token=aa06c9f12d80adac1a6288b97bf8030522cecc92202dbb20</span></span><br><span class="line"><span class="string">[I 04:47:52.315 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span></span><br><span class="line"><span class="string">[C 04:47:52.319 NotebookApp]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    To access the notebook, open this file in a browser:</span></span><br><span class="line"><span class="string">        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html</span></span><br><span class="line"><span class="string">    Or copy and paste one of these URLs:</span></span><br><span class="line"><span class="string">        http://tensorflow-6cbc85744b-c567p:8888/?token=aa06c9f12d80adac1a6288b97bf8030522cecc92202dbb20</span></span><br><span class="line"><span class="string">     or http://127.0.0.1:8888/?token=aa06c9f12d80adac1a6288b97bf8030522cecc92202dbb20</span></span><br><span class="line"><span class="string">[I 04:49:28.692 NotebookApp] 302 GET / (172.16.0.193) 0.57ms</span></span><br><span class="line"><span class="string">[I 04:49:28.700 NotebookApp] 302 GET /tree? (172.16.0.193) 0.67ms</span></span><br></pre></td></tr></table></figure><p>登陆之后即可看到 <code>Jupiter Notebook</code>：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_tensorflow-jupiter.png"></p><p>新建Notebook，运行命令如下：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_tensorflow-gpu.png"></p><p>可以看到，TensorFlow 支持在GPU上的运算</p><ul><li><code>&quot;/device:GPU:0&quot;</code>：TensorFlow 可见的机器上第一个 GPU 的速记表示法。</li><li><code>&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;</code>：TensorFlow 可见的机器上第一个 GPU 的完全限定名称。</li></ul><h3 id="分布式Tensorflow"><a href="#分布式Tensorflow" class="headerlink" title="分布式Tensorflow"></a>分布式Tensorflow</h3><p>整体架构：</p><p>这个架构图是分布式tensorflow的实战图，其中有</p><ul><li>两个参数服务</li><li>多个worker服务</li><li>还有个shuffle和抽样的服务</li></ul><p>shuffle就是对样根据其标签进行混排，然后对外提供batch抽样服务（可以是有放回和无放回，抽样是一门科学，详情可以参考抽样技术一书），每个batch的抽样是由每个worker去触发，worker拿到抽样的数据样本ID后就去基于kubernetes构建的分布式数据库里边提取该batchSize的样本数据，进行训练计算，由于分布式的tensorflow能够保证异步梯度下降算法，所以每次训练batch数据的时候都会基于最新的参数迭代，然而，更新参数操作就是两个参数服务做的，架构中模型（参数）的存储在NFS中，这样以来，参数服务与worker就可以共享参数了，最后说明一下，我们训练的所有数据都是存储在分布式数据库中（数据库的选型可以根据具体的场景而定）。为什么需要一个shuffle和抽样的服务，因为当数据量很大的时候，我们如果对所有的样本数据进行shuffle和抽样计算的话会浪费很大的资源，因此需要一个这样的服务专门提取数据的（id，label）来进行混排和抽样，这里如果（id, label）的数据量也很大的时候我们可以考虑基于spark 来分布式的进行shuffle和抽样，目前spark2.3已经原生支持kubernetes调度</p><p><img alt data-src="https://upload-images.jianshu.io/upload_images/3521279-7ab3727232db8073.png"></p><p>首先是 <code>Parameter Server</code>：</p><figure class="highlight yaml"><figcaption><span>tf-ps.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span> </span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-ps</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">tensorflow-ps</span></span><br><span class="line">        <span class="attr">role:</span> <span class="string">ps</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ps</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">tensorflow/tensorflow:2.2.1-gpu-py3-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2222</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">4</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/datanfs</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">        <span class="attr">nfs:</span></span><br><span class="line">          <span class="attr">server:</span> <span class="string">你的nfs服务地址</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">"/data/nfs"</span>   </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-ps-service</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow-ps</span></span><br><span class="line">    <span class="attr">role:</span> <span class="string">service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">2222</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">2222</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow-ps</span></span><br></pre></td></tr></table></figure><p>然后是 <code>Worker</code>：</p><figure class="highlight yaml"><figcaption><span>tf-worker.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-worker</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">tensorflow-worker</span></span><br><span class="line">        <span class="attr">role:</span> <span class="string">worker</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">worker</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">tensorflow/tensorflow:2.2.1-gpu-py3-jupyter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2222</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">4</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">2Gi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/datanfs</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">        <span class="attr">nfs:</span></span><br><span class="line">          <span class="attr">server:</span> <span class="string">你的nfs服务地址</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">"/data/nfs"</span>   </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-wk-service</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow-worker</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">2222</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">2222</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tensorflow-worker</span></span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://cloud.tencent.com/developer/article/1005137" target="_blank" rel="external nofollow noopener noreferrer">https://cloud.tencent.com/developer/article/1005137</a></li><li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA Container Toolkit</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在 &lt;a href=&quot;https://houmin.cc/posts/5004f8e5/&quot;&gt;GPU 与 CUDA 编程入门&lt;/a&gt; 这篇博客中初步介绍了如何Linux上使用GPU的方法，随着容器和k8s的迅猛发展，人们对于在容器中使用GPU的需求越发强烈。本文将基于前文，继续介绍如何在容器中使用GPU，进一步地，介绍在Kubernetes中如何调度GPU，并以Tensorflow为例，介绍如何基于Docker搭建部署了GPU的深度学习开发环境。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-22_nvidia-gpu-docker.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="container" scheme="http://houmin.cc/tags/container/"/>
    
      <category term="docker" scheme="http://houmin.cc/tags/docker/"/>
    
      <category term="k8s" scheme="http://houmin.cc/tags/k8s/"/>
    
      <category term="GPU" scheme="http://houmin.cc/tags/GPU/"/>
    
      <category term="Nvidia" scheme="http://houmin.cc/tags/Nvidia/"/>
    
      <category term="tensorflow" scheme="http://houmin.cc/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>【Kubernetes】Device Plugin</title>
    <link href="http://houmin.cc/posts/3f069334/"/>
    <id>http://houmin.cc/posts/3f069334/</id>
    <published>2020-11-16T07:31:42.000Z</published>
    <updated>2020-12-17T03:27:17.835Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Kubernetes 原生支持对于CPU和内存资源的发现，但是有很多其他的设备 kubelet不能原生处理，比如GPU、FPGA、RDMA、存储设备和其他类似的异构计算资源设备。为了能够使用这些设备资源，我们需要进行各个设备的初始化和设置。按照 Kubernetes 的 <code>OutOfTree</code> 的哲学理念，我们不应该把各个厂商的设备初始化设置相关代码与 Kubernetes 核心代码放在一起。与之相反，我们需要一种机制能够让各个设备厂商向 Kubelet 上报设备资源，而不需要修改 Kubernetes 核心代码。这即是 <code>Device Plugin</code> 这一机制的来源，本文将介绍 Device Plugin 的实现原理，并介绍其使用。</p><a id="more"></a><h2 id="Device-插件原理"><a href="#Device-插件原理" class="headerlink" title="Device 插件原理"></a>Device 插件原理</h2><p>Device Plugin 实际上是一个 gPRC server，Device 插件一般推荐使用 DaemonSet 的方式部署，并将 <code>/var/lib/kubelet/device-plugins</code> 以 Volume 的形式挂载到容器中。当然，也可以手动运行的方式来部署，但这样就没有失败自动恢复的功能了。</p><p>为了能够使用某个厂商的特定设备，一般有两步：</p><ul><li><code>kubectl create -f http://vendor.com/device-plugin-daemonset.yaml</code></li><li>执行 <code>kubectl describe nodes</code>的时候，相关设备会出现在node status中：<code>vendor-domain/vendor-device</code></li></ul><p>当 Device Plugin 向 kubelet 注册后，kubelet 就通过 RPC 与 Device Plugin 交互：</p><ul><li><code>ListAndWatch()</code> ：让 kubelet 发现设备资源和对应属性，并且在设备资源发生变动的时候接收通知</li><li><code>Allocate()</code> ：kubelet 在创建容器前通过 Allocate来申请相关设备资源</li></ul><p><img alt="Process" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_k8s-device-plugin.png"></p><h3 id="Registration"><a href="#Registration" class="headerlink" title="Registration"></a>Registration</h3><p>为了向 kubelet 告知 Device Plugin 的存在，Device Plugin 必须向 kubelet 发出注册请求，这之后 kubelet 才会和 Device Plugin 通过 <code>gRPC</code>交互，具体过程如下：</p><ul><li>Device Plugin 向 Kubelet 发送一个 <code>RegisterRequest</code>的请求</li><li>Kubelet 收到 <code>RegisterRequest</code> 请求后，返回一个 <code>RegisterResponse</code>，如果Kubelet碰到任何错误，会把错误附在Response中</li><li>如果 Device Plugin 没有收到任何错误，则启动他的 gRPC server</li></ul><p>插件启动后要持续监控 Kubelet 的状态，并在 Kubelet 重启后重新注册自己。比如，Kubelet 刚启动后会清空 <code>/var/lib/kubelet/device-plugins/</code> 目录，所以插件作者可以监控自己监听的 unix socket 是否被删除了，并根据此事件重新注册自己</p><h3 id="Unix-Socket"><a href="#Unix-Socket" class="headerlink" title="Unix Socket"></a>Unix Socket</h3><p>Device Plugin 和 Kubelet 通过在一个 Unix Socket上使用 gRPC 交互，当启动 gRPC server的时候，Device Plugin 将会在 <code>/var/lib/kubelet/device-plugins/</code>  这个 HostPath 创建一个 UnixSocket，比如 <code>/var/lib/kubelet/device-plugins/nvidiaGPU.sock</code>。</p><p>在实现 Device 插件时需要注意</p><ul><li>插件启动时，需要通过 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code> 向 Kubelet 注册，同时提供插件的 Unix Socket 名称、API 的版本号和插件名称（格式为 <code>vendor-domain/resource</code>，如 <code>nvidia.com/gpu</code>）。Kubelet 会将这些设备暴露到 Node 状态中，方便后续调度器使用</li><li>插件启动后向 Kubelet 发送插件列表、按需分配设备并持续监控设备的实时状态</li></ul><h3 id="Protocol-Overview"><a href="#Protocol-Overview" class="headerlink" title="Protocol Overview"></a>Protocol Overview</h3><p><img alt="Protocol Overview" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_k8s-device-plugin-protocol.png"></p><h3 id="API-specification"><a href="#API-specification" class="headerlink" title="API specification"></a>API specification</h3><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Registration is the service advertised by the Kubelet</span></span><br><span class="line"><span class="comment">// Only when Kubelet answers with a success code to a Register Request</span></span><br><span class="line"><span class="comment">// may Device Plugins start their service</span></span><br><span class="line"><span class="comment">// Registration may fail when device plugin version is not supported by</span></span><br><span class="line"><span class="comment">// Kubelet or the registered resourceName is already taken by another</span></span><br><span class="line"><span class="comment">// active device plugin. Device plugin is expected to terminate upon registration failure</span></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">Registration</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">rpc</span> Register(RegisterRequest) <span class="keyword">returns</span> (Empty) &#123;&#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">message DevicePluginOptions &#123;</span></span><br><span class="line"><span class="function">  // Indicates if PreStartContainer call is required before each container start</span></span><br><span class="line"><span class="function">    bool pre_start_required = 1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">RegisterRequest</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Version of the API the Device Plugin was built against</span></span><br><span class="line">    <span class="built_in">string</span> version = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Name of the unix socket the device plugin is listening on</span></span><br><span class="line">    <span class="comment">// PATH = path.Join(DevicePluginPath, endpoint)</span></span><br><span class="line">    <span class="built_in">string</span> endpoint = <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// Schedulable resource name. As of now it's expected to be a DNS Label</span></span><br><span class="line">    <span class="built_in">string</span> resource_name = <span class="number">3</span>;</span><br><span class="line">    <span class="comment">// Options to be communicated with Device Manager</span></span><br><span class="line">    options = <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Empty</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DevicePlugin is the service advertised by Device Plugins</span></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">DevicePlugin</span> </span>&#123;</span><br><span class="line">    <span class="comment">// GetDevicePluginOptions returns options to be communicated with Device</span></span><br><span class="line">    <span class="comment">// Manager</span></span><br><span class="line">    <span class="function"><span class="keyword">rpc</span> GetDevicePluginOptions(Empty) <span class="keyword">returns</span> (DevicePluginOptions) &#123;&#125;</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    // ListAndWatch <span class="keyword">returns</span> a stream of List of Devices</span></span><br><span class="line"><span class="function">    // Whenever a Device state change or a Device disapears, ListAndWatch</span></span><br><span class="line"><span class="function">    // <span class="keyword">returns</span> the new list</span></span><br><span class="line"><span class="function">    <span class="keyword">rpc</span> ListAndWatch(Empty) <span class="keyword">returns</span> (stream ListAndWatchResponse) &#123;&#125;</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    // Allocate is called during container creation so that the Device</span></span><br><span class="line"><span class="function">    // Plugin can run device specific operations and instruct Kubelet</span></span><br><span class="line"><span class="function">    // of the steps to make the Device available in the container</span></span><br><span class="line"><span class="function">    <span class="keyword">rpc</span> Allocate(AllocateRequest) <span class="keyword">returns</span> (AllocateResponse) &#123;&#125;</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    // PreStartContainer is called, if indicated by Device Plugin during registeration phase,</span></span><br><span class="line"><span class="function">    // before each container start. Device plugin can run device specific operations</span></span><br><span class="line"><span class="function">    // such as reseting the device before making devices available to the container</span></span><br><span class="line"><span class="function">    <span class="keyword">rpc</span> PreStartContainer(PreStartContainerRequest) <span class="keyword">returns</span> (PreStartContainerResponse) &#123;&#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">// ListAndWatch <span class="keyword">returns</span> a stream of List of Devices</span></span><br><span class="line"><span class="function">// Whenever a Device state change or a Device disapears, ListAndWatch</span></span><br><span class="line"><span class="function">// <span class="keyword">returns</span> the new list</span></span><br><span class="line"><span class="function">message ListAndWatchResponse &#123;</span></span><br><span class="line"><span class="function">    repeated Device devices = 1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* E.g:</span><br><span class="line">* struct Device &#123;</span><br><span class="line">*    ID: <span class="string">"GPU-fef8089b-4820-abfc-e83e-94318197576e"</span>,</span><br><span class="line">*    State: <span class="string">"Healthy"</span>,</span><br><span class="line">*&#125; */</span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Device</span> </span>&#123;</span><br><span class="line">    <span class="comment">// A unique ID assigned by the device plugin used</span></span><br><span class="line">    <span class="comment">// to identify devices during the communication</span></span><br><span class="line">    <span class="comment">// Max length of this field is 63 characters</span></span><br><span class="line">    <span class="built_in">string</span> ID = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Health of the device, can be healthy or unhealthy, see constants.go</span></span><br><span class="line">    <span class="built_in">string</span> health = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// - PreStartContainer is expected to be called before each container start if indicated by plugin during registration phase.</span></span><br><span class="line"><span class="comment">// - PreStartContainer allows kubelet to pass reinitialized devices to containers.</span></span><br><span class="line"><span class="comment">// - PreStartContainer allows Device Plugin to run device specific operations on</span></span><br><span class="line"><span class="comment">//   the Devices requested</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">PreStartContainerRequest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">repeated</span> <span class="built_in">string</span> devicesIDs = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreStartContainerResponse will be send by plugin in response to PreStartContainerRequest</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">PreStartContainerResponse</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// - Allocate is expected to be called during pod creation since allocation</span></span><br><span class="line"><span class="comment">//   failures for any container would result in pod startup failure.</span></span><br><span class="line"><span class="comment">// - Allocate allows kubelet to exposes additional artifacts in a pod's</span></span><br><span class="line"><span class="comment">//   environment as directed by the plugin.</span></span><br><span class="line"><span class="comment">// - Allocate allows Device Plugin to run device specific operations on</span></span><br><span class="line"><span class="comment">//   the Devices requested</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">AllocateRequest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">repeated</span> ContainerAllocateRequest container_requests = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">ContainerAllocateRequest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">repeated</span> <span class="built_in">string</span> devicesIDs = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// AllocateResponse includes the artifacts that needs to be injected into</span></span><br><span class="line"><span class="comment">// a container for accessing 'deviceIDs' that were mentioned as part of</span></span><br><span class="line"><span class="comment">// 'AllocateRequest'.</span></span><br><span class="line"><span class="comment">// Failure Handling:</span></span><br><span class="line"><span class="comment">// if Kubelet sends an allocation request for dev1 and dev2.</span></span><br><span class="line"><span class="comment">// Allocation on dev1 succeeds but allocation on dev2 fails.</span></span><br><span class="line"><span class="comment">// The Device plugin should send a ListAndWatch update and fail the</span></span><br><span class="line"><span class="comment">// Allocation request</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">AllocateResponse</span> </span>&#123;</span><br><span class="line">    <span class="keyword">repeated</span> ContainerAllocateResponse container_responses = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">ContainerAllocateResponse</span> </span>&#123;</span><br><span class="line">    <span class="comment">// List of environment variable to be set in the container to access one of more devices.</span></span><br><span class="line">    map&lt;<span class="built_in">string</span>, <span class="built_in">string</span>&gt; envs = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Mounts for the container.</span></span><br><span class="line">    <span class="keyword">repeated</span> Mount mounts = <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// Devices for the container.</span></span><br><span class="line">    <span class="keyword">repeated</span> DeviceSpec devices = <span class="number">3</span>;</span><br><span class="line">    <span class="comment">// Container annotations to pass to the container runtime</span></span><br><span class="line">    map&lt;<span class="built_in">string</span>, <span class="built_in">string</span>&gt; annotations = <span class="number">4</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Mount specifies a host volume to mount into a container.</span></span><br><span class="line"><span class="comment">// where device library or tools are installed on host and container</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Mount</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Path of the mount within the container.</span></span><br><span class="line">    <span class="built_in">string</span> container_path = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Path of the mount on the host.</span></span><br><span class="line">    <span class="built_in">string</span> host_path = <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// If set, the mount is read-only.</span></span><br><span class="line">    <span class="built_in">bool</span> read_only = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DeviceSpec specifies a host device to mount into a container.</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">DeviceSpec</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Path of the device within the container.</span></span><br><span class="line">  <span class="built_in">string</span> container_path = <span class="number">1</span>;</span><br><span class="line">  <span class="comment">// Path of the device on the host.</span></span><br><span class="line">  <span class="built_in">string</span> host_path = <span class="number">2</span>;</span><br><span class="line">  <span class="comment">// Cgroups permissions of the device, candidates are one or more of</span></span><br><span class="line">  <span class="comment">// * r - allows container to read from the specified device.</span></span><br><span class="line">  <span class="comment">// * w - allows container to write to the specified device.</span></span><br><span class="line">  <span class="comment">// * m - allows container to create device files that do not yet exist.</span></span><br><span class="line">  <span class="built_in">string</span> permissions = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="插件生命周期管理"><a href="#插件生命周期管理" class="headerlink" title="插件生命周期管理"></a>插件生命周期管理</h3><p>插件启动时，以grpc的形式通过/var/lib/kubelet/device-plugins/kubelet.sock向Kubelet注册，同时提供插件的监听Unix Socket，API版本号和设备名称（比如nvidia.com/gpu）。Kubelet将会把这些设备暴露到Node状态中，以Extended Resource的要求发送到API server中，后续Scheduler会根据这些信息进行调度。</p><p>插件启动后，Kubelet会建立一个到插件的listAndWatch长连接，当插件检测到某个设备不健康的时候，就会主动通知Kubelet。此时如果这个设备处于空闲状态，Kubelet就会将其挪出可分配列表；如果该设备已经被某个pod使用，Kubelet就会将该Pod杀掉</p><p>插件启动后可以利用Kubelet的socket持续检查Kubelet的状态，如果Kubelet重启，插件也会相应的重启，并且重新向Kubelet注册自己</p><h2 id="NVIDIA-Device-Plugin"><a href="#NVIDIA-Device-Plugin" class="headerlink" title="NVIDIA Device Plugin"></a>NVIDIA Device Plugin</h2><p>NVIDIA 提供了一个基于 Device Plugins 接口的 GPU 设备插件 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA/k8s-device-plugin</a>。</p><p>部署</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/master/nvidia-device-plugin.yml</span><br></pre></td></tr></table></figure><p>创建 Pod 时请求 GPU 资源</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nvidia/cuda</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pod1-ctr</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">["sleep"]</span></span><br><span class="line">    <span class="attr">args:</span> <span class="string">["100000"]</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>注意：使用该插件时需要配置 <a href="https://github.com/NVIDIA/nvidia-docker/" target="_blank" rel="external nofollow noopener noreferrer">nvidia-docker 2.0</a>，并配置 <code>nvidia</code> 为默认运行时 （即配置 docker daemon 的选项 <code>--default-runtime=nvidia</code>）。nvidia-docker 2.0 的安装方法为（以 Ubuntu Xenial 为例，其他系统的安装方法可以参考 <a href="http://nvidia.github.io/nvidia-docker/" target="_blank" rel="external nofollow noopener noreferrer">这里</a>）：</p><p>整个Kubernetes调度GPU的过程如下：</p><ul><li>GPU Device plugin 部署到GPU节点上，通过 <code>ListAndWatch</code> 接口，上报注册节点的GPU信息和对应的DeviceID。 </li><li>当有声明 <code>nvidia.com/gpu</code> 的GPU Pod创建出现，调度器会综合考虑GPU设备的空闲情况，将Pod调度到有充足GPU设备的节点上。</li><li>节点上的kubelet 启动Pod时，根据request中的声明调用各个Device plugin 的 allocate接口， 由于容器声明了GPU。 kubelet 根据之前 <code>ListAndWatch</code> 接口收到的Device信息，选取合适的设备，DeviceID 作为参数，调用GPU DevicePlugin的 <code>Allocate</code> 接口</li><li>GPU DevicePlugin ，接收到调用，将DeviceID 转换为 <code>NVIDIA_VISIBLE_DEVICES</code> 环境变量，返回kubelet</li><li>kubelet将环境变量注入到Pod， 启动容器</li><li>容器启动时， <code>gpu-container-runtime</code> 调用 <code>gpu-containers-runtime-hook</code> </li><li><code>gpu-containers-runtime-hook</code> 根据容器的 <code>NVIDIA_VISIBLE_DEVICES</code> 环境变量，转换为 <code>--devices</code> 参数，调用 <code>nvidia-container-cli prestart</code> </li><li><code>nvidia-container-cli</code> 根据 <code>--devices</code> ，将GPU设备映射到容器中。 并且将宿主机的Nvidia Driver Lib 的so文件也映射到容器中。 此时容器可以通过这些so文件，调用宿主机的Nvidia Driver。</li></ul><p>在前面 <code>API Specification</code> 中，通过 <code>Protobuf</code> 定义了 <code>DevicePlugin</code> 应该提供的服务，在 <code>Kubelet</code> 中会调用 <code>DevicePluginClient</code> 来使用对应的服务，这里的 <code>DevicePluginClient</code> 即是通过 <code>Protobuf</code> 自动生成的代码。</p><figure class="highlight go"><figcaption><span>k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1/api.pb.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> DevicePluginClient <span class="keyword">interface</span> &#123;</span><br><span class="line">    GetDevicePluginOptions(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*DevicePluginOptions, error)</span><br><span class="line">    ListAndWatch(ctx context.Context, in *Empty, opts ...grpc.CallOption) (DevicePlugin_ListAndWatchClient, error)</span><br><span class="line">    Allocate(ctx context.Context, in *AllocateRequest, opts ...grpc.CallOption) (*AllocateResponse, error)</span><br><span class="line">    PreStartContainer(ctx context.Context, in *PreStartContainerRequest, opts ...grpc.CallOption) (*PreStartContainerResponse, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>NVIDIA/k8s-device-plugin</code> 中，我们可以看到上面不同服务的具体实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">GetDevicePluginOptions</span><span class="params">(context.Context, *pluginapi.Empty)</span> <span class="params">(*pluginapi.DevicePluginOptions, error)</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">ListAndWatch</span><span class="params">(e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer)</span> <span class="title">error</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">Allocate</span><span class="params">(ctx context.Context, reqs *pluginapi.AllocateRequest)</span> <span class="params">(*pluginapi.AllocateResponse, error)</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">PreStartContainer</span><span class="params">(context.Context, *pluginapi.PreStartContainerRequest)</span> <span class="params">(*pluginapi.PreStartContainerResponse, error)</span></span></span><br></pre></td></tr></table></figure><p>对 <code>NVIDIA/k8s-device-plugin</code> 来说，这里的关键数据结构为 <code>NvidiaDevicePlugin</code>，它实现了 <code>Device Plugin</code> 架构定义的API：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> NvidiaDevicePlugin <span class="keyword">struct</span> &#123;</span><br><span class="line">    ResourceManager</span><br><span class="line">    resourceName     <span class="keyword">string</span></span><br><span class="line">    deviceListEnvvar <span class="keyword">string</span></span><br><span class="line">    allocatePolicy   gpuallocator.Policy</span><br><span class="line">    socket           <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line">    server        *grpc.Server</span><br><span class="line">    cachedDevices []*Device</span><br><span class="line">    health        <span class="keyword">chan</span> *Device</span><br><span class="line">    stop          <span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面根据 <code>Device Plugin</code> 的生命周期，依次分析每个部分的实现机制。</p><h3 id="NVIDIA-DevicePlugin-启动"><a href="#NVIDIA-DevicePlugin-启动" class="headerlink" title="NVIDIA DevicePlugin 启动"></a>NVIDIA DevicePlugin 启动</h3><p><code>NVIDIA</code> 的 <code>k8s-device-plugin</code> 启动之后逻辑如下，总的来说干了三件事：</p><ul><li>Serve：启动 <code>gRPC server</code>  </li><li>Register：向 <code>Kubelet</code> 注册给定的 <code>resourceName</code></li><li>CheckHealth：执行设备的健康检查逻辑，当检查到不健康的设备时，写到 <code>unhealthy</code> 的 channel 中</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">Start</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    m.initialize()</span><br><span class="line"></span><br><span class="line">    err := m.Serve()</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    err = m.Register()</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">go</span> m.CheckHealth(m.stop, m.cachedDevices, m.health)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Serve"><a href="#Serve" class="headerlink" title="Serve"></a>Serve</h4><p><code>Serve</code> 监听在<code>/var/lib/kubelet/device-plugins/nvidia-gpu.sock</code> 这 个 <code>Unix Socket</code>，并且启动了 <code>gRPC server</code>，其他的就是启动失败重试的逻辑了。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">Serve</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    os.Remove(m.socket)</span><br><span class="line">    sock, err := net.Listen(<span class="string">"unix"</span>, m.socket)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    pluginapi.RegisterDevicePluginServer(m.server, m)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        lastCrashTime := time.Now()</span><br><span class="line">        restartCount := <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> &#123;</span><br><span class="line">            log.Printf(<span class="string">"Starting GRPC server for '%s'"</span>, m.resourceName)</span><br><span class="line">            err := m.server.Serve(sock)</span><br><span class="line">            <span class="keyword">if</span> err == <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            log.Printf(<span class="string">"GRPC server for '%s' crashed with error: %v"</span>, m.resourceName, err)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// restart if it has not been too often</span></span><br><span class="line">            <span class="comment">// i.e. if server has crashed more than 5 times and it didn't last more than one hour each time</span></span><br><span class="line">            <span class="keyword">if</span> restartCount &gt; <span class="number">5</span> &#123;</span><br><span class="line">                <span class="comment">// quit</span></span><br><span class="line">                log.Fatalf(<span class="string">"GRPC server for '%s' has repeatedly crashed recently. Quitting"</span>, m.resourceName)</span><br><span class="line">            &#125;</span><br><span class="line">            timeSinceLastCrash := time.Since(lastCrashTime).Seconds()</span><br><span class="line">            lastCrashTime = time.Now()</span><br><span class="line">            <span class="keyword">if</span> timeSinceLastCrash &gt; <span class="number">3600</span> &#123;</span><br><span class="line">                <span class="comment">// it has been one hour since the last crash.. reset the count</span></span><br><span class="line">                <span class="comment">// to reflect on the frequency</span></span><br><span class="line">                restartCount = <span class="number">1</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                restartCount++</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for server to start by launching a blocking connexion</span></span><br><span class="line">    conn, err := m.dial(m.socket, <span class="number">5</span>*time.Second)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    conn.Close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Register"><a href="#Register" class="headerlink" title="Register"></a>Register</h4><p><code>Register</code> 通过和 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code> 这个 <code>Unix Socket</code> 向 <code>Kubelet</code> 注册，传递了 <code>DevicePlugin</code> 的 <code>Unix Socket</code> 的 Endpoint、资源的名称、API的版本号等信息。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">Register</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    conn, err := m.dial(pluginapi.KubeletSocket, <span class="number">5</span>*time.Second)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> conn.Close()</span><br><span class="line"></span><br><span class="line">    client := pluginapi.NewRegistrationClient(conn)</span><br><span class="line">    reqt := &amp;pluginapi.RegisterRequest&#123;</span><br><span class="line">        Version:      pluginapi.Version,</span><br><span class="line">        Endpoint:     path.Base(m.socket),</span><br><span class="line">        ResourceName: m.resourceName,</span><br><span class="line">        Options: &amp;pluginapi.DevicePluginOptions&#123;</span><br><span class="line">            GetPreferredAllocationAvailable: (m.allocatePolicy != <span class="literal">nil</span>),</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    _, err = client.Register(context.Background(), reqt)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="CheckHealth"><a href="#CheckHealth" class="headerlink" title="CheckHealth"></a>CheckHealth</h4><p>这里调用了 <code>nvml.NewEventSet</code> 来监听 GPU 是否发生变化的事件，并且将 <code>unhealthy Device</code>  传递给 <code>m.health</code> 这个<code>channel</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">checkHealth</span><span class="params">(stop &lt;-<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, devices []*Device, unhealthy <span class="keyword">chan</span>&lt;- *Device)</span></span> &#123;</span><br><span class="line">    disableHealthChecks := strings.ToLower(os.Getenv(envDisableHealthChecks))</span><br><span class="line">    <span class="keyword">if</span> disableHealthChecks == <span class="string">"all"</span> &#123;</span><br><span class="line">        disableHealthChecks = allHealthChecks</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> strings.Contains(disableHealthChecks, <span class="string">"xids"</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    eventSet := nvml.NewEventSet()</span><br><span class="line">    <span class="keyword">defer</span> nvml.DeleteEventSet(eventSet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, d := <span class="keyword">range</span> devices &#123;</span><br><span class="line">        gpu, _, _, err := nvml.ParseMigDeviceUUID(d.ID)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            gpu = d.ID</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        err = nvml.RegisterEventForDevice(eventSet, nvml.XidCriticalError, gpu)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &amp;&amp; strings.HasSuffix(err.Error(), <span class="string">"Not Supported"</span>) &#123;</span><br><span class="line">            log.Printf(<span class="string">"Warning: %s is too old to support healthchecking: %s. Marking it unhealthy."</span>, d.ID, err)</span><br><span class="line">            unhealthy &lt;- d</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">        check(err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-stop:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        e, err := nvml.WaitForEvent(eventSet, <span class="number">5000</span>)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &amp;&amp; e.Etype != nvml.XidCriticalError &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// <span class="doctag">FIXME:</span> formalize the full list and document it.</span></span><br><span class="line">        <span class="comment">// http://docs.nvidia.com/deploy/xid-errors/index.html#topic_4</span></span><br><span class="line">        <span class="comment">// Application errors: the GPU should still be healthy</span></span><br><span class="line">        <span class="keyword">if</span> e.Edata == <span class="number">31</span> || e.Edata == <span class="number">43</span> || e.Edata == <span class="number">45</span> &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> e.UUID == <span class="literal">nil</span> || <span class="built_in">len</span>(*e.UUID) == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="comment">// All devices are unhealthy</span></span><br><span class="line">            log.Printf(<span class="string">"XidCriticalError: Xid=%d, All devices will go unhealthy."</span>, e.Edata)</span><br><span class="line">            <span class="keyword">for</span> _, d := <span class="keyword">range</span> devices &#123;</span><br><span class="line">                unhealthy &lt;- d</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _, d := <span class="keyword">range</span> devices &#123;</span><br><span class="line">            <span class="comment">// Please see https://github.com/NVIDIA/gpu-monitoring-tools/blob/148415f505c96052cb3b7fdf443b34ac853139ec/bindings/go/nvml/nvml.h#L1424</span></span><br><span class="line">            <span class="comment">// for the rationale why gi and ci can be set as such when the UUID is a full GPU UUID and not a MIG device UUID.</span></span><br><span class="line">            gpu, gi, ci, err := nvml.ParseMigDeviceUUID(d.ID)</span><br><span class="line">            <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                gpu = d.ID</span><br><span class="line">                gi = <span class="number">0xFFFFFFFF</span></span><br><span class="line">                ci = <span class="number">0xFFFFFFFF</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> gpu == *e.UUID &amp;&amp; gi == *e.GpuInstanceId &amp;&amp; ci == *e.ComputeInstanceId &#123;</span><br><span class="line">                log.Printf(<span class="string">"XidCriticalError: Xid=%d on Device=%s, the device will go unhealthy."</span>, e.Edata, d.ID)</span><br><span class="line">                unhealthy &lt;- d</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kubelet-DeviceManager"><a href="#Kubelet-DeviceManager" class="headerlink" title="Kubelet DeviceManager"></a>Kubelet DeviceManager</h3><h4 id="DeviceManager-启动"><a href="#DeviceManager-启动" class="headerlink" title="DeviceManager 启动"></a>DeviceManager 启动</h4><figure class="highlight go"><figcaption><span>kubernetes/pkg/kubelet/cm/devicemanager/manager.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> ManagerImpl <span class="keyword">struct</span> &#123;</span><br><span class="line">    socketname <span class="keyword">string</span></span><br><span class="line">    socketdir  <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line">    endpoints <span class="keyword">map</span>[<span class="keyword">string</span>]endpointInfo <span class="comment">// Key is ResourceName</span></span><br><span class="line">    mutex     sync.Mutex</span><br><span class="line"></span><br><span class="line">    server *grpc.Server</span><br><span class="line">    wg     sync.WaitGroup</span><br><span class="line"></span><br><span class="line">    <span class="comment">// activePods is a method for listing active pods on the node</span></span><br><span class="line">    <span class="comment">// so the amount of pluginResources requested by existing pods</span></span><br><span class="line">    <span class="comment">// could be counted when updating allocated devices</span></span><br><span class="line">    activePods ActivePodsFunc</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sourcesReady provides the readiness of kubelet configuration sources such as apiserver update readiness.</span></span><br><span class="line">    <span class="comment">// We use it to determine when we can purge inactive pods from checkpointed state.</span></span><br><span class="line">    sourcesReady config.SourcesReady</span><br><span class="line"></span><br><span class="line">    <span class="comment">// callback is used for updating devices' states in one time call.</span></span><br><span class="line">    <span class="comment">// e.g. a new device is advertised, two old devices are deleted and a running device fails.</span></span><br><span class="line">    callback monitorCallback</span><br><span class="line"></span><br><span class="line">    <span class="comment">// allDevices is a map by resource name of all the devices currently registered to the device manager</span></span><br><span class="line">    allDevices <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">map</span>[<span class="keyword">string</span>]pluginapi.Device</span><br><span class="line"></span><br><span class="line">    <span class="comment">// healthyDevices contains all of the registered healthy resourceNames and their exported device IDs.</span></span><br><span class="line">    healthyDevices <span class="keyword">map</span>[<span class="keyword">string</span>]sets.String</span><br><span class="line"></span><br><span class="line">    <span class="comment">// unhealthyDevices contains all of the unhealthy devices and their exported device IDs.</span></span><br><span class="line">    unhealthyDevices <span class="keyword">map</span>[<span class="keyword">string</span>]sets.String</span><br><span class="line"></span><br><span class="line">    <span class="comment">// allocatedDevices contains allocated deviceIds, keyed by resourceName.</span></span><br><span class="line">    allocatedDevices <span class="keyword">map</span>[<span class="keyword">string</span>]sets.String</span><br><span class="line"></span><br><span class="line">    <span class="comment">// podDevices contains pod to allocated device mapping.</span></span><br><span class="line">    podDevices        podDevices</span><br><span class="line">    checkpointManager checkpointmanager.CheckpointManager</span><br><span class="line"></span><br><span class="line">    <span class="comment">// List of NUMA Nodes available on the underlying machine</span></span><br><span class="line">    numaNodes []<span class="keyword">int</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Store of Topology Affinties that the Device Manager can query.</span></span><br><span class="line">    topologyAffinityStore topologymanager.Store</span><br><span class="line"></span><br><span class="line">    <span class="comment">// devicesToReuse contains devices that can be reused as they have been allocated to</span></span><br><span class="line">    <span class="comment">// init containers.</span></span><br><span class="line">    devicesToReuse PodReusableDevices</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Device Manager</code> 在 <code>kubelet</code> 启动时的 <code>NewContainerManager</code> 中创建,属于 <code>containerManager</code> 的子模块。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewContainerManager</span><span class="params">(mountUtil mount.Interface, cadvisorInterface cadvisor.Interface, nodeConfig NodeConfig, failSwapOn <span class="keyword">bool</span>, devicePluginEnabled <span class="keyword">bool</span>, recorder record.EventRecorder)</span> <span class="params">(ContainerManager, error)</span></span> &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  </span><br><span class="line">    klog.Infof(<span class="string">"Creating device plugin manager: %t"</span>, devicePluginEnabled)</span><br><span class="line">    <span class="keyword">if</span> devicePluginEnabled &#123;</span><br><span class="line">        cm.deviceManager, err = devicemanager.NewManagerImpl(numaNodeInfo, cm.topologyManager)</span><br><span class="line">        cm.topologyManager.AddHintProvider(cm.deviceManager)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cm.deviceManager, err = devicemanager.NewManagerStub()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体创建 <code>DeviceManager</code> 的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newManagerImpl</span><span class="params">(socketPath <span class="keyword">string</span>, numaNodeInfo cputopology.NUMANodeInfo, topologyAffinityStore topologymanager.Store)</span> <span class="params">(*ManagerImpl, error)</span></span> &#123;</span><br><span class="line">    klog.V(<span class="number">2</span>).Infof(<span class="string">"Creating Device Plugin manager at %s"</span>, socketPath)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> socketPath == <span class="string">""</span> || !filepath.IsAbs(socketPath) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(errBadSocket+<span class="string">" %s"</span>, socketPath)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> numaNodes []<span class="keyword">int</span></span><br><span class="line">    <span class="keyword">for</span> node := <span class="keyword">range</span> numaNodeInfo &#123;</span><br><span class="line">        numaNodes = <span class="built_in">append</span>(numaNodes, node)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    dir, file := filepath.Split(socketPath)</span><br><span class="line">    manager := &amp;ManagerImpl&#123;</span><br><span class="line">        endpoints: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]endpointInfo),</span><br><span class="line"></span><br><span class="line">        socketname:            file,</span><br><span class="line">        socketdir:             dir,</span><br><span class="line">        allDevices:            <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">map</span>[<span class="keyword">string</span>]pluginapi.Device),</span><br><span class="line">        healthyDevices:        <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]sets.String),</span><br><span class="line">        unhealthyDevices:      <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]sets.String),</span><br><span class="line">        allocatedDevices:      <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]sets.String),</span><br><span class="line">        podDevices:            <span class="built_in">make</span>(podDevices),</span><br><span class="line">        numaNodes:             numaNodes,</span><br><span class="line">        topologyAffinityStore: topologyAffinityStore,</span><br><span class="line">        devicesToReuse:        <span class="built_in">make</span>(PodReusableDevices),</span><br><span class="line">    &#125;</span><br><span class="line">    manager.callback = manager.genericDeviceUpdateCallback</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The following structures are populated with real implementations in manager.Start()</span></span><br><span class="line">    <span class="comment">// Before that, initializes them to perform no-op operations.</span></span><br><span class="line">    manager.activePods = <span class="function"><span class="keyword">func</span><span class="params">()</span> []*<span class="title">v1</span>.<span class="title">Pod</span></span> &#123; <span class="keyword">return</span> []*v1.Pod&#123;&#125; &#125;</span><br><span class="line">    manager.sourcesReady = &amp;sourcesReadyStub&#123;&#125;</span><br><span class="line">    checkpointManager, err := checkpointmanager.NewCheckpointManager(dir)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"failed to initialize checkpoint manager: %v"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">    manager.checkpointManager = checkpointManager</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> manager, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中除了构建 <code>DeviceManager</code> 相关的结构之外，另外做的一个事情就是注册了一个 <code>callback</code>，用来处理对应 <code>devices</code> 的<code>add</code>，<code>delete</code>，<code>update</code> 事件。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">genericDeviceUpdateCallback</span><span class="params">(resourceName <span class="keyword">string</span>, devices []pluginapi.Device)</span></span> &#123;</span><br><span class="line">    m.mutex.Lock()</span><br><span class="line">    m.healthyDevices[resourceName] = sets.NewString()</span><br><span class="line">    m.unhealthyDevices[resourceName] = sets.NewString()</span><br><span class="line">    m.allDevices[resourceName] = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]pluginapi.Device)</span><br><span class="line">    <span class="keyword">for</span> _, dev := <span class="keyword">range</span> devices &#123;</span><br><span class="line">        m.allDevices[resourceName][dev.ID] = dev</span><br><span class="line">        <span class="keyword">if</span> dev.Health == pluginapi.Healthy &#123;</span><br><span class="line">            m.healthyDevices[resourceName].Insert(dev.ID)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            m.unhealthyDevices[resourceName].Insert(dev.ID)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    m.mutex.Unlock()</span><br><span class="line">    <span class="keyword">if</span> err := m.writeCheckpoint(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Errorf(<span class="string">"writing checkpoint encountered %v"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来到了 <code>DeviceManager</code> 启动的方法，它读取了 <code>checkpoint file</code> 中的数据，恢复 <code>ManagerImpl</code>中的相关数据，包括：</p><ul><li>podDevices</li><li>allocatedDevices</li><li>healthyDevices</li><li>unhealthyDevices</li><li>endpoints</li></ul><p>然后将 <code>/var/lib/kubelet/device-plugins/</code> 下面的除了 <code>checkpiont文件</code> 的所有文件清空，也就是清空所有的socket文件，包括自己的 <code>kubelet.sock</code>，以及其他所有之前的 <code>DevicePlugin</code> 的socket文件。最后创建 <code>kubelet.sock</code> 并启动 <code>gRPC Server</code>对外提供gRPC服务，其中 <code>Register()</code>用于 <code>DevicePlugin</code> 调用进行插件注册。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">Start</span><span class="params">(activePods ActivePodsFunc, sourcesReady config.SourcesReady)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    klog.V(<span class="number">2</span>).Infof(<span class="string">"Starting Device Plugin manager"</span>)</span><br><span class="line"></span><br><span class="line">    m.activePods = activePods</span><br><span class="line">    m.sourcesReady = sourcesReady</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Loads in allocatedDevices information from disk.</span></span><br><span class="line">    err := m.readCheckpoint()</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Warningf(<span class="string">"Continue after failing to read checkpoint file. Device allocation info may NOT be up-to-date. Err: %v"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    socketPath := filepath.Join(m.socketdir, m.socketname)</span><br><span class="line">    <span class="keyword">if</span> err = os.MkdirAll(m.socketdir, <span class="number">0750</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> selinux.SELinuxEnabled() &#123;</span><br><span class="line">        <span class="keyword">if</span> err := selinux.SetFileLabel(m.socketdir, config.KubeletPluginsDirSELinuxLabel); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            klog.Warningf(<span class="string">"Unprivileged containerized plugins might not work. Could not set selinux context on %s: %v"</span>, m.socketdir, err)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Removes all stale sockets in m.socketdir. Device plugins can monitor</span></span><br><span class="line">    <span class="comment">// this and use it as a signal to re-register with the new Kubelet.</span></span><br><span class="line">    <span class="keyword">if</span> err := m.removeContents(m.socketdir); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Errorf(<span class="string">"Fail to clean up stale contents under %s: %v"</span>, m.socketdir, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    s, err := net.Listen(<span class="string">"unix"</span>, socketPath)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Errorf(errListenSocket+<span class="string">" %v"</span>, err)</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    m.wg.Add(<span class="number">1</span>)</span><br><span class="line">    m.server = grpc.NewServer([]grpc.ServerOption&#123;&#125;...)</span><br><span class="line"></span><br><span class="line">    pluginapi.RegisterRegistrationServer(m.server, m)</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">defer</span> m.wg.Done()</span><br><span class="line">        m.server.Serve(s)</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    klog.V(<span class="number">2</span>).Infof(<span class="string">"Serving device plugin registration server on %q"</span>, socketPath)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DeviceManager-注册"><a href="#DeviceManager-注册" class="headerlink" title="DeviceManager 注册"></a>DeviceManager 注册</h4><p><code>DeviceManager</code> 接收到 <code>DevicePlugin</code>的 RegisterRequest请求，其结构体如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> RegisterRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">   Version <span class="keyword">string</span></span><br><span class="line">   Endpoint <span class="keyword">string</span> </span><br><span class="line">   ResourceName <span class="keyword">string</span> </span><br><span class="line">   Options   *DevicePluginOptions </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>检查注册的device Name、version是否符合 <code>Extended Resource</code> 的规则，Name不能属于kubernetes.i  o，得有自己的domain，比如<code>nvidia.com</code></p><p>根据 <code>endpoint</code> 信息创建 <code>EndpointImpl</code> 对象，即根据 <code>endpoint</code> 建立 <code>socket</code> 连接：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">RegisterPlugin</span><span class="params">(pluginName <span class="keyword">string</span>, endpoint <span class="keyword">string</span>, versions []<span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    klog.V(<span class="number">2</span>).Infof(<span class="string">"Registering Plugin %s at endpoint %s"</span>, pluginName, endpoint)</span><br><span class="line"></span><br><span class="line">    e, err := newEndpointImpl(endpoint, pluginName, m.callback)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to dial device plugin with socketPath %s: %v"</span>, endpoint, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    options, err := e.client.GetDevicePluginOptions(context.Background(), &amp;pluginapi.Empty&#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to get device plugin options: %v"</span>, err)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    m.registerEndpoint(pluginName, options, e)</span><br><span class="line">    <span class="keyword">go</span> m.runEndpoint(pluginName, e)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是 <code>endPointsImpl</code>  的具体实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> endpointImpl <span class="keyword">struct</span> &#123;</span><br><span class="line">    client     pluginapi.DevicePluginClient</span><br><span class="line">    clientConn *grpc.ClientConn</span><br><span class="line"></span><br><span class="line">    socketPath   <span class="keyword">string</span></span><br><span class="line">    resourceName <span class="keyword">string</span></span><br><span class="line">    stopTime     time.Time</span><br><span class="line"></span><br><span class="line">    mutex sync.Mutex</span><br><span class="line">    cb    monitorCallback</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newEndpointImpl</span><span class="params">(socketPath, resourceName <span class="keyword">string</span>, callback monitorCallback)</span> <span class="params">(*endpointImpl, error)</span></span> &#123;</span><br><span class="line">    client, c, err := dial(socketPath)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Errorf(<span class="string">"Can't create new endpoint with path %s err %v"</span>, socketPath, err)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &amp;endpointImpl&#123;</span><br><span class="line">        client:     client,</span><br><span class="line">        clientConn: c,</span><br><span class="line"></span><br><span class="line">        socketPath:   socketPath,</span><br><span class="line">        resourceName: resourceName,</span><br><span class="line"></span><br><span class="line">        cb: callback,</span><br><span class="line">    &#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行 <code>EndpointImpl</code> 对象的 <code>run()</code>，在 <code>run</code>方法中:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(e *endpointImpl)</span> <span class="title">run</span><span class="params">()</span></span> &#123;</span><br><span class="line">    stream, err := e.client.ListAndWatch(context.Background(), &amp;pluginapi.Empty&#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.Errorf(errListAndWatch, e.resourceName, err)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        response, err := stream.Recv()</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            klog.Errorf(errListAndWatch, e.resourceName, err)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        devs := response.Devices</span><br><span class="line">        klog.V(<span class="number">2</span>).Infof(<span class="string">"State pushed for device plugin %s"</span>, e.resourceName)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> newDevs []pluginapi.Device</span><br><span class="line">        <span class="keyword">for</span> _, d := <span class="keyword">range</span> devs &#123;</span><br><span class="line">            newDevs = <span class="built_in">append</span>(newDevs, *d)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        e.callback(e.resourceName, newDevs)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>调用 <code>DevicePlugin</code> 的<code>ListAndWatch gRPC</code> 接口，通过长连接持续获取 <code>ListAndWatch gRPC stream</code></li><li>从 <code>stream</code> 流中获取的devices详情列表然后调用Endpoint的 <code>callback</code>，也就是 <code>ManagerImpl</code> 注册的callback方法<code>genericDeviceUpdateCallback</code>进行Device Manager的缓存更新并写到checkpoint文件中</li><li>run()是通过协程启动的，持续获取device server的ListAndWatch结果，持续更新device状态</li><li>当获取异常时，deviceManager断开连接，将device设置为不健康的状态。</li></ul><h3 id="ListAndWatch"><a href="#ListAndWatch" class="headerlink" title="ListAndWatch"></a>ListAndWatch</h3><p>看一下 <code>DevicePlugin</code> 实现的 <code>ListAndWatch</code>，先是立马返回device详情列表，然后开启协程，一旦感知device的健康状态发生变化了，更新 <code>device</code> 详情列表再次返回给 <code>deviceManager</code>。回想起健康检查，<code>DevicePlugin</code> 的 <code>CheckHealth</code> 就就会将设备的健康状态传递给 <code>m.health</code> 这个 <code>channel</code>。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">func (m *NvidiaDevicePlugin) ListAndWatch(e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer) error &#123;</span><br><span class="line">    s.Send(&amp;pluginapi.ListAndWatchResponse&#123;<span class="attr">Devices</span>: m.apiDevices()&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        select &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="xml"><span class="tag">&lt;<span class="name">-m.stop:</span></span></span></span><br><span class="line"><span class="xml">            return nil</span></span><br><span class="line">        case d := &lt;-m.health:</span><br><span class="line">            // FIXME: there is no way to recover from the Unhealthy state.</span><br><span class="line">            d.Health = pluginapi.Unhealthy</span><br><span class="line">            log.Printf("'%s' device marked unhealthy: %s", m.resourceName, d.ID)</span><br><span class="line">            s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: m.apiDevices()&#125;)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么问题来了，<code>DevicePlugin</code> 是如何知道有多少 <code>Device</code> 的呢？我们看看 <code>apiDevices</code> 的实现：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">apiDevices</span><span class="params">()</span> []*<span class="title">pluginapi</span>.<span class="title">Device</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> pdevs []*pluginapi.Device</span><br><span class="line">    <span class="keyword">for</span> _, d := <span class="keyword">range</span> m.cachedDevices &#123;</span><br><span class="line">        pdevs = <span class="built_in">append</span>(pdevs, &amp;d.Device)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pdevs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的 <code>cachedDevices</code> 是通过 <code>ResourceManager</code> 获得的 <code>Device</code> 信息，其具体通过 <code>GpuDeviceManager</code> 结构来实现，可以看到它们是调用了 <code>nvml</code> 库而实现的。这里还有一个 <code>MigDeviceManager</code> 本质上相同，不再概述。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(g *GpuDeviceManager)</span> <span class="title">Devices</span><span class="params">()</span> []*<span class="title">Device</span></span> &#123;</span><br><span class="line">    n, err := nvml.GetDeviceCount()</span><br><span class="line">    check(err)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> devs []*Device</span><br><span class="line">    <span class="keyword">for</span> i := <span class="keyword">uint</span>(<span class="number">0</span>); i &lt; n; i++ &#123;</span><br><span class="line">        d, err := nvml.NewDeviceLite(i)</span><br><span class="line">        check(err)</span><br><span class="line"></span><br><span class="line">        migEnabled, err := d.IsMigEnabled()</span><br><span class="line">        check(err)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> migEnabled &amp;&amp; g.skipMigEnabledGPUs &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        devs = <span class="built_in">append</span>(devs, buildDevice(d))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> devs</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Allocation"><a href="#Allocation" class="headerlink" title="Allocation"></a>Allocation</h3><p><code>kubelet</code> 接收到被调度到本节点的pods后</p><h4 id="HandlePodAdditions"><a href="#HandlePodAdditions" class="headerlink" title="HandlePodAdditions"></a>HandlePodAdditions</h4><p>当 Node 上的 <code>Kubelet</code> 监听到有新的 <code>Pod</code> 创建时，会调用 <code>HandlerPodAdditions</code> 来处理 <code>Pod</code> 创建的事件。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">syncLoopIteration</span><span class="params">(configCh &lt;-<span class="keyword">chan</span> kubetypes.PodUpdate, handler SyncHandler,</span></span></span><br><span class="line"><span class="function"><span class="params">    syncCh &lt;-<span class="keyword">chan</span> time.Time, housekeepingCh &lt;-<span class="keyword">chan</span> time.Time, plegCh &lt;-<span class="keyword">chan</span> *pleg.PodLifecycleEvent)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">select</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> u, open := &lt;-configCh:</span><br><span class="line">        <span class="keyword">switch</span> u.Op &#123;</span><br><span class="line">        <span class="keyword">case</span> kubetypes.ADD:</span><br><span class="line">            klog.V(<span class="number">2</span>).Infof(<span class="string">"SyncLoop (ADD, %q): %q"</span>, u.Source, format.Pods(u.Pods))</span><br><span class="line">            handler.HandlePodAdditions(u.Pods)</span><br><span class="line">        <span class="keyword">case</span> kubetypes.UPDATE:</span><br><span class="line">            klog.V(<span class="number">2</span>).Infof(<span class="string">"SyncLoop (UPDATE, %q): %q"</span>, u.Source, format.PodsWithDeletionTimestamps(u.Pods))</span><br><span class="line">            handler.HandlePodUpdates(u.Pods)</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">case</span> e := &lt;-plegCh:</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来进一步看下 <code>HandlerPodAdditions</code> 的实现，对于传入的每一个 <code>Pod</code> ，如果它没有被 <code>terminate</code>，则通过 <code>canAdmitPod</code> 检查是否可以允许该 <code>Pod</code> 创建。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">HandlePodAdditions</span><span class="params">(pods []*v1.Pod)</span></span> &#123;</span><br><span class="line">    start := kl.clock.Now()</span><br><span class="line">    sort.Sort(sliceutils.PodsByCreationTime(pods))</span><br><span class="line">    <span class="keyword">for</span> _, pod := <span class="keyword">range</span> pods &#123;</span><br><span class="line">        existingPods := kl.podManager.GetPods()</span><br><span class="line">        kl.podManager.AddPod(pod)</span><br><span class="line">   </span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">        <span class="keyword">if</span> !kl.podIsTerminated(pod) &#123;</span><br><span class="line">            activePods := kl.filterOutTerminatedPods(existingPods)</span><br><span class="line">            <span class="keyword">if</span> ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123;</span><br><span class="line">                kl.rejectPod(pod, reason, message)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>canAdmitPod</code> 里面，<code>Kubelet</code> 将会依次执行每一个 <code>admit handler</code> 来看 Pod 能否通过。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// "pod" is new pod, while "pods" are all admitted pods</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">canAdmitPod</span><span class="params">(pods []*v1.Pod, pod *v1.Pod)</span> <span class="params">(<span class="keyword">bool</span>, <span class="keyword">string</span>, <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">    attrs := &amp;lifecycle.PodAdmitAttributes&#123;Pod: pod, OtherPods: pods&#125;</span><br><span class="line">    <span class="keyword">for</span> _, podAdmitHandler := <span class="keyword">range</span> kl.admitHandlers &#123;</span><br><span class="line">        <span class="keyword">if</span> result := podAdmitHandler.Admit(attrs); !result.Admit &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>, result.Reason, result.Message</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>, <span class="string">""</span>, <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>admitHandlers</code> 是一个 <code>PodAdmitHandler</code> 的切片，其接口如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> PodAdmitHandler <span class="keyword">interface</span> &#123;</span><br><span class="line">    <span class="comment">// Admit evaluates if a pod can be admitted.</span></span><br><span class="line">    Admit(attrs *PodAdmitAttributes) PodAdmitResult</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Kubelet</code> 在创建的时候会添加一系列的 <code>PodAdmitHandler</code> 用于检查，对pod的资源做一些准入判断，比如：</p><ul><li><code>evictionAdmitHandler</code> :当节点有内存压力时，拒绝创建best effort的pod，还有其它条件先略过</li><li><code>TopologyPodAdmitHandler</code>：拒绝创建因为Topology locality冲突而无法分配资源的pod</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)</span><br><span class="line">  klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler())</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>与我们 <code>DevicePlugin</code> 相关的则是 <code>containerManager</code> 的 <code>resourceAllocator</code>，这里会分别调用 <code>DeviceManager</code> 和 <code>CpuManager</code> 的 <code>Allocate</code> 函数，看是否能够申请到相关的资源。这里会对 Pod 的每一个 <code>InitContainer</code> 和 <code>Container</code>检查，看能否申请到。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *resourceAllocator)</span> <span class="title">Admit</span><span class="params">(attrs *lifecycle.PodAdmitAttributes)</span> <span class="title">lifecycle</span>.<span class="title">PodAdmitResult</span></span> &#123;</span><br><span class="line">    pod := attrs.Pod</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, container := <span class="keyword">range</span> <span class="built_in">append</span>(pod.Spec.InitContainers, pod.Spec.Containers...) &#123;</span><br><span class="line">        err := m.deviceManager.Allocate(pod, &amp;container)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> lifecycle.PodAdmitResult&#123;</span><br><span class="line">                Message: fmt.Sprintf(<span class="string">"Allocate failed due to %v, which is unexpected"</span>, err),</span><br><span class="line">                Reason:  <span class="string">"UnexpectedAdmissionError"</span>,</span><br><span class="line">                Admit:   <span class="literal">false</span>,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> m.cpuManager != <span class="literal">nil</span> &#123;</span><br><span class="line">            err = m.cpuManager.Allocate(pod, &amp;container)</span><br><span class="line">            <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> lifecycle.PodAdmitResult&#123;</span><br><span class="line">                    Message: fmt.Sprintf(<span class="string">"Allocate failed due to %v, which is unexpected"</span>, err),</span><br><span class="line">                    Reason:  <span class="string">"UnexpectedAdmissionError"</span>,</span><br><span class="line">                    Admit:   <span class="literal">false</span>,</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> lifecycle.PodAdmitResult&#123;Admit: <span class="literal">true</span>&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来我们看 <code>ManagerImpl</code> 的 <code>Allocate</code> 函数实现。</p><h4 id="ManagerImpl-Allocate"><a href="#ManagerImpl-Allocate" class="headerlink" title="ManagerImpl.Allocate"></a>ManagerImpl.Allocate</h4><ul><li>allocateContainerResources为Pod中的init container分配devices，并更新deviceManager中PodDevices缓存；</li><li><code>allocateContainerResources为</code> Pod中的regular container分配devices，并更新deviceManager中PodDevices缓存<ul><li>每次在为Pod分配devices之前，都去检查一下此时的active pods，并与podDevices缓存中的pods进行比对，将已经terminated的Pods的devices从podDevices中删除，即进行了devices的GC操作。</li><li>从 <code>healthyDevices</code> 中随机分配对应数量的devices给该Pod，并注意更新allocatedDevices，否则会导致一个device被分配给多个Pod。</li><li>拿到devices后，就通过Grpc调用 <code>DevicePlugin</code> 的 <code>Allocate</code>方法，<code>DevicePlugin</code> 返回 <code>ContainerAllocateResponse</code> (包括注入的环境变量、挂载信息、Annotations)，<code>deviceManager</code> </li><li>根据 <code>pod uuid</code> 和 <code>container name</code> 将返回的信息存入 <code>podDevices</code> 缓存，更新 <code>podDevices</code> 缓存信息，并将<code>deviceManager</code> 中缓存数据更新到 <code>checkpoint</code> 文件中。</li></ul></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">Allocate</span><span class="params">(pod *v1.Pod, container *v1.Container)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> _, ok := m.devicesToReuse[<span class="keyword">string</span>(pod.UID)]; !ok &#123;</span><br><span class="line">        m.devicesToReuse[<span class="keyword">string</span>(pod.UID)] = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]sets.String)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// If pod entries to m.devicesToReuse other than the current pod exist, delete them.</span></span><br><span class="line">    <span class="keyword">for</span> podUID := <span class="keyword">range</span> m.devicesToReuse &#123;</span><br><span class="line">        <span class="keyword">if</span> podUID != <span class="keyword">string</span>(pod.UID) &#123;</span><br><span class="line">            <span class="built_in">delete</span>(m.devicesToReuse, podUID)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Allocate resources for init containers first as we know the caller always loops</span></span><br><span class="line">    <span class="comment">// through init containers before looping through app containers. Should the caller</span></span><br><span class="line">    <span class="comment">// ever change those semantics, this logic will need to be amended.</span></span><br><span class="line">    <span class="keyword">for</span> _, initContainer := <span class="keyword">range</span> pod.Spec.InitContainers &#123;</span><br><span class="line">        <span class="keyword">if</span> container.Name == initContainer.Name &#123;</span><br><span class="line">            <span class="keyword">if</span> err := m.allocateContainerResources(pod, container, m.devicesToReuse[<span class="keyword">string</span>(pod.UID)]); err != <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> err</span><br><span class="line">            &#125;</span><br><span class="line">            m.podDevices.addContainerAllocatedResources(<span class="keyword">string</span>(pod.UID), container.Name, m.devicesToReuse[<span class="keyword">string</span>(pod.UID)])</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> err := m.allocateContainerResources(pod, container, m.devicesToReuse[<span class="keyword">string</span>(pod.UID)]); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> err</span><br><span class="line">    &#125;</span><br><span class="line">    m.podDevices.removeContainerAllocatedResources(<span class="keyword">string</span>(pod.UID), container.Name, m.devicesToReuse[<span class="keyword">string</span>(pod.UID)])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来我们看 <code>allocateContainerResource</code> 的实现，因为扩展资源是<code>DevicePlugin</code> 所发现的，而扩展资源不允许过量提交，因此要求容器中的 <code>Request</code> 与 <code>Limits</code> 相等，并且 <code>DevicePlugin</code> 会遍历所有的 <code>Limits</code> 保证资源是充足的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">allocateContainerResources</span><span class="params">(pod *v1.Pod, container *v1.Container, devicesToReuse <span class="keyword">map</span>[<span class="keyword">string</span>]sets.String)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    podUID := <span class="keyword">string</span>(pod.UID)</span><br><span class="line">    contName := container.Name</span><br><span class="line">    allocatedDevicesUpdated := <span class="literal">false</span></span><br><span class="line">    <span class="comment">// Extended resources are not allowed to be overcommitted.</span></span><br><span class="line">    <span class="comment">// Since device plugin advertises extended resources,</span></span><br><span class="line">    <span class="comment">// therefore Requests must be equal to Limits and iterating</span></span><br><span class="line">    <span class="comment">// over the Limits should be sufficient.</span></span><br><span class="line">    <span class="keyword">for</span> k, v := <span class="keyword">range</span> container.Resources.Limits &#123;</span><br><span class="line">        resource := <span class="keyword">string</span>(k)</span><br><span class="line">        needed := <span class="keyword">int</span>(v.Value())</span><br><span class="line">        klog.V(<span class="number">3</span>).Infof(<span class="string">"needs %d %s"</span>, needed, resource)</span><br><span class="line">        <span class="keyword">if</span> !m.isDevicePluginResource(resource) &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Updates allocatedDevices to garbage collect any stranded resources</span></span><br><span class="line">        <span class="comment">// before doing the device plugin allocation.</span></span><br><span class="line">        <span class="keyword">if</span> !allocatedDevicesUpdated &#123;</span><br><span class="line">            m.UpdateAllocatedDevices()</span><br><span class="line">            allocatedDevicesUpdated = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        allocDevices, err := m.devicesToAllocate(podUID, contName, resource, needed, devicesToReuse[resource])</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> allocDevices == <span class="literal">nil</span> || <span class="built_in">len</span>(allocDevices) &lt;= <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        startRPCTime := time.Now()</span><br><span class="line">        <span class="comment">// Manager.Allocate involves RPC calls to device plugin, which</span></span><br><span class="line">        <span class="comment">// could be heavy-weight. Therefore we want to perform this operation outside</span></span><br><span class="line">        <span class="comment">// mutex lock. Note if Allocate call fails, we may leave container resources</span></span><br><span class="line">        <span class="comment">// partially allocated for the failed container. We rely on UpdateAllocatedDevices()</span></span><br><span class="line">        <span class="comment">// to garbage collect these resources later. Another side effect is that if</span></span><br><span class="line">        <span class="comment">// we have X resource A and Y resource B in total, and two containers, container1</span></span><br><span class="line">        <span class="comment">// and container2 both require X resource A and Y resource B. Both allocation</span></span><br><span class="line">        <span class="comment">// requests may fail if we serve them in mixed order.</span></span><br><span class="line">        <span class="comment">// <span class="doctag">TODO:</span> may revisit this part later if we see inefficient resource allocation</span></span><br><span class="line">        <span class="comment">// in real use as the result of this. Should also consider to parallelize device</span></span><br><span class="line">        <span class="comment">// plugin Allocate grpc calls if it becomes common that a container may require</span></span><br><span class="line">        <span class="comment">// resources from multiple device plugins.</span></span><br><span class="line">        m.mutex.Lock()</span><br><span class="line">        eI, ok := m.endpoints[resource]</span><br><span class="line">        m.mutex.Unlock()</span><br><span class="line">        <span class="keyword">if</span> !ok &#123;</span><br><span class="line">            m.mutex.Lock()</span><br><span class="line">            m.allocatedDevices = m.podDevices.devices()</span><br><span class="line">            m.mutex.Unlock()</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"unknown Device Plugin %s"</span>, resource)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        devs := allocDevices.UnsortedList()</span><br><span class="line">        <span class="comment">// <span class="doctag">TODO:</span> refactor this part of code to just append a ContainerAllocationRequest</span></span><br><span class="line">        <span class="comment">// in a passed in AllocateRequest pointer, and issues a single Allocate call per pod.</span></span><br><span class="line">        klog.V(<span class="number">3</span>).Infof(<span class="string">"Making allocation request for devices %v for device plugin %s"</span>, devs, resource)</span><br><span class="line">        resp, err := eI.e.allocate(devs)</span><br><span class="line">        metrics.DevicePluginAllocationDuration.WithLabelValues(resource).Observe(metrics.SinceInSeconds(startRPCTime))</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="comment">// In case of allocation failure, we want to restore m.allocatedDevices</span></span><br><span class="line">            <span class="comment">// to the actual allocated state from m.podDevices.</span></span><br><span class="line">            m.mutex.Lock()</span><br><span class="line">            m.allocatedDevices = m.podDevices.devices()</span><br><span class="line">            m.mutex.Unlock()</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(resp.ContainerResponses) == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"no containers return in allocation response %v"</span>, resp)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update internal cached podDevices state.</span></span><br><span class="line">        m.mutex.Lock()</span><br><span class="line">        m.podDevices.insert(podUID, contName, resource, allocDevices, resp.ContainerResponses[<span class="number">0</span>])</span><br><span class="line">        m.mutex.Unlock()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Checkpoints device to container allocation information.</span></span><br><span class="line">    <span class="keyword">return</span> m.writeCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看到，这里通过 <code>resp, err := eI.e.allocate(devs)</code> 执行 RPC 调用，进入到了 <code>DevicePlugin</code> 的逻辑。这里有一个问题，<code>RPC</code> 远程调用中的 <code>deviceIDs</code> 参数是怎么来的呢？我们看到这里有一个 <code>devicesToAllocate</code>的调用。这里的主要逻辑如下：</p><ul><li>拿到对应Pod的对应容器已经申请的资源的设备列表，检查是否只申请了部分，如果只有一部分，那么报错</li><li>然后从 <code>resuableDevices</code> 结构中拿到可以使用的设备列表，如果可用的足够则返回，否则继续从 <code>healthyDevices</code> 中找</li><li>从 <code>healthyDevices</code> 去掉已经在使用的设备，然后检查是否足够，如果不够则报错</li><li>如果足够的话，根据是否有满足拓扑亲和性去拿到足够的设备列表</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">devicesToAllocate</span><span class="params">(podUID, contName, resource <span class="keyword">string</span>, required <span class="keyword">int</span>, reusableDevices sets.String)</span> <span class="params">(sets.String, error)</span></span> &#123;</span><br><span class="line">    m.mutex.Lock()</span><br><span class="line">    <span class="keyword">defer</span> m.mutex.Unlock()</span><br><span class="line">    needed := required</span><br><span class="line">    <span class="comment">// Gets list of devices that have already been allocated.</span></span><br><span class="line">    <span class="comment">// This can happen if a container restarts for example.</span></span><br><span class="line">    devices := m.podDevices.containerDevices(podUID, contName, resource)</span><br><span class="line">    <span class="keyword">if</span> devices != <span class="literal">nil</span> &#123;</span><br><span class="line">        klog.V(<span class="number">3</span>).Infof(<span class="string">"Found pre-allocated devices for resource %s container %q in Pod %q: %v"</span>, resource, contName, podUID, devices.List())</span><br><span class="line">        needed = needed - devices.Len()</span><br><span class="line">        <span class="comment">// A pod's resource is not expected to change once admitted by the API server,</span></span><br><span class="line">        <span class="comment">// so just fail loudly here. We can revisit this part if this no longer holds.</span></span><br><span class="line">        <span class="keyword">if</span> needed != <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"pod %q container %q changed request for resource %q from %d to %d"</span>, podUID, contName, resource, devices.Len(), required)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> needed == <span class="number">0</span> &#123;</span><br><span class="line">        <span class="comment">// No change, no work.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    klog.V(<span class="number">3</span>).Infof(<span class="string">"Needs to allocate %d %q for pod %q container %q"</span>, needed, resource, podUID, contName)</span><br><span class="line">    <span class="comment">// Needs to allocate additional devices.</span></span><br><span class="line">    <span class="keyword">if</span> _, ok := m.healthyDevices[resource]; !ok &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"can't allocate unregistered device %s"</span>, resource)</span><br><span class="line">    &#125;</span><br><span class="line">    devices = sets.NewString()</span><br><span class="line">    <span class="comment">// Allocates from reusableDevices list first.</span></span><br><span class="line">    <span class="keyword">for</span> device := <span class="keyword">range</span> reusableDevices &#123;</span><br><span class="line">        devices.Insert(device)</span><br><span class="line">        needed--</span><br><span class="line">        <span class="keyword">if</span> needed == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> devices, <span class="literal">nil</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Needs to allocate additional devices.</span></span><br><span class="line">    <span class="keyword">if</span> m.allocatedDevices[resource] == <span class="literal">nil</span> &#123;</span><br><span class="line">        m.allocatedDevices[resource] = sets.NewString()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Gets Devices in use.</span></span><br><span class="line">    devicesInUse := m.allocatedDevices[resource]</span><br><span class="line">    <span class="comment">// Gets a list of available devices.</span></span><br><span class="line">    available := m.healthyDevices[resource].Difference(devicesInUse)</span><br><span class="line">    <span class="keyword">if</span> available.Len() &lt; needed &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"requested number of devices unavailable for %s. Requested: %d, Available: %d"</span>, resource, needed, available.Len())</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// By default, pull devices from the unsorted list of available devices.</span></span><br><span class="line">    allocated := available.UnsortedList()[:needed]</span><br><span class="line">    <span class="comment">// If topology alignment is desired, update allocated to the set of devices</span></span><br><span class="line">    <span class="comment">// with the best alignment.</span></span><br><span class="line">    hint := m.topologyAffinityStore.GetAffinity(podUID, contName)</span><br><span class="line">    <span class="keyword">if</span> m.deviceHasTopologyAlignment(resource) &amp;&amp; hint.NUMANodeAffinity != <span class="literal">nil</span> &#123;</span><br><span class="line">        allocated = m.takeByTopology(resource, available, hint.NUMANodeAffinity, needed)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Updates m.allocatedDevices with allocated devices to prevent them</span></span><br><span class="line">    <span class="comment">// from being allocated to other pods/containers, given that we are</span></span><br><span class="line">    <span class="comment">// not holding lock during the rpc call.</span></span><br><span class="line">    <span class="keyword">for</span> _, device := <span class="keyword">range</span> allocated &#123;</span><br><span class="line">        m.allocatedDevices[resource].Insert(device)</span><br><span class="line">        devices.Insert(device)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> devices, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>RPC</code> 调用成功后，会将对应的 <code>Response</code> 记录到 <code>m.podDevices</code> 中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pdev podDevices)</span> <span class="title">insert</span><span class="params">(podUID, contName, resource <span class="keyword">string</span>, devices sets.String, resp *pluginapi.ContainerAllocateResponse)</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> _, podExists := pdev[podUID]; !podExists &#123;</span><br><span class="line">        pdev[podUID] = <span class="built_in">make</span>(containerDevices)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> _, contExists := pdev[podUID][contName]; !contExists &#123;</span><br><span class="line">        pdev[podUID][contName] = <span class="built_in">make</span>(resourceAllocateInfo)</span><br><span class="line">    &#125;</span><br><span class="line">    pdev[podUID][contName][resource] = deviceAllocateInfo&#123;</span><br><span class="line">        deviceIds: devices,</span><br><span class="line">        allocResp: resp,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DevicePlugin-Allocate"><a href="#DevicePlugin-Allocate" class="headerlink" title="DevicePlugin.Allocate"></a>DevicePlugin.Allocate</h4><p><code>Allocate</code> 接口给容器加上 <code>NVIDIA_VISIBLE_DEVICES</code> 环境变量，设置了相关的 <code>DeviceSpec</code>参数，将 <code>Response</code> 返回给 <code>Kubelet</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *NvidiaDevicePlugin)</span> <span class="title">Allocate</span><span class="params">(ctx context.Context, reqs *pluginapi.AllocateRequest)</span> <span class="params">(*pluginapi.AllocateResponse, error)</span></span> &#123;</span><br><span class="line">    responses := pluginapi.AllocateResponse&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> _, req := <span class="keyword">range</span> reqs.ContainerRequests &#123;</span><br><span class="line">        <span class="keyword">for</span> _, id := <span class="keyword">range</span> req.DevicesIDs &#123;</span><br><span class="line">            <span class="keyword">if</span> !m.deviceExists(id) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"invalid allocation request for '%s': unknown device: %s"</span>, m.resourceName, id)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        response := pluginapi.ContainerAllocateResponse&#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> *deviceListStrategyFlag == DeviceListStrategyEnvvar &#123;</span><br><span class="line">            response.Envs = m.apiEnvs(m.deviceListEnvvar, req.DevicesIDs)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> *deviceListStrategyFlag == DeviceListStrategyVolumeMounts &#123;</span><br><span class="line">            response.Envs = m.apiEnvs(m.deviceListEnvvar, []<span class="keyword">string</span>&#123;deviceListAsVolumeMountsContainerPathRoot&#125;)</span><br><span class="line">            response.Mounts = m.apiMounts(req.DevicesIDs)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> *passDeviceSpecs &#123;</span><br><span class="line">            response.Devices = m.apiDeviceSpecs(req.DevicesIDs)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        responses.ContainerResponses = <span class="built_in">append</span>(responses.ContainerResponses, &amp;response)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &amp;responses, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前面我们提到， Nvidia的 <code>gpu-container-runtime</code> 根据容器的 <code>NVIDIA_VISIBLE_DEVICES</code> 环境变量，会决定这个容器是否为GPU容器，并且可以使用哪些GPU设备。 而Nvidia GPU device plugin做的事情，就是根据kubelet 请求中的GPU DeviceId， 转换为 <code>NVIDIA_VISIBLE_DEVICES</code> 环境变量返回给kubelet， kubelet收到返回内容后，会自动将返回的环境变量注入到容器中。当容器中包含环境变量，启动时 <code>gpu-container-runtime</code> 会根据 <code>NVIDIA_VISIBLE_DEVICES</code> 里声明的设备信息，将设备映射到容器中，并将对应的Nvidia Driver Lib 也映射到容器中。</p><h3 id="Device-的使用"><a href="#Device-的使用" class="headerlink" title="Device 的使用"></a>Device 的使用</h3><p>在kubelet的 <code>GetResource</code> 中，会调用 <code>DeviceManager</code> 的 <code>GetDeviceRunContainerOptions</code>，并将这些 <code>options</code>添加到<code>kubecontainer.RunContainerOptions</code> 中。<code>RunContainerOptions</code> 包括 <code>Envs</code>、<code>Mounts</code>、<code>Devices</code>、<code>PortMappings</code>、<code>Annotations</code>等信息。kubelet调用 <code>GetResources()</code> 为启动<code>container</code>获取启动参数 <code>runtimeapi.ContainerConfig{Args...}</code></p><figure class="highlight go"><figcaption><span>kubernetes/pkg/kubelet/cm/container_manager_linux.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cm *containerManagerImpl)</span> <span class="title">GetResources</span><span class="params">(pod *v1.Pod, container *v1.Container)</span> <span class="params">(*kubecontainer.RunContainerOptions, error)</span></span> &#123;</span><br><span class="line">    opts := &amp;kubecontainer.RunContainerOptions&#123;&#125;</span><br><span class="line">    <span class="comment">// Allocate should already be called during predicateAdmitHandler.Admit(),</span></span><br><span class="line">    <span class="comment">// just try to fetch device runtime information from cached state here</span></span><br><span class="line">    devOpts, err := cm.deviceManager.GetDeviceRunContainerOptions(pod, container)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> devOpts == <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> opts, <span class="literal">nil</span></span><br><span class="line">    &#125;</span><br><span class="line">    opts.Devices = <span class="built_in">append</span>(opts.Devices, devOpts.Devices...)</span><br><span class="line">    opts.Mounts = <span class="built_in">append</span>(opts.Mounts, devOpts.Mounts...)</span><br><span class="line">    opts.Envs = <span class="built_in">append</span>(opts.Envs, devOpts.Envs...)</span><br><span class="line">    opts.Annotations = <span class="built_in">append</span>(opts.Annotations, devOpts.Annotations...)</span><br><span class="line">    <span class="keyword">return</span> opts, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>GetDeviceRunContainerOptions()</code> 根据 <code>pod uuid</code> 和 <code>container name</code> 从 <code>podDevices</code> 缓存（device的分配过程中会设置缓存数据）中取出Envs、Mounts、Devices、PortMappings、Annotations等信息，另外对于一些PreStartRequired为true的 <code>DevicePlugin</code>，deviceManager需要在启动container之前调用 <code>DevicePlugin</code>的 <code>PreStartContainer</code>grpc接口，做一些device的初始化工作，超时时间限制为30秒。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *ManagerImpl)</span> <span class="title">GetDeviceRunContainerOptions</span><span class="params">(pod *v1.Pod, container *v1.Container)</span> <span class="params">(*DeviceRunContainerOptions, error)</span></span> &#123;</span><br><span class="line">    podUID := <span class="keyword">string</span>(pod.UID)</span><br><span class="line">    contName := container.Name</span><br><span class="line">    needsReAllocate := <span class="literal">false</span></span><br><span class="line">    <span class="keyword">for</span> k := <span class="keyword">range</span> container.Resources.Limits &#123;</span><br><span class="line">        resource := <span class="keyword">string</span>(k)</span><br><span class="line">        <span class="keyword">if</span> !m.isDevicePluginResource(resource) &#123;</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        &#125;</span><br><span class="line">        err := m.callPreStartContainerIfNeeded(podUID, contName, resource)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// This is a device plugin resource yet we don't have cached</span></span><br><span class="line">        <span class="comment">// resource state. This is likely due to a race during node</span></span><br><span class="line">        <span class="comment">// restart. We re-issue allocate request to cover this race.</span></span><br><span class="line">        <span class="keyword">if</span> m.podDevices.containerDevices(podUID, contName, resource) == <span class="literal">nil</span> &#123;</span><br><span class="line">            needsReAllocate = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> needsReAllocate &#123;</span><br><span class="line">        klog.V(<span class="number">2</span>).Infof(<span class="string">"needs re-allocate device plugin resources for pod %s, container %s"</span>, podUID, container.Name)</span><br><span class="line">        <span class="keyword">if</span> err := m.Allocate(pod, container); err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    m.mutex.Lock()</span><br><span class="line">    <span class="keyword">defer</span> m.mutex.Unlock()</span><br><span class="line">    <span class="keyword">return</span> m.podDevices.deviceRunContainerOptions(<span class="keyword">string</span>(pod.UID), container.Name), <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Device-的状态管理"><a href="#Device-的状态管理" class="headerlink" title="Device 的状态管理"></a>Device 的状态管理</h3><p>device的状态管理涉及到以下3个部分：</p><ul><li>node上的device状态管理当kubelet更新node status时会调用GetCapacity更新device plugins对应的Resource信息。</li></ul><p>kubelet_node_status.go调用deviceManager的GetCapacity()获取device的状态，将device状态添加到node info并通过kube-apiserver存入etcd，GetCapacity()返回device server含有的所有device、已经分配给pod使用的device、pod不能使用的device即no-active的device kubelet_node_status.go根据返回的数据更新node info</p><ul><li>kubelet deviceManager服务的device状态管理其实在device的注册、device分配中都有讲解，即使用checkpoint机制默认是将podDevices以 PodDevicesEntry的格式存入<em>/var/lib/kubelet/device-plugins/kubelet_internal_checkpoint 文件</em></li></ul><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">type PodDevicesEntry struct &#123;</span><br><span class="line">   PodUID        string</span><br><span class="line">   ContainerName string</span><br><span class="line">   ResourceName  string</span><br><span class="line">   DeviceIDs     []string</span><br><span class="line">   AllocResp     []byte     <span class="comment">//包含启动container时使用的Envs、Mounts、Devices、PortMappings、Annotations等信息</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>只要device的状态发生了变化（如注册新device、device被分配、device的健康状态发生变化、device被删除），就要将podDevices存入<em>kubelet_internal_checkpoint 文件。kubelet在启动或重启时，都需要读取kubelet_internal_checkpoint 文件里的数据，并以podDevices格式存入podDevices缓存。</em></p><ul><li><code>DevicePlugin</code> 上报device状态在device的注册部分已经讲解过，归纳为<ul><li><code>deviceManager</code> 注册完 <code>DevicePlugin</code> 后，会跟 <code>DevicePlugin</code> 建立长连接，持续获取 <code>DevicePlugin</code> 的ListAndWatch结果，持续更新device状态；</li><li>当获取异常时，<code>deviceManager</code>断开连接，将device设置为不健康的状态；</li><li><code>DevicePlugin</code> 默认会重启重新注册，重新上报device的状态</li></ul></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank" rel="external nofollow noopener noreferrer">Kubernetes device plugin design proposal</a></li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/plugin-watcher.md" target="_blank" rel="external nofollow noopener noreferrer">Kubernetes plugin watcher design proposal</a></li><li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="external nofollow noopener noreferrer">Nvidia Device Plugin</a></li><li><a href="https://cloud.tencent.com/developer/article/1592800" target="_blank" rel="external nofollow noopener noreferrer">https://cloud.tencent.com/developer/article/1592800</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 原生支持对于CPU和内存资源的发现，但是有很多其他的设备 kubelet不能原生处理，比如GPU、FPGA、RDMA、存储设备和其他类似的异构计算资源设备。为了能够使用这些设备资源，我们需要进行各个设备的初始化和设置。按照 Kubernetes 的 &lt;code&gt;OutOfTree&lt;/code&gt; 的哲学理念，我们不应该把各个厂商的设备初始化设置相关代码与 Kubernetes 核心代码放在一起。与之相反，我们需要一种机制能够让各个设备厂商向 Kubelet 上报设备资源，而不需要修改 Kubernetes 核心代码。这即是 &lt;code&gt;Device Plugin&lt;/code&gt; 这一机制的来源，本文将介绍 Device Plugin 的实现原理，并介绍其使用。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_k8s-device-plugin.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="k8s" scheme="http://houmin.cc/tags/k8s/"/>
    
      <category term="GPU" scheme="http://houmin.cc/tags/GPU/"/>
    
      <category term="device plugin" scheme="http://houmin.cc/tags/device-plugin/"/>
    
      <category term="RDMA" scheme="http://houmin.cc/tags/RDMA/"/>
    
      <category term="FPGA" scheme="http://houmin.cc/tags/FPGA/"/>
    
  </entry>
  
  <entry>
    <title>GPU 与 CUDA 编程入门</title>
    <link href="http://houmin.cc/posts/5004f8e5/"/>
    <id>http://houmin.cc/posts/5004f8e5/</id>
    <published>2020-11-15T05:16:10.000Z</published>
    <updated>2020-11-21T16:41:14.107Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>随着近年来深度学习的爆发，原来被用于图形渲染的GPU被大量用于并行加速深度学习的模型训练中，在这个过程中 CUDA 作为 NVIDIA 推出的基于GPU的一个通用并行计算平台和编程模型也得到了广泛的使用。或许你已经十分了解 <a href="../b893097a/">现代CPU的体系架构</a>，但是对于GPU还不甚清晰，GPU的体系架构到底和CPU有何区别，CUDA模型是什么，我们该如何使用 CUDA实现并行计算，本文将为你扫盲祛魅，本文中使用到的所有代码可以在我的 <a href="https://github.com/SimpCosm/cuda-tutorial" target="_blank" rel="external nofollow noopener noreferrer">Github</a> 中找到。</p><a id="more"></a><h2 id="GPU-体系架构"><a href="#GPU-体系架构" class="headerlink" title="GPU 体系架构"></a>GPU 体系架构</h2><h3 id="为什么我们需要-GPU"><a href="#为什么我们需要-GPU" class="headerlink" title="为什么我们需要 GPU"></a>为什么我们需要 GPU</h3><p>如前所述，GPU （Graphics Processing Unit）最开始只是用于游戏、视频中的图形渲染，而现在最热门的一个应用领域是在深度学习的加速计算上。为什么需要 GPU 来加速计算呢？我们知道，随着摩尔定律的发展，在过去五十年间CPU的性能获得了巨大的提升，不论是从芯片上晶体管数目，还是时钟频率，到后来的从单核处理器发展到后来的多核多处理器。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-18_moores-law-develop.jpg"></p><p>下图是过去五十年间各款CPU处理器上晶体管数目的变化，基本上满足每18个月提升一倍的规律，虽然现在看起来50十年后摩尔定律对CPU来说有停滞的迹象（这是另一个话题，此处不表）</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-18_moores-law.png"></p><p>在 CPU 算力快速提升的这五十年，人们需要的计算量也同时在迅猛发展着，从最开始的桌面互联网，到后来的移动互联网，以及5年前爆发的深度学习，无一不需要庞大的计算力。在这个过程中，仅仅依靠CPU的算力开始力有不逮，这个过程中像GPU、FPGA、DSP等异构计算单元开始得到广泛的应用。下面，我回归计算的本质，以GPU为例来分析为什么我们需要这些异构计算单元。</p><p>无论是 CPU 还是 GPU，我们可以把计算模型抽象为下面这张图，这也是典型的冯诺伊曼体系架构。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_computing.png"></p><p>影响计算能力的4个主要因素如下：</p><ul><li><strong>Parallel Processing</strong>：Amount of data processed at one time</li><li><strong>Clock Frequency</strong>：Processing speed on each data element</li><li><strong>Memory Bandwidth</strong>：Amount of data transferred at one time</li><li><strong>Memory Lantency</strong>：Time for each data element to be transferred</li></ul><p>对于CPU，依次分析这几个因素：</p><ul><li>为了提供并行处理能力，我们从单核单处理器发展到多核多处理器，每个时钟周期CPU也能够处理多条指令</li><li>因为CPU时钟频率和功率的关系  $ Power = k <em> ClockFrequency </em> Voltage^2 $ ，在CPU过去的发展历史中，通过提高CPU时钟频率可以变得更快，与此同时为了保持CPU功耗的正常，也需要不断降低电压。但是当主频逐渐逼近到 4GHz 时，电压已经不能再降低了，因为这已经到达了晶体管高低电平反转的极限，关于这部分的更多内容可以参考 <a href="../">摩尔定律</a> 。</li><li>现在CPU用的是常规的DDR内存，明显存在着内存带宽限制</li><li>从CPU到DDR内存的延时很高，2020年的时候大概有100ns，具体可以参考 <a href="../fb3d782a/">Key Numbers Every Programmer Should Know</a>。CPU通过其他的方式隐藏了这个问题：<ul><li>Large On-Chip Low-Latency Cache，大概1ns</li><li>MultiThreading</li><li>Out-of-order execution</li></ul></li></ul><p><img alt="Credit to https://queue.acm.org/detail.cfm?id=2181798" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_processor-frequency-scaling.png"></p><p>尽管现在CPU的能力还在发展，但是以上的问题极大的限制了其算力的提高，当前仅靠CPU已经不能够满足人们对庞大算力的需求了。因此我们需要其他的专用芯片来帮助CPU一起计算，这就是异构计算的来源。GPU等专用计算单元虽然工作频率较低，但具有更多的内核数和并行计算能力，总体性能/芯片面积比和性能/功耗比都很高。随着人工智能时代的降临，GPU从游戏走进了人们的视野。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_cpu-vs-gpu.png"></p><p>无论是CPU还是GPU，在进行计算时都需要用核心（Core）来做算术逻辑运算。核心中有ALU（逻辑运算单元）和寄存器等电路。在进行计算时，一个核心只能顺序执行某项任务。CPU作为通用计算芯片，不仅仅做算术逻辑计算，其很重要的一部分功能是做复杂的逻辑控制，一般而言CPU上的Core数目相对较少，数据中心的服务器一般也就40左右个CPU核心。但是GPU动辄有上千个核心，这些核心可以独立的进行算术逻辑计算，大大提高了并行计算处理能力。</p><p>GPU时代的最大获益者是NVIDIA，当然AMD他们家也有GPU产品，但是因为AMD并没有形成CUDA这样的软件生态导致深度学习中主要用的都是NVIDIA的GPU，后面的分析都将基于NVIDIA的GPU产品。NVIDIA 不同时代产品的芯片设计不同，每代产品背后有一个架构代号，架构均以著名的物理学家为名，以向先贤致敬，对于消费者而言，英伟达主要有两条产品线：</p><ul><li>消费级产品 GeForce系列：GeForce 2080 Ti…</li><li>高性能计算产品 Telsa系列：Telsa V100、Telsa P100、Telsa P40…</li></ul><p><img alt="NVIDIA GPU产品体系" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_nvidia-gpu.png"></p><h3 id="GPU-硬件模型"><a href="#GPU-硬件模型" class="headerlink" title="GPU 硬件模型"></a>GPU 硬件模型</h3><h4 id="Host-and-Device"><a href="#Host-and-Device" class="headerlink" title="Host and Device"></a>Host and Device</h4><p>GPU并不是一个独立运行的计算平台，而是需要与CPU的协同工作，可以看作是CPU的协处理器，因此当我们说GPU并行计算的时候，实质上是指的 <code>CPU+GPU</code> 的异构计算架构。由于CPU和GPU是分开的，在NVIDIA的设计理念里，CPU和主存被称为 <strong>Host</strong>，GPU和显存被称为 <strong>Device</strong>。Host 和 Device 概念会贯穿整个NVIDIA GPU编程。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_cpu-and-gpu.png"></p><p>基于 CPU + GPU 的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。CUDA 程序中既包含 <strong>Host</strong> 程序，又包含 <strong>Device</strong> 程序，它们分别在CPU和GPU上运行。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_cuda-application.jpg"></p><p>同时， <strong>Host</strong> 与 <strong>Device</strong> 之间通过PCIe总线交互进行数据拷贝，典型的 CUDA 程序的执行流程如下：</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_cuda-flow.jpg"></p><ol><li>初始化后，将数据从 Main Memory 拷贝到 GPU Memory</li><li>CPU 调用 CUDA 的核函数</li><li>GPU 的 CUDA Core 并行执行核函数</li><li>将 <strong>Device</strong> 上的运算结果拷贝到 <strong>Host</strong> 上</li></ol><p>GPU核心在做计算时，只能直接从显存中读写数据，程序员需要在代码中指明哪些数据需要从内存和显存之间相互拷贝。这些数据传输都是在总线上，因此总线的传输速度和带宽成了部分计算任务的瓶颈。当前最新的总线技术是NVLink，IBM的 Power CPU 和 NVIDIA 的高端显卡可以通过NVLink直接通信，Intel 的 CPU目前不支持NVLink，只能使用PCIe技术。同时，单台机器上的多张英伟达显卡也可以使用NVLink相互通信，适合多GPU卡并行计算的场景。</p><p><img alt="NVLink可以连接CPU和GPU" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_nvlink.png"></p><h4 id="Streaming-Multiprocessor"><a href="#Streaming-Multiprocessor" class="headerlink" title="Streaming Multiprocessor"></a>Streaming Multiprocessor</h4><p>在 NVIDIA 的设计里，一张GPU卡有多个Streaming Multiprocessor（<strong>SM</strong>），每个 SM 中有多个计算核心，SM 是运算和调度的基本单元。下图为当前计算力最强的显卡Tesla V100，密密麻麻的绿色小格子就是GPU小核心，多个小核心一起组成了一个SM。</p><p><img alt="Tesla V100 with 84 SM Units" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_nvidia-tesla-v100.png"></p><p>将 SM 放大，单个SM的结构如图所示：</p><p><img alt="Tesla V100 Streaming Multiprocessor(SM)" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_nvidia-tesla-v100-sm.png"></p><p>可以看到一个SM中包含了计算核心和存储部分，SM的核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发地执行数百个线程，并发能力就取决于SM所拥有的资源数。</p><ul><li>针对不同计算的小核心（绿色小格子），包括优化深度学习的TENSOR CORE，32个64位浮点核心（FP64），64个整型核心(INT)，64个32位浮点核心(FP32)</li><li>计算核心直接从寄存器（Register）中读写数据</li><li>调度和分发器（Scheduler和Dispatch Unit）</li><li>L0和L1级缓存</li></ul><p>具体而言，SM中的FP32进行32位浮点加乘运算，INT进行整型加乘运算，SFU（Special Functional Unit）执行一些倒数和三角函数等运算。Tensor Core是 NVIDIA 新的微架构中提出的一种混合精度的计算核心。我们知道，当前深度神经网络中使用到最频繁的矩阵运算是： $ D = A \times B + C $。Tensor Core可以对 $ 4 \times 4  $ 的矩阵做上述运算。其中：</p><ul><li>涉及乘法的 A 和 B 使用FP16的16位浮点运算，精度较低</li><li>涉及加法的 C 和 D 使用FP16或FP32精度</li></ul><p>Tensor Core是在 Volta 架构开始提出的，使用Volta架构的V100在深度学习上的性能远超Pascal架构的P100。</p><p><img alt="Tensor Core是一种为优化深度学习计算核心" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_tensor-core.png"></p><h2 id="CUDA-编程模型"><a href="#CUDA-编程模型" class="headerlink" title="CUDA 编程模型"></a>CUDA 编程模型</h2><p>前面提到，NVIDIA 相对于 AMD 的一个巨大优势是它的 CUDA 软件生态，下图是 NVIDIA GPU 编程的软件栈，从底层的GPU驱动和CUDA 工具包，上面还提供了科学计算所必需的cuBLAS线性代数库，cuFFT快速傅里叶变换库以及cuDNN深度神经网络加速库，当前常见的 TensorFlow 和 PyTorch 深度学习框架底层大多都基于 cuDNN 库。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_gpu-software-stack.png"></p><h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>在进一步学习 CUDA 编程模型之前，我们首先配置好 CUDA 的运行环境，跑通 <code>Hello World</code> 从而对 CUDA 编程有一个直观的认识，这里使用的是腾讯云的 GPU 服务器，机器安装的是 CentOS 7 系统，CUDA 环境配置可以参考 <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html" target="_blank" rel="external nofollow noopener noreferrer">CUDA Installation Guide Linux</a> 。</p><p>根据上图的 NVIDIA GPU 软件栈，有了一个插上了 GPU 的服务器之后，我们首先查看机器上的 GPU，可以看到当前机器上装GPU是 <code>Tesla P40</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lspci | grep -i nvidia</span><br><span class="line">00:08.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1)</span><br></pre></td></tr></table></figure><p>接下来在 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external nofollow noopener noreferrer">这里</a>下载 CUDA Toolkit，这里选择的是 <code>rpm local</code> 的安装方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda-repo-rhel7-11-1-local-11.1.1_455.32.00-1.x86_64.rpm</span><br><span class="line">$ sudo rpm -i cuda-repo-rhel7-11-1-local-11.1.1_455.32.00-1.x86_64.rpm</span><br><span class="line">$ sudo yum clean all</span><br><span class="line">$ sudo yum -y install nvidia-driver-latest-dkms cuda</span><br><span class="line">$ sudo yum -y install cuda-drivers</span><br></pre></td></tr></table></figure><p>执行上面的安装操作之后，我们可以看到在 <code>/usr/lib64/</code> 看到 <code>libcuda.so</code> ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls /usr/lib64 -al | grep cuda</span><br><span class="line">lrwxrwxrwx   1 root root        20 Nov 21 15:05 libcuda.so -&gt; libcuda.so.455.32.00</span><br><span class="line">lrwxrwxrwx   1 root root        20 Nov 21 15:05 libcuda.so.1 -&gt; libcuda.so.455.32.00</span><br><span class="line">-rwxr-xr-x   1 root root  21074296 Oct 15 06:58 libcuda.so.455.32.00</span><br></pre></td></tr></table></figure><p>下面是一些我们会经常用到的 CUDA 工具，你需要通过配置环境变量来使用他们：</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">编译器：nvcc</span> <span class="comment">(C/C</span>++<span class="comment">)</span></span><br><span class="line"><span class="comment">调试器：nvcc</span><span class="literal">-</span><span class="comment">gdb</span></span><br><span class="line"><span class="comment">性能分析：nsight</span><span class="string">,</span> <span class="comment">nvprof</span></span><br><span class="line"><span class="comment">函数库：cublas</span><span class="string">,</span> <span class="comment">nvblas</span><span class="string">,</span> <span class="comment">cusolver</span><span class="string">,</span> <span class="comment">cufftw</span><span class="string">,</span> <span class="comment">cusparse</span><span class="string">,</span> <span class="comment">nvgraph</span></span><br></pre></td></tr></table></figure><p>设置环境变量如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-11.1/bin<span class="variable">$&#123;PATH:+:$&#123;PATH&#125;</span>&#125;</span><br><span class="line">$ nvcc --version</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2020 NVIDIA Corporation</span><br><span class="line">Built on Mon_Oct_12_20:09:46_PDT_2020</span><br><span class="line">Cuda compilation tools, release 11.1, V11.1.105</span><br><span class="line">Build cuda_11.1.TC455_06.29190527_0</span><br></pre></td></tr></table></figure><p>除此之外，对于 64 位系统，需要设置 <code>LD_LIBRARY_PATH</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-11.1/lib64<span class="variable">$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span><br></pre></td></tr></table></figure><p>这个时候可以确认驱动的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/driver/nvidia/version</span><br><span class="line">NVRM version: NVIDIA UNIX x86_64 Kernel Module  455.32.00  Wed Oct 14 22:46:18 UTC 2020</span><br><span class="line">GCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC)</span><br></pre></td></tr></table></figure><p>可以使用<code>nvidia-smi</code>命令查看显卡情况，比如这台机器上几张显卡，CUDA版本，显卡上运行的进程等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-smi</span><br><span class="line">Sat Nov 21 17:09:13 2020</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla P40           Off  | 00000000:00:08.0 Off |                    0 |</span><br><span class="line">| N/A   27C    P0    49W / 250W |      0MiB / 22919MiB |      3%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><p><code>CUDA</code> 自己提供了一系列的代码示例，可以通过下面的方法安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cuda-install-samples-11.1.sh &lt;dir&gt;</span><br></pre></td></tr></table></figure><p>在对应目录下，我们可以看到 <code>CUDA</code> 提供的源代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls NVIDIA_CUDA-11.1_Samples</span><br><span class="line">0_Simple     2_Graphics  4_Finance      6_Advanced       bin     EULA.txt  Makefile</span><br><span class="line">1_Utilities  3_Imaging   5_Simulations  7_CUDALibraries  common  LICENSE</span><br></pre></td></tr></table></figure><p>直接在这个目录下执行 <code>make</code>，可以在 <code>bin</code>目录下得到所有代码的二进制程序，选择其中的 <code>deviceQuery</code> 执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">$ ./deviceQuery</span><br><span class="line">./deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: <span class="string">"Tesla P40"</span></span><br><span class="line">  CUDA Driver Version / Runtime Version          11.1 / 11.1</span><br><span class="line">  CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">  Total amount of global memory:                 22919 MBytes (24032378880 bytes)</span><br><span class="line">  (30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1531 MHz (1.53 GHz)</span><br><span class="line">  Memory Clock rate:                             3615 Mhz</span><br><span class="line">  Memory Bus Width:                              384-bit</span><br><span class="line">  L2 Cache Size:                                 3145728 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total shared memory per multiprocessor:        98304 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time <span class="built_in">limit</span> on kernels:                     No</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement <span class="keyword">for</span> Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Enabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device supports Managed Memory:                Yes</span><br><span class="line">  Device supports Compute Preemption:            Yes</span><br><span class="line">  Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 8</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.1, CUDA Runtime Version = 11.1, NumDevs = 1</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure><p>到现在，<code>CUDA Toolkit</code> 安装完毕，接下来通过编写一个简单的 <code>hello world</code> 来直观感受 CUDA 编程：</p><figure class="highlight c"><figcaption><span>hello.cu</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">"\"Hello, world!\", says the GPU.\n"</span> );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">hello_from_cpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">"\"Hello, world!\", says the CPU.\n"</span> );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// host code entrance</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span> **argv )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    hello_from_cpu();</span><br><span class="line">    hello_from_gpu &lt;&lt;&lt; <span class="number">2</span>, <span class="number">4</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，CUDA 程序基本上和标准 C 语言程序一样，主要的区别在于 <code>__global__</code> 限定词 和 <code>&lt;&lt;&lt;... &gt;&gt;&gt;</code> 符号。其中 <code>__global__</code> 标记用来告诉编译器这段代码会运行在 <strong>Device</strong>  （GPU）上，它会被运行在 <strong>Host</strong> 上的代码调用，也被称作是在 <strong>Device</strong> 上线程中并行执行的核函数（Kernel），是在 <strong>Device</strong> 上线程中并行执行的函数。</p><p>当一个核函数被调用时，需要通过 <code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code> 符号 来设置核函数执行时的配置，在 CUDA 的术语中，这称作 <code>kernel lauch</code>，在后面我们将深入介绍这部分。</p><p><code>hello world</code> 程序写完，我们以 <code>hello.cu</code> 这样的后缀名来保存，接下来使用 <code>nvcc</code> 来编译，整体上用法与 <code>gcc</code> 几乎一样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc hello.cu -o hello</span><br><span class="line">$./hello</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the CPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br><span class="line"><span class="string">"Hello, world!"</span>, says the GPU.</span><br></pre></td></tr></table></figure><p>可以看到，来自 CPU 的 <code>Hello World</code> 执行了一次，来自 GPU 的 <code>Hello World</code> 执行了8次。</p><h3 id="核函数与线程模型"><a href="#核函数与线程模型" class="headerlink" title="核函数与线程模型"></a>核函数与线程模型</h3><p>上文提到，为了实现 GPU 并行加速计算，我们需要在 <strong>Host</strong> 上执行 <code>kernel launch</code>，让 核函数 在 <strong>Device</strong> 上的多个线程并发执行。具体的方式就是在调用核函数的时候通过 <code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code> 来指定核函数要执行的线程数量N，之后GPU上的N个Core会并行执行核函数，并且每个线程会分配一个唯一的线程号threadID，这个ID值可以通过核函数的内置变量<code>threadIdx</code>来获得。</p><p>CUDA将核函数所定义的运算称为<strong>线程（Thread）</strong>，多个线程组成一个<strong>块（Block）</strong>，多个块组成<strong>网格（Grid）</strong>。这样一个Grid可以定义成千上万个线程，也就解决了并行执行上万次操作的问题。 <code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code> 中括号中第一个数字表示整个Grid有多少个Block，括号中第二个数字表示一个Block有多少个Thread。前面 <code>Hello World</code> 用 2 个Block，每个Block中有4个Thread，所以总共执行了8次。</p><p><img alt="Grid of Thread Blocks" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-thread-hierarchy.png"></p><p>实际上，线程（Thread）是一个编程上的软件概念。从硬件来看，Thread运行在一个CUDA核心上，多个Thread组成的Block运行在Streaming Multiprocessor（SM），多个Block组成的Grid运行在一个GPU显卡上。当一个 <code>kernel</code> 被执行时，它的gird中的线程块被分配到SM上，<strong>一个线程块只能在一个SM上被调度</strong>。SM一般可以调度多个线程块，这要看SM本身的能力。那么有可能一个 <code>kernel</code> 的各个线程块被分配多个SM，所以grid只是逻辑层，而SM才是执行的物理层。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-software-hardware-view.png"></p><p><code>grid</code> 和 <code>block</code>都是定义为<code>dim3</code>类型的变量，<code>dim3</code>可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此 <code>grid</code> 和 <code>block</code> 可以灵活地定义为 <code>1-dim</code>，<code>2-dim</code> 以及<code>3-dim</code> 结构，对于上图中结构（主要水平方向为x轴），定义的 <code>grid</code>和 <code>block</code> 如下所示， <code>kernel</code> 在调用时也必须通过<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration" target="_blank" rel="external nofollow noopener noreferrer">执行配置</a><code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定 <code>kernel</code> 所使用的线程数及结构。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">3</span>, <span class="number">2</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">5</span>, <span class="number">3</span>)</span></span>;</span><br><span class="line">kernel_fun&lt;&lt;&lt; grid, block &gt;&gt;&gt;(prams...);</span><br></pre></td></tr></table></figure><p>所以，一个线程需要两个内置的坐标变量<code>（blockIdx，threadIdx）</code>来唯一标识，它们都是<code>dim3</code>类型变量，其中blockIdx指明线程所在grid中的位置，而threaIdx指明线程所在block中的位置，如图中的 <code>Thread (1,1)</code> 满足：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">threadIdx.x &#x3D; 1</span><br><span class="line">threadIdx.y &#x3D; 1</span><br><span class="line">blockIdx.x &#x3D; 1</span><br><span class="line">blockIdx.y &#x3D; 1</span><br></pre></td></tr></table></figure><p>不同的执行配置会影响GPU程序的速度，一般需要多次调试才能找到较好的执行配置，在实际编程中，执行配置<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>应参考下面的方法：</p><ul><li>Block运行在SM上，不同硬件架构（Turing、Volta、Pascal…）的CUDA核心数不同，一般需要根据当前硬件来设置Block的大小<code>block</code>（执行配置中第二个参数）。一个Block中的Thread数最好是32、128、256的倍数。注意，限于当前硬件的设计，Block大小不能超过1024。</li><li>Grid的大小<code>grid</code>（执行配置中第一个参数），即一个Grid中Block的个数可以由总次数<code>N</code>除以<code>block</code>，并向上取整。</li></ul><p>例如，我们想并行启动1000个Thread，可以将blockDim设置为128，<code>1000 ÷ 128 = 7.8</code>，向上取整为8。使用时，执行配置可以写成<code>gpuWork&lt;&lt;&lt;8, 128&gt;&gt;&gt;()</code>，CUDA共启动<code>8 * 128 = 1024</code>个Thread，实际计算时只使用前1000个Thread，多余的24个Thread不进行计算。</p> <div class="note info">            <p>这几个变量比较容易混淆，再次明确一下：<code>block</code>是Block中Thread的个数，一个Block中的<code>threadIdx</code>最大不超过<code>block</code>；<code>grid</code>是Grid中Block的个数，一个Grid中的<code>blockIdx</code>最大不超过<code>grid</code>。</p>          </div><p>这几个变量比较容易混淆，再次明确一下：<code>block</code>是Block中Thread的个数，一个Block中的<code>threadIdx</code>最大不超过<code>block</code>；<code>grid</code>是Grid中Block的个数，一个Grid中的<code>blockIdx</code>最大不超过<code>grid</code>。</p><p> <code>kernel</code> 的这种线程组织结构天然适合vector，matrix等运算，我们将在后面实现向量加法和矩阵乘法。如我们将利用上图2-dim结构实现两个矩阵的加法，每个线程负责处理每个位置的两个元素相加，代码如下所示。线程块大小为(16, 16)，然后将 $ N*N $ 大小的矩阵均分为不同的线程块来执行加法运算。</p><p>SM采用的是<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" target="_blank" rel="external nofollow noopener noreferrer">SIMT</a> (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是 <strong>线程束（wraps)</strong>，线程束包含32个线程，这些线程同时执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。</p><p>当线程块被划分到某个SM上时，它将进一步划分为多个线程束，因为这才是SM的基本执行单元，但是一个SM同时并发的线程束数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束中的线程分配独立的寄存器。所以SM的配置会影响其所支持的线程块和线程束并发数量。由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。<code>(16, 16)</code>的二维Block是一个常用的配置，共256个线程。之前也曾提到过，每个Block的Thread个数最好是128、256或512，这与GPU的硬件架构高度相关。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel定义</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">MatAdd</span><span class="params">(<span class="keyword">float</span> A[N][N], <span class="keyword">float</span> B[N][N], <span class="keyword">float</span> C[N][N])</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">    <span class="keyword">int</span> j = blockIdx.y * blockDim.y + threadIdx.y; </span><br><span class="line">    <span class="keyword">if</span> (i &lt; N &amp;&amp; j &lt; N) </span><br><span class="line">        C[i][j] = A[i][j] + B[i][j]; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Kernel 线程配置</span></span><br><span class="line">    <span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>; </span><br><span class="line">    <span class="function">dim3 <span class="title">numBlocks</span><span class="params">(N / threadsPerBlock.x, N / threadsPerBlock.y)</span></span>;</span><br><span class="line">    <span class="comment">// kernel调用</span></span><br><span class="line">    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C); </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>线程块中的线程数是有限制的，现代GPUs的线程块可支持的线程数可达1024个。有时候，我们要知道一个线程在 <code>blcok</code> 中的全局ID，此时就必须还要知道 <code>block</code> 的组织结构，这是通过线程的内置变量 <code>blockDim</code>来获得。它获取线程块各个维度的大小。</p><ul><li>对于一个 <code>2-dim</code> 的block $ (D_x, D_y) $ ，线程  $ (x, y) $ 的ID值为 $ (x + y * D_x) $ </li><li>对于一个<code>3-dim</code> 的block  $ (D_x, D_y, D_z) $，线程 $(x, y, z)$  的ID值为 $ (x + y <em> D_z + z </em> D_z * D_y) $  </li></ul><p>另外线程还有内置变量 <code>gridDim</code>，用于获得网格块各个维度的大小。</p><h3 id="内存模型与管理"><a href="#内存模型与管理" class="headerlink" title="内存模型与管理"></a>内存模型与管理</h3><p>此外这里简单介绍一下CUDA的内存模型，如下图所示。可以看到，</p><ul><li>每个 <strong>Thread</strong> 有自己的私有本地内存（Local Memory）</li><li>每个 <strong>Block</strong> 有包含共享内存（Shared Memory），可以被线程块中所有线程共享，其生命周期与线程块一致</li><li>所有的 <strong>Thread</strong>  都可以访问全局内存（Global Memory）</li><li>访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）</li><li>L1 Cache，L2 Cache</li></ul><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-memory-sm.png"></div><div class="group-picture-column" style="width: 50%;"><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-memory-model.jpg"></div></div></div></div><p>下面简单介绍一下CUDA编程中内存管理常用的API。首先是在 <strong>Device</strong> 上分配内存的 <code>cudaMalloc</code> 、<code>cudaFree</code> 和 <code>cudaMemcpy</code>函数，分别对应C语言中的 <code>malloc</code>、<code>free</code>和 <code>memcpy</code>函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 在 Device 上申请一定字节大小的显存，其中 `devPtr` 是指向所分配内存的指针</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="keyword">void</span>** devPtr, <span class="keyword">size_t</span> <span class="built_in">size</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 Device 上释放一定大小的现存， `devPtr` 是指向所释放内存的指针</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="keyword">void</span>* devPtr)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 负责 Host 和 Device 之间数据通信，src指向数据源，dst是目标区域，count是复制的字节数，kind控制复制的方向</span></span><br><span class="line"><span class="comment">// 这里的 kind 有四种类型：</span></span><br><span class="line"><span class="comment">// - cudaMemcpyHostToHost</span></span><br><span class="line"><span class="comment">// - cudaMemcpyHostToDevice</span></span><br><span class="line"><span class="comment">// - cudaMemcpyDeviceToHost</span></span><br><span class="line"><span class="comment">// - cudaMemcpyDeviceToDevice</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src, <span class="keyword">size_t</span> count, cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure><h2 id="CUDA-编程实战"><a href="#CUDA-编程实战" class="headerlink" title="CUDA 编程实战"></a>CUDA 编程实战</h2><p>知道了CUDA编程基础，接下来我们以两个向量的加法为例，介绍如何利用CUDA编程来实现GPU加速计算。</p><h3 id="CPU-向量加法：传统计算方法"><a href="#CPU-向量加法：传统计算方法" class="headerlink" title="CPU 向量加法：传统计算方法"></a>CPU 向量加法：传统计算方法</h3><p>我们首先来看利用 CPU 来计算向量加法该如何编程：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize array</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>;</span><br><span class="line">        b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Main function</span></span><br><span class="line">    vector_add(out, a, b, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verification</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        assert(<span class="built_in">fabs</span>(out[i] - a[i] - b[i]) &lt; MAX_ERR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"out[0] = %f\n"</span>, out[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"PASSED\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="GPU-向量加法：一个Block一个Thread"><a href="#GPU-向量加法：一个Block一个Thread" class="headerlink" title="GPU 向量加法：一个Block一个Thread"></a>GPU 向量加法：一个Block一个Thread</h3><p>我们将 CPU 的向量加法转换成 CUDA 程序，使用 GPU 来计算，下面这段代码演示了如何使用 CUDA 编程规范来编写程序。实际上仍然只是使用一个 <code>core</code> 来进行计算，不仅没有提高并行度，反而还增加了数据拷贝的成本，显然相比原来的计算是会更慢的，这里主要作为演示。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i ++)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out;</span><br><span class="line">    <span class="keyword">float</span> *d_a, *d_b, *d_out; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate host memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize host arrays</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>;</span><br><span class="line">        b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate device memory</span></span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transfer data from host to device memory</span></span><br><span class="line">    cudaMemcpy(d_a, a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_b, b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Executing kernel </span></span><br><span class="line">    vector_add&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(d_out, d_a, d_b, N);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Transfer data back to host memory</span></span><br><span class="line">    cudaMemcpy(out, d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verification</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        assert(<span class="built_in">fabs</span>(out[i] - a[i] - b[i]) &lt; MAX_ERR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"PASSED\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate device memory</span></span><br><span class="line">    cudaFree(d_a);</span><br><span class="line">    cudaFree(d_b);</span><br><span class="line">    cudaFree(d_out);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate host memory</span></span><br><span class="line">    <span class="built_in">free</span>(a); </span><br><span class="line">    <span class="built_in">free</span>(b); </span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="GPU-向量加法：一个Block多个Thread"><a href="#GPU-向量加法：一个Block多个Thread" class="headerlink" title="GPU 向量加法：一个Block多个Thread"></a>GPU 向量加法：一个Block多个Thread</h3><p>为了提高并行度，我们设置一个 <code>Block</code> 多个 <code>Thread</code> 同时进行计算，如下图所示总共有256个<code>Thread</code>，每个 Thread 负责处理 Vector 中的一部分。每一次迭代中，256个Thread分别计算 Vector 的这256个数，然后在下一次迭代中每个Thread往后推进256个数，继续计算。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-parallel_thread.png"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> index = threadIdx.x;</span><br><span class="line">    <span class="keyword">int</span> stride = blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = index; i &lt; n; i += stride)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out;</span><br><span class="line">    <span class="keyword">float</span> *d_a, *d_b, *d_out; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate host memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize host arrays</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>;</span><br><span class="line">        b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate device memory </span></span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transfer data from host to device memory</span></span><br><span class="line">    cudaMemcpy(d_a, a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_b, b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Executing kernel </span></span><br><span class="line">    vector_add&lt;&lt;&lt;<span class="number">1</span>,<span class="number">256</span>&gt;&gt;&gt;(d_out, d_a, d_b, N);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Transfer data back to host memory</span></span><br><span class="line">    cudaMemcpy(out, d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verification</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        assert(<span class="built_in">fabs</span>(out[i] - a[i] - b[i]) &lt; MAX_ERR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"PASSED\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate device memory</span></span><br><span class="line">    cudaFree(d_a);</span><br><span class="line">    cudaFree(d_b);</span><br><span class="line">    cudaFree(d_out);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate host memory</span></span><br><span class="line">    <span class="built_in">free</span>(a); </span><br><span class="line">    <span class="built_in">free</span>(b); </span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>相比 CPU 程序，这里的并行度显著提高，GPU 计算的时间也大大减小。</p><h3 id="GPU-向量加法：多个Block多个Thread"><a href="#GPU-向量加法：多个Block多个Thread" class="headerlink" title="GPU 向量加法：多个Block多个Thread"></a>GPU 向量加法：多个Block多个Thread</h3><p>在上一个方案中，我们的256个Thread仍然需要计算多个数字，如果我们将并行度继续扩大，让每个Thread只需要计算Vector中的一个数，那么计算消耗时间将会更短。如下图所示，我们使用多个Block多个Thread，其中每个Block还是256个Thread，但是我们现在的Grid有多个Block，Block数字由Vector的长度除以BlockSize得到。</p><p><img alt data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-21_cuda-parallel_block.png"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Handling arbitrary vector size</span></span><br><span class="line">    <span class="keyword">if</span> (tid &lt; n)&#123;</span><br><span class="line">        out[tid] = a[tid] + b[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out;</span><br><span class="line">    <span class="keyword">float</span> *d_a, *d_b, *d_out; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate host memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize host arrays</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>;</span><br><span class="line">        b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate device memory </span></span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transfer data from host to device memory</span></span><br><span class="line">    cudaMemcpy(d_a, a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_b, b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Executing kernel </span></span><br><span class="line">    <span class="keyword">int</span> block_size = <span class="number">256</span>;</span><br><span class="line">    <span class="keyword">int</span> grid_size = ((N + block_size - <span class="number">1</span>) / block_size);</span><br><span class="line">    vector_add&lt;&lt;&lt;grid_size,block_size&gt;&gt;&gt;(d_out, d_a, d_b, N);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Transfer data back to host memory</span></span><br><span class="line">    cudaMemcpy(out, d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verification</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        assert(<span class="built_in">fabs</span>(out[i] - a[i] - b[i]) &lt; MAX_ERR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"PASSED\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate device memory</span></span><br><span class="line">    cudaFree(d_a);</span><br><span class="line">    cudaFree(d_b);</span><br><span class="line">    cudaFree(d_out);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate host memory</span></span><br><span class="line">    <span class="built_in">free</span>(a); </span><br><span class="line">    <span class="built_in">free</span>(b); </span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="GPU-向量加法：Unified-Memory"><a href="#GPU-向量加法：Unified-Memory" class="headerlink" title="GPU 向量加法：Unified Memory"></a>GPU 向量加法：Unified Memory</h3><p>在上面的实现中，我们需要单独在 <strong>Host</strong> 和 <strong>Device</strong> 上进行内存分配，并且要进行数据拷贝，这是很容易出错的。好在CUDA 6.0引入统一内存（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd" target="_blank" rel="external nofollow noopener noreferrer">Unified Memory</a>）来避免这种麻烦，简单来说就是统一内存使用一个托管内存来共同管理 <strong>Host</strong> 和 <strong>Device</strong> 中的内存，并且自动在 <strong>Host</strong> 和 <strong>Device</strong> 中进行数据传输。CUDA中使用cudaMallocManaged函数分配托管内存：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocManaged</span><span class="params">(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span> <span class="built_in">size</span>, <span class="keyword">unsigned</span> <span class="keyword">int</span> flag=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>利用统一内存，可以将上面的程序简化如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ERR 1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Handling arbitrary vector size</span></span><br><span class="line">    <span class="keyword">if</span> (tid &lt; n)&#123;</span><br><span class="line">        out[tid] = a[tid] + b[tid];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// Allocate managed memory</span></span><br><span class="line">    <span class="keyword">float</span> *x, *y, *z;</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;x, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;y, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;z, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize host arrays</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        x[i] = <span class="number">1.0f</span>;</span><br><span class="line">        y[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Executing kernel </span></span><br><span class="line">    <span class="keyword">int</span> block_size = <span class="number">256</span>;</span><br><span class="line">    <span class="keyword">int</span> grid_size = ((N + block_size - <span class="number">1</span>) / block_size);</span><br><span class="line">    vector_add&lt;&lt;&lt;grid_size,block_size&gt;&gt;&gt;(z, x, y, N);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 同步 Device 保证结果能正确访问</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Verification</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        assert(<span class="built_in">fabs</span>(out[i] - a[i] - b[i]) &lt; MAX_ERR);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"PASSED\n"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Deallocate managed memory</span></span><br><span class="line">    cudaFree(x);</span><br><span class="line">    cudaFree(y);</span><br><span class="line">    cudaFree(z);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>相比之前的代码，使用统一内存更简洁了，值得注意的是 <code>kernel</code> 执行是与 <strong>Host</strong> 异步的，由于托管内存自动进行数据传输，这里要用<code>cudaDeviceSynchronize()</code> 函数保证 <strong>Device</strong> 和 <strong>Host</strong> 同步，这样后面才可以正确访问 <code>kernel</code> 计算的结果。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://download.nvidia.com/developer/cuda/seminar/TDCI_Arch.pdf" target="_blank" rel="external nofollow noopener noreferrer">An Introduction to Modern GPU Architecture</a></li><li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" target="_blank" rel="external nofollow noopener noreferrer">NVIDIA CUDA 编程模型官方文档</a></li><li><a href="https://github.com/huiscliu/Tutorials/tree/master/CUDA编程入门" target="_blank" rel="external nofollow noopener noreferrer">CUDA编程入门</a></li><li><a href="http://www.mat.unimi.it/users/sansotte/cuda/CUDA_by_Example.pdf" target="_blank" rel="external nofollow noopener noreferrer">CUDA By Example</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着近年来深度学习的爆发，原来被用于图形渲染的GPU被大量用于并行加速深度学习的模型训练中，在这个过程中 CUDA 作为 NVIDIA 推出的基于GPU的一个通用并行计算平台和编程模型也得到了广泛的使用。或许你已经十分了解 &lt;a href=&quot;../b893097a/&quot;&gt;现代CPU的体系架构&lt;/a&gt;，但是对于GPU还不甚清晰，GPU的体系架构到底和CPU有何区别，CUDA模型是什么，我们该如何使用 CUDA实现并行计算，本文将为你扫盲祛魅，本文中使用到的所有代码可以在我的 &lt;a href=&quot;https://github.com/SimpCosm/cuda-tutorial&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Github&lt;/a&gt; 中找到。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-20_nvidia-tesla-v100.png" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="GPU" scheme="http://houmin.cc/tags/GPU/"/>
    
      <category term="CUDA" scheme="http://houmin.cc/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>The Social Dilemma</title>
    <link href="http://houmin.cc/posts/b86144be/"/>
    <id>http://houmin.cc/posts/b86144be/</id>
    <published>2020-11-14T10:39:02.000Z</published>
    <updated>2020-11-26T13:10:59.103Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>Back on track，本周继续「朝花夕拾」的定期发布，这里是今年的第二十四期「The Social Dilemma」，标题来自最新 Netflix 推出的一个剧情式纪录片。本期会简单聊聊社交困境，除了纪录片所涵盖的内容，还包括自己从字面上对其的漫无边际的延伸思考。</p>    <div id="aplayer-yqTqmcGf" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="446874778" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><p>这周末出去和高中同学KM一起吃了顿饭，跟进了同学间的动态，最大的感慨就是，班上有几个同学自从高中毕业之后就如同人间蒸发了一半，再也没有消息。北京有雾霾了，没有太出去拍照，自己在家做饭看剧。嗯，我越来越喜欢吃意大利面了，真香。</p><p>继续看数据，首先是Rescue Time：</p><ul><li>工作日的ScreenTime依旧是9个小时左右，其中周三因为公司消防演练降低了很多，周五因为一些事情走的比较早，此处不表</li><li>具体到实际每个应用，可以看到企业微信是软件开发以外占据时间最多的，这个感觉可以具体到某个时间来处理，避免每次进入到状态的时候被打断</li><li>周末的时间就很分散，现在对于周末的一个感觉就是时间太短，本来很多想做的事情都没有做完（比如这次的朝花夕拾）</li></ul><p><img alt="Rescue Time" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-15_rescue-time.png"></p><p>还是希望自己的时间使用能够更专注更有效，不喜欢那种漫无目的的刷，下周继续观察。</p><p>时间方面谷歌日历已经可以做到每天具体的时间分配在什么事项上，但是每天的总结还是欠缺，下周开始补上。接下来是Forest专注时间观察，</p><p><img alt="Forest - Nov 8 ~ Nov 14, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-14_forest.jpg"></p><p>这周加入一个新的观察维度，那就是健身数据，目前我的健身行为比较单一，就暂且以跑步的数字作为衡量。是的，在年初的时候，我给自己定下的目标是700公里，今年实际完成度很低。以年为单位总是会让人松懈，本周开始加入每周的跑步数据观察：</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img alt="Running Records" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-14_year-running.jpg"></div><div class="group-picture-column" style="width: 50%;"><img alt="Running - Nov 8 ~ Nov 14, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-14_running.jpg"></div></div></div></div><p>这里还希望加入的一个观察数据是睡眠时间，前一周看每天的上班时间，经常性的9点才出门，早上不想起来，晚上也睡的很晚。为了改善这个方面，继续立Flag。</p><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><h3 id="反垄断法"><a href="#反垄断法" class="headerlink" title="反垄断法"></a>反垄断法</h3><p>背景：11月10日上午，国家市场监管总局发布《关于平台经济领域的反垄断指南（征求意见稿）》公开征求意见，目的是为预防和制止平台经济领域垄断行为，加强和改进平台经济领域反垄断监管，保护市场公平竞争，维护消费者利益和社会公共利益。指南发布后，阿里、美团、腾讯、京东等企业股价大跌。</p><p>分析：中国互联网野蛮生长了二十年，终于发展到现在足以影响到每一个人的体量。互联网企业作为平台型企业，做大之后开始从各个环节抽佣。另一方面，电商平台的二选一，社交平台的链接封杀，都引起了众多争议。这一次的反垄断指南，从某个方面反映了政府监管部门对于大平台问题的重视。但是，毕竟是平台型企业，反垄断指南能够执行到何种程度仍然值得观望，股价该回来还是会回来的。</p><h3 id="荣耀拆分出售"><a href="#荣耀拆分出售" class="headerlink" title="荣耀拆分出售"></a>荣耀拆分出售</h3><p>背景：荣耀拆分出售从最早的传闻到现越来越像真的了。本次传闻：根据去年荣耀60亿元利润，16倍PE来定，约为1000亿人民币，<strong>收购方包括神州数码、三家国资机构，以及TCL等公司组成的小股东阵营</strong>。拆分的缘由还是美国封杀，若华为分拆或出售荣耀手机，荣耀手机的采购零部件不受美国的华为禁令限制，将有助荣耀手机业务与供货商增长，这对荣耀品牌、供货商以及大陆电子业都是多赢局面。</p><p>分析：拜登当选，美国封杀令是否仍会继续？如果不再继续，华为是否能够凤凰涅槃，继续关注。</p><h3 id="Mac换芯"><a href="#Mac换芯" class="headerlink" title="Mac换芯"></a>Mac换芯</h3><p>背景：苹果发布M1芯片，宣布笔记本等产品线将从Intel芯片切换到基于ARM自研的M1芯片。</p><p>分析：从最早的PowerPC，到现在的M1，苹果软硬件一体的初心依然不变。这当然得益于其开创的软硬件生态，也是因为牙膏厂最年来牙膏越来越难挤了。关于M1具体细节尚未研究，不过毕竟是新体系下的第一款芯片，仍然需要时间打磨，现在的主要卖点应该还是功耗。</p><h3 id="辉瑞疫苗"><a href="#辉瑞疫苗" class="headerlink" title="辉瑞疫苗"></a>辉瑞疫苗</h3><p>背景：美国辉瑞制药和德国BioNTech在美股盘前宣布了其合作新冠疫苗的三期临床实验的首批结果，显示其有效性超过90%。受这一消息影响，欧美股市突然暴涨，道指期货涨逾5%，欧洲三大股指全线拉升，与此同时，随着全球风险偏好回升，美元、黄金等避险资产大幅下挫。</p><p>分析：疫苗才是真正的群体免疫。但是疫苗是否能够应对新冠病毒变异，是否最后真正有效，还需要继续观察，至少这个冬天应该还用不了。</p><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>本期主题是「The Social Dilemma」，主题就是讲述了互联网公司通过获取用户数据，通过社交媒体来影响用户的事情。</p><p>这并不是一个新话题，早在年初的时候我就在 <a href="../3e030bdb/">Carpe Diem</a> 里面简单讨论过这个话题。正如那句话，羊毛出在猪身上，互联网用户享受的免费便捷的互联网产品是由广告厂商们为之付费的。互联网用户并不是大厂们的客户，广告商才是。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Back on track，本周继续「朝花夕拾」的定期发布，这里是今年的第二十四期「The Social Dilemma」，标题来自最新 Netflix 推出的一个剧情式纪录片。本期会简单聊聊社交困境，除了纪录片所涵盖的内容，还包括自己从字面上对其的漫无边际的延伸思考。&lt;/p&gt;

    &lt;div id=&quot;aplayer-yqTqmcGf&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;446874778&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-07_winter-bulrush.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="反垄断" scheme="http://houmin.cc/tags/%E5%8F%8D%E5%9E%84%E6%96%AD/"/>
    
      <category term="surveillance capitalism" scheme="http://houmin.cc/tags/surveillance-capitalism/"/>
    
      <category term="平台" scheme="http://houmin.cc/tags/%E5%B9%B3%E5%8F%B0/"/>
    
  </entry>
  
  <entry>
    <title>生活不止五险一金</title>
    <link href="http://houmin.cc/posts/c39a52c0/"/>
    <id>http://houmin.cc/posts/c39a52c0/</id>
    <published>2020-11-07T04:48:32.000Z</published>
    <updated>2020-11-17T03:55:38.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>又逢周末，上期的 <a href>「生日快乐」</a> 告诉自己要恢复「朝花夕拾」的更新，隔了一周，从今天开始。本期「朝花夕拾」的题目来自于前两天听「贤者时间」的一期播客，本文内容与播客内容基本无关，仅仅作为飘飞思绪的引子。恰逢立冬，白昼渐短，宵寒渐长，开始切身的感受到了节气的变化。凛冬将至，蛰伏开始。</p><blockquote><p>秋风吹尽旧庭柯，黄叶丹枫客里过。 </p><p>一点禅灯半轮月，今宵寒较昨宵多。</p></blockquote>    <div id="aplayer-AOQLEmmb" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="1331892086" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"></div><a id="more"></a><h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><h3 id="秋天"><a href="#秋天" class="headerlink" title="秋天"></a>秋天</h3><p>上个星期又回了一次学校，和实验室的老师同学们打了一下午羽毛球，一起吃了顿饭。午饭间隙简单到未名湖转了一圈，补上了未名湖美丽的秋景。</p><p><img alt="同样的角度，今年年初冬天和夏天都拍过一次，这次补上了秋天的美丽" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-10-31_lake.png"></p><p><img alt="年初的时候，还记得这里有同学在打冰球" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-10-31_lake_1.png"></p><h3 id="立冬"><a href="#立冬" class="headerlink" title="立冬"></a>立冬</h3><p>豆瓣上 <a href="https://www.douban.com/people/zhoujie221/" target="_blank" rel="external nofollow noopener noreferrer">青简</a> 有一个 <a href="https://www.douban.com/photos/album/61629667/" target="_blank" rel="external nofollow noopener noreferrer">二十四节气</a>，记录了一年二十四节气中的各个美丽瞬间，正是这个相册让我有了拍出自己的一套二十四节气的想法。在以前，我基本上每年冬天都会走一趟颐和园，或者自己一个人，或者和ZY、和PT他们走过。今天立冬，在节气上冬天的开始，适逢 Feng 来北京，完成了今年的颐和园之行。想起来，自从初中毕业就很少见面，这次在北京两个人一起胡乱的聊起了身边的种种变化，忽的有一种中年人的感觉，此处略去不表。</p><p>尽管是立冬，北京的这个时候还是美丽的深秋，截取了这一天的美丽瞬间，算是我 「二十四节气」相册的开始：</p><p><img alt="下午时分，金色的阳光打在棕黄的芦苇身上，分外美丽" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-07_winter-bulrush.png"></p><p><img alt="这天的风很大，湖畔的柳带飘飞，对面的长堤已经染成了金色，远处的玉峰塔烟雾迷蒙" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-07_beginning-of-winter.png"></p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>关于数据记录的意义我曾经在 <a href="../3e030bdb/">Carpe Diem</a> 这期专门讨论过这个问题，当时列出来几个工具用于记录自己的生活。因为种种原因（或者是因为你懒，或者是因为你没有时间，或者是因为你把宝贵的时间耗在了别处），数据记录的很多部分已经不再启用。没有做到这一步，也是因为定期总结的缺失。随着工作后生活走入固定节奏，我在这里再次重启（希望没有下一次重启），分析数据并修正自己的行为模式。</p><p>Resue Time记录一周的时间消耗，可以看到在工作日，每天的ScreenTime大概是9到10个小时，基本约等于自己的工作时间，这很合理（每天在班时间早上9点到晚上9点，减去中间休息的3个小时，剩下来的时间基本上都在看屏幕）。具体到每天的时间，除了软件开发使用 iTerm2 和 Goland，另外的时间就是企业微信。</p><p><img alt="Rescue Time" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-08_rescue-time.png"></p><p>总的来说，感觉自己每天工作的时候还是不够专注。日常会被企业微信的各种消息打扰，最近一个月以来积累的技术笔记也渐渐减少，更多的时间在做业务相关的实现，这很不好。一方面可以提高自己工作写码调试的效率，另一方面可以集中时间处理一些琐碎的事情，这是一个可以持续改进的问题。为了改进自己的工作状态，继续用Forest使用番茄工作法记录专注时间。另外一个令人开心的结果是，我现在已经形成了用日历来记录每天时间的习惯，虽然固定的每日总结有时候会鸽掉，至少现在在往好的方向走。</p><p><img alt="Forest - Nov 1 ~ Nov 7, 2020" data-src="https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-08_forest.png"></p><h2 id="世界"><a href="#世界" class="headerlink" title="世界"></a>世界</h2><p>「世界」是我在「朝花夕拾」中新开的栏目，用于关注我周围的世界正在发生的事情，想法来自于 <a href="https://github.com/ruanyf/weekly" target="_blank" rel="external nofollow noopener noreferrer">阮一峰的科技爱好者周刊</a>。与之前简单的通过刷微博来获取新闻信息源并吃瓜关注的效果不同，我希望这里会对过去一周发生的那些大事件进行深度阅读，理出自己的思绪，给出自己的分析与预测。此外，还会积极关注科技和商业领域那些新出现的有意思的东西，保持对新鲜事物的好奇心和敏感度。</p><h3 id="大事件"><a href="#大事件" class="headerlink" title="大事件"></a>大事件</h3><h4 id="美国大选"><a href="#美国大选" class="headerlink" title="美国大选"></a>美国大选</h4><ul><li>背景：大选前期民调显示拜登领先川普，然而在出票过程中经过了几轮反转，在最开始拜登领先的情况下，川普拿下了佛州，其他几个摇摆州的选票也逐渐开始反超，像极了2016年。然而随着邮寄选票的逐步计票，拜登开始抢回宾州、威斯康辛州等的选票，并最终获得超过270张选票。</li><li>现状：美国各大媒体已经宣布拜登胜选，欧洲英法德等国也积极向拜登表示祝贺。然而到现在(2020.11.09)川普仍然没有承认败选，反而鼓吹民主党选举舞弊，黑天鹅仍然存在。</li><li>分析：选举前的想法是，2020年全球疫情的背景下，川普在美国的疫情管理一塌糊涂（当然欧洲也是难兄难弟），种族矛盾爆发（BLM运动如火如荼），如此混乱的情况下，我们对于大选的预期大致是这么混乱满嘴胡话的川普应该是不太可能连任吧。然而，虽然最终却是是拜登胜选，但是选情能够胶着到这种情况着实是没有预料到的。这证明了川普在美国民众是有一大票民众真心支持他的，即使在这种情况下他们也仍然会选择川普。他们是些什么人？在全球化背景下，美国实体产业流失，原有的大批工人失业，从中产阶级坠落，生活状况急剧降低，与之相反，跨国公司的资本家在全球化的浪潮中获得了巨大收益。巨大的反差是特朗普各种操作下仍能够获得众多支持的重要原因，而且这种支持是长久的支持。</li><li>不论最后拜登是否能够顺利就任，美国社会的撕裂是仍将继续存在的。对于中国，应该做的仍然是做好自己的事情。拜登就任，也许将不再采用川普这种粗暴的贸易战做法，而是像之前一直以来的做法一样（典型的例子是操作系统、芯片等构建自己的生态，凭借高额的利润，以恰到好处的产品价格让你没有动力去研发自己系统下的市场产品），这才是最难受的，这也是我们大部分企业会放弃研发无法进阶高端产业的原因。</li><li>未来四年（2020-2024），全球化仍将继续，我们的实力会继续增长。前面还有很多困难，我们内部也还有很多问题，我们要继续努力，稳扎稳打，未来终将属于中国。</li></ul><h4 id="蚂蚁风波"><a href="#蚂蚁风波" class="headerlink" title="蚂蚁风波"></a>蚂蚁风波</h4><ul><li>背景<ul><li>10月24日，马云在上海外滩金融峰会抨击中国金融没有系统，监管太严、巴塞尔协议是老年俱乐部引发争议</li><li>11月2日，中国人民银行、中国银保监会、中国证监会、国家外汇管理局对蚂蚁集团实际控制人马云、董事长井贤栋、总裁胡晓明进行了监管约谈</li><li>11月2日，中国银保监会和中国人民银行对关于《网络小额贷款业务管理暂行办法（征求意见稿）》公开征求意见</li><li>11月3日，上海证券交易所致函蚂蚁科技集团股份有限公司《关于暂缓蚂蚁科技集团股份有限公司科创板上市的决定》</li><li>11月5日，蚂蚁集团原定于上海和香港同步挂牌上市，暂缓上市</li><li>11月6日，蚂蚁集团启动退款程序，投资人认购股份将注销</li></ul></li><li>分析<ul><li>什么是巴塞尔协议？巴塞尔协议是银行业为了维持资本市场稳定、降低银行系统信用风险和市场风险提出的资本充足率的要求。</li><li>我国银行给出的标准是10%左右，也就是说银行顶多只能有10倍杠杆，银行有1块钱，顶多只能借出去10块。</li><li>蚂蚁金服在重庆的两家小额贷款公司花呗和借呗，注册资金30亿，以1：2杠杆从银行借来60亿，凑足90亿资本金。</li><li>之后利用这90资本金开始放贷，获得大批的贷款合同，然后将这些贷款合同打包成资产卖给别人收回本金。这里的打包成资产，金融里称作ABS(Assets Backed Securities，资产抵押债券)。对没错，这就是在2008年次贷危机中发生巨大作用的金融产品。收回本金后，蚂蚁继续放贷，反复循环四十多次，形成3000亿多的贷款规模。本金只有30亿，杠杆率超过100。</li><li>按照蚂蚁集团的上市财报，其放贷规模目前已经达到了1.8万亿，但本金仅360亿，本金率约2%。换句话说，杠杆率高达50倍。自己只有1块钱，蚂蚁集团可以借出去50块钱，其他的49块钱银行出。这49块钱，银行收取5~6%的融资利息，但花呗和借呗放出去的利率，高达14~18%，中间的息差达到8~9%，全部归蚂蚁集团所有。要注意，你的利润是49块钱的8~9%，但你的本金只有1块钱。</li><li>11月2日中国银保监会和中国人民银行提出的网络小贷监管意见主要包括：<ul><li>放出去的贷款中，小贷公司自己的出资比例不得低于30%。</li><li>小贷公司通过银行借款、股东借款等非标融资形式，融入资金不得超过其净资产的1倍。</li><li>通过发行债券，资产证券化（ABS）等形式融资的金额，不得超过净资产的4倍。</li></ul></li><li>对比蚂蚁集团的上市招股书，可以看到网络小贷监管对于蚂蚁集团义务影响巨大。按照监管要求，蚂蚁的杠杆率将由50压缩到16倍以内，这对其市盈率将是巨大的打击。这也是为什么蚂蚁一直号称自己是科技公司，而不是金融公司。金融公司由于监管政策的限制，银行业的市盈率一般在10左右。蚂蚁原定上市计划中市值2万亿，大约3000亿美金，动态市盈率是47倍，显然大幅超过了一般的金融公司。</li><li>自己的观点，支持银行监管，毕竟有次贷危机的前车之鉴。但是蚂蚁作为新型公司，在大数据和互联网科技上肯定是有积淀的，肯定也可以在监管下发挥自己的作用，毕竟他们做的还是金融。这玩意儿不能松，不然出问题就是大问题。</li></ul></li></ul><h3 id="新东西"><a href="#新东西" class="headerlink" title="新东西"></a>新东西</h3><p>本期「朝花夕拾」对新东西、新概念的积累较浅，这次仅仅提一提最近在各个地方看到的DeFi（DecentralizedFinance），DeFi是区块链领域最近很火的一个概念，具体相关的内容以后要写一期来专门扫盲。</p><p>之前一直说要保持对于新科技、新创造的追踪，要做到这个，首先需要有一个各个行业的Overview扫盲贴，后续才能够持续的追踪。其实按照二八原则成本并不算太大，需要自己一点一点的积累。</p><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><p>本期的主题是「生活不止五险一金」，最开始写这个题目是想借这个机会好好了解下五险一金与我具体相关的事情。是的，这些规则细碎而繁琐，但是你作为活在当前中国社会不得不接触的东西，从这里你可以了解到影响上亿人生活的规则。这些东西，确定了中国社会的最最基本的形态。</p><h2 id="附录：五险一金"><a href="#附录：五险一金" class="headerlink" title="附录：五险一金"></a>附录：五险一金</h2><h3 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h3><p>五险一金，就是指<strong>“养老保险、医疗保险、生育保险、工伤保险、失业保险和住房公积金”</strong></p><h4 id="覆盖人群"><a href="#覆盖人群" class="headerlink" title="覆盖人群"></a>覆盖人群</h4><ul><li>五险一金是有工作的人交的，因此上班族、自由职业者都可以参与。自由职业者可以自己选择缴费基数，但公司纳的一部分也要承担。</li><li>没有工作的人，比如家庭主妇，儿童及学生只能参加养老保险和医疗保险。<ul><li>其中，没有进城务工的农民参加的是<strong>新农保</strong>（新型农村养老保险）、<strong>城乡医保</strong>（城乡居民医疗保险）。</li><li>其他城镇居民则参加<strong>城居保</strong>（城镇居民养老保险）、<strong>城乡医保，</strong>大学生参加就读当地的城乡医保。</li></ul></li></ul><h4 id="缴费规则"><a href="#缴费规则" class="headerlink" title="缴费规则"></a>缴费规则</h4><p>五险一金分为公司缴费和个人缴费，公司缴费进入统筹账户，个人缴费进入个人账户（社保卡/医保卡）。</p><p>五险一金交多少取决于两个因素：<strong>缴费基数和缴费比率</strong></p><p><strong>缴费基数</strong>是你<strong>上个年度月均工资</strong>，但不能超过当地平均工资的<strong>三倍</strong>，也不会低于当地平均工资的<strong>60%</strong>。</p><p><strong>缴费比率</strong>全国各统筹地规定不同，但一般如下：</p><p><img alt="五险一金缴费比例" data-src="https://pic1.zhimg.com/80/v2-9774cb8f17777a3702d4827a13669a00_720w.jpg"></p><p>五险一金费用交下来，<strong>个人缴费</strong>占到你工资的<strong>11%</strong>左右，<strong>公司缴费</strong>占到你工资的<strong>25%</strong>左右。</p><h4 id="好处概述"><a href="#好处概述" class="headerlink" title="好处概述"></a>好处概述</h4><p><strong>总的来说：五险一金让我们能在大城市安家落户</strong></p><p><strong>分开来说：五险一金能在我们养老和生病，生孩子买房子这些事情上带给我们保障。</strong></p><h3 id="分述"><a href="#分述" class="headerlink" title="分述"></a>分述</h3><h4 id="医疗保险"><a href="#医疗保险" class="headerlink" title="医疗保险"></a>医疗保险</h4><h5 id="报销规则"><a href="#报销规则" class="headerlink" title="报销规则"></a>报销规则</h5><p>医保报销限定在“<strong>两定点，三目录</strong>”内——<strong>定点医院</strong>、<strong>定点药费</strong>和国家规定的医保可以报销的<strong>药品目录、诊疗项目目录、服务设施目录。</strong></p><p>在“两定点、三目录”内，还设置了<strong>起付线和封顶线</strong>，在两线之间，不同等级的医院，不同的人群，报销比率不一样：</p><p>医保报销分为门急诊报销和住院报销，</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-a4540ce83307f8482722dd9cee78d189_720w.jpg"></p><p>门急诊报销少，住院报销多；医院等级越低报销越多，反之报销越少；在职人员报销少，退休人员报销多。</p><p>至于具体的报销比率，各统筹地有自己的规定，</p><p>拿上海举例：</p><p>上海在职人员走医保门急诊，刷完医保卡当年计入账户部分后，自掏1500元便可开始报销，几万块的门诊费用医保能报销50%-65%。</p><p><img alt="上海在职人员门急诊医保报销" data-src="https://pic4.zhimg.com/80/v2-cb9646eb2f3219736b98746343cd0b77_720w.jpg"></p><p>如果是住院报销，先用医保卡刷掉1500元以后，在53万元以下的住院费用都可以报销85%，超过53万元的附加基金再报销80%。</p><p><img alt data-src="https://pic4.zhimg.com/80/v2-299c5c19d8528b004b44350e69ffc7bf_720w.jpg"></p><p><strong>我举个门急诊报销的例子：</strong></p><p><img alt data-src="https://pic1.zhimg.com/80/v2-a75aeefd82d4a41b29dce0e8c9313b24_720w.jpg"></p><p><strong>5万块的支付顺序是：</strong></p><p><img alt data-src="https://pic1.zhimg.com/80/v2-d77fad0b4a3c2ad66aabf62aaa24a660_720w.jpg"></p><p><strong>于是这次生病，</strong></p><p><strong>5万元的支付结构为：</strong></p><p><img alt data-src="https://pic2.zhimg.com/80/v2-9188ee739dad6985ff39bd1dd830ca49_720w.jpg"></p><p>生病花了5万，自己只掏了13250元，算下来医保报销了73.5%！</p><p>为患者减轻了很大的负担。</p><p>但现实中一些人的医保卡额度积攒得并不多，再加上报销上限只有两万块，</p><p>一旦遭遇大点的疾病或手术，一下子花个十几二十万，</p><p>还有很多能极大提高治愈率的技术不在报销范围内，比如癌症的质子重离子技术。</p><p>医保的作用就只能算是铺底了，真正能扛住大病风险的还是百万医疗险和重疾险。</p><h5 id="享受标准"><a href="#享受标准" class="headerlink" title="享受标准"></a>享受标准</h5><p>一般来说，医保今天交，<strong>次月</strong>就可以用<strong>统筹账户</strong>报销，<strong>半年</strong>或<strong>一年后</strong>可以用<strong>个人账户</strong>刷卡报销。</p><p>在退休前<strong>男性交满25-30年</strong>，<strong>女性退休前交满20-25年</strong>，退休后可以免费享受。</p><p>如果退休时缴费年限不够，可以一次性补缴剩余费用，然后才可以免费享受。</p><p>此外，也有不能享受的标准，主要为以下四方面：</p><ul><li><strong>应该由工伤保险基金支付的，比如尘肺病；</strong></li><li><strong>境外就医的；</strong></li><li><strong>应该由第三人（单位和个人）负担的，比如车祸；</strong></li><li><strong>应该由公共卫生负担的，比如新冠肺炎。</strong></li></ul><h5 id="异地就医"><a href="#异地就医" class="headerlink" title="异地就医"></a>异地就医</h5><p>医保不是全国通，是各省各地统筹，各统筹地的政策不一样，如果跨统筹地就医，就涉及到异地就医医保如何报销的问题。</p><p>在进行异地医保报销前，可以到先到<strong><a href="https://link.zhihu.com/?target=http%3A//si.12333.gov.cn/120692.jhtml" rel="external nofollow noopener noreferrer" target="_blank">国家社会保险公共服务平台</a></strong>查询<strong>支持异地医保直接结算</strong>的医保定点医院，再去就诊。</p><p><img alt data-src="https://pic4.zhimg.com/80/v2-00db9805efaa55c20380076736eeb9db_720w.jpg"></p><p>异地就医，分三种情况：</p><ul><li><strong>长期异地就诊</strong>——如在上海参保，但却在北京长期居住，居住期间生病就医；</li><li><strong>临时异地转院</strong>——如在上海参保，但上海治不了转到北京去医治；</li><li><strong>临时异地就诊</strong>——如在上海参保，但去北京旅游，出差，见亲人等生病就医。</li></ul><p><strong>三种情况处理方式不同</strong>，具体参见下图：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-1e14254c44d7d3d18b3ea92a8c98fec4_720w.jpg"></p><p><strong>异地医保报销需要特别注意的三点：</strong></p><ol><li><p>医保卡异地<strong>报销只限住院，门急诊部分城市才有。</strong></p></li><li><p>医保卡异地报销能<strong>报销的范围取决于就诊地医保政策</strong>，但能<strong>报销多少钱取决于你的参保地政策</strong>；</p></li><li><p>如果你办理了异地就医备案，回到原参保地之后，医保报销资格可能被取消，也可能还能用，也有可能取消备案之后才能用，这需要你咨询参保地医保局。</p></li></ol><p>所以，有什么问题一定不要忘记拨打<strong>12333</strong>的电话问清楚，因为各地的规定都不相同。</p><p>关于更详细的医保报销操作细节，也可以参考我这篇按照就医看病过程写的医保报销指南：</p><h4 id="养老保险"><a href="#养老保险" class="headerlink" title="养老保险"></a>养老保险</h4><p>养老保险简单理解就像我们每个月往银行存一笔钱一样，退休之后每个月往外取钱用。不同的地方是这钱存到了养老保险基金那里，由国家指派的专家组进行运作保值增值。</p><h5 id="养老金计算"><a href="#养老金计算" class="headerlink" title="养老金计算"></a>养老金计算</h5><p>养老金能拿到多少跟个人<strong>累计缴费年限、缴费工资、当地职工平均工资、个人账户金额、城镇人口平均预期寿命</strong>等因素有关，但一定是<strong>“长缴多得，多缴多得”。</strong></p><p><strong>公司交的钱进入基础账户，个人交的钱进入个人账户</strong>，退休后，两个账户都可以拿钱。</p><p>个人账户简单，就是每个月你存起来的钱，按照8%左右的年利增长（当年缴入按单利，历年累计按复利）。最后假设你60岁退休就将累积的这笔钱分成139个月发给你。</p><p>基础账户能拿到的钱由一个复杂的公式确定，计算比较复杂，，但可以肯定的是，你退休前的工资越高，当地工资越高，交的年限越久，你能从统筹账户拿到的退休工资就越多。</p><p>举个例子：王华，23岁，上海工作，2020年工资8600，从今年交社保交37年后60岁在上海退休，未来他的工资假设如下：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-ed6f8aba8f8ebc1f05c948c5affc3184_720w.jpg"></p><h5 id="领取资格"><a href="#领取资格" class="headerlink" title="领取资格"></a>领取资格</h5><p>领取养老金要满足三个条件：</p><ul><li><strong>至少缴满15年；</strong></li><li><strong>第二必须到退休年龄；</strong></li><li><strong>办理完退休证明。</strong></li></ul><p>退休时还没有交满15年的<strong>必须补满15年才能领，</strong></p><p>但按照上面的公式，如果我们只交15年养老金就不交的话，<strong>最后只能拿到社会平均工资的15%</strong>，这是非常少的。</p><p>如果是工伤导致退休，这笔钱由企业补满，自然退休则是自己补满。</p><p>退休年龄为国家规定的退休年龄，如下表：</p><p>其他关于养老金断缴，领取地以及是否能拿回来的问题，参见后面<strong>番外篇详述部分</strong>。</p><h4 id="生育保险"><a href="#生育保险" class="headerlink" title="生育保险"></a>生育保险</h4><p>生育保险也是五险一金中非常重要的一个，待遇丰厚，与其他四险一金绑定在一起缴纳。</p><p>而且是企业为我们缴纳，职工不用缴纳。</p><p><strong>第一：生育险的待遇有哪些？</strong></p><p>生育保险的待遇有三项：<strong>产假、生育津贴、生育医疗费用报销。</strong></p><p>参保的女职工不仅能修超长产假，在休假期间还有生育津贴（产假工资），生孩子的医疗费也可以报销一部分。</p><p><strong>产假</strong>：分为<strong>基本产假、产前检查、产前工间休息、授乳时间。</strong></p><p>基本产假如下表：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-feeb1f582ed895f246d2a695fc6eb46c_1440w.jpg"></p><p>其他与产假相关的假期各地规定不一，具体参考当地女职工劳动保护办法。</p><p>以上海为例：</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-eef4e6ae24e32a05c4ac4c3f6ae33e85_1440w.jpg"></p><p>另外，女职工还可以因为生育而请的假如下：</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-2c8b95fd6beaee3cd821f9e63e71c9d5_1440w.jpg"></p><p><strong>生育津贴：</strong></p><p>生育津贴又叫产假工资，由生育保险基金支付，可以自己去社保网点领取，也可以拨给公司，由公司代付。</p><p>发放生育津贴的时间主要是<strong>产假期间</strong>和计划<strong>生育手术休假期间</strong>，计算如下：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-e82255a792f032755bc110e87e6c62fc_1440w.jpg"></p><p>举个例子：</p><p>花花生孩子时休息了98天，后来去上环又休息了15天，她单位上年度月均工资是6500，则她可以领到（6500/30）*（98+15）=24483元。</p><p><strong>特别注意：</strong>如果公司的缴费基数高于职工的缴费基数，<strong>不足的部分由公司补足。</strong></p><p><strong>生育医疗费用报销：</strong></p><p>生育医疗费用报销的项目有：因怀孕、生育而发生的<strong>检查费、接生费、手术费、住院费、药费、计划生育医疗费用</strong>。</p><p>至于报销限额，<strong>各地规定不一样</strong>，以上海市为例：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-fb1732dd90d143fb9d544e1367adc9ac_1440w.jpg"></p><p>上海市采取固定额度报销的方法（有的地方是按比率报销）。</p><p>只要符合生育金缴纳标准，正常生育报销3600，</p><p>自然流产的，大于4个月的报销600，小于4个月的报销400，超出的地方自费。</p><p><strong>第二：生育险待遇领取的标准</strong></p><p>各地规定不一，以上海市为例：</p><p>要么是某公司为职工<strong>连续缴纳至少9个月，</strong>或者不同公司已经为职工<strong>累计缴纳至少12个月。</strong></p><p><strong>生育期间千万别断交，不然报不了。</strong></p><p>如果是<strong>未婚先孕</strong>，则<strong>不能享受</strong>生育险待遇，因为不符合计划生育政策，</p><p>只能先跟另一位把证领了才能报销。</p><h4 id="工伤保险"><a href="#工伤保险" class="headerlink" title="工伤保险"></a>工伤保险</h4><p>工伤保险是用来在职工因为工作相关的事情受伤或者罹患职业病之后给职工提供医疗和补偿的，用以保障职工往后的生活，待遇非常丰厚，由企业为职工缴纳，职工不用掏一分钱。</p><p><strong>第一：工伤保险有哪些待遇？</strong></p><p>工伤保险赔付丰厚，既报销，又给钱。</p><p>具体包括：</p><p><strong>医疗费、住院伙食补助费、交通费、食宿费、康复治疗费、辅助器具费</strong>，这些都<strong>报销</strong>；<strong>停工留薪、护理费、伤残待遇、工亡待遇</strong>，这些都<strong>给钱</strong>。</p><p>其中最重要的是<strong>医疗费、停工留薪、伤残待遇和工亡待遇。</strong></p><ul><li><strong>医疗费：</strong>遭遇工伤或者职业病时治疗所必须的医疗费<strong>全部报销</strong>；</li><li><strong>停工留薪：</strong>遭遇工伤或者职业病时不能工作期间的工资和福利由工伤基金按照正常上班的标准给付；</li><li><strong>伤残待遇：</strong>遭遇工伤或者职业病造成不同程度的残疾，工伤基金按照不同伤残等级(1-10级)给付伤残待遇；</li><li><strong>工亡待遇：</strong>遭遇工伤或者职业病之后死亡了，工伤基金会赔给你的家人<strong>一笔钱</strong>，还有<strong>丧葬补贴</strong>等。</li></ul><p>具体规定见下表：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-9d7e1f36a65e927e228e07916bb2c766_1440w.jpg"></p><h5 id="工伤认定的流程"><a href="#工伤认定的流程" class="headerlink" title="工伤认定的流程"></a>工伤认定的流程</h5><p>当我们不幸在工作中出现事故，导致伤残时，我们就成了弱势群体，这个时候必须得保持<strong>头脑清醒</strong>，<strong>主动维权。</strong></p><p>大致描述一下情形，当我们因为工作出事故后，会经历的过程如下：</p><p><strong>紧急送医→企业为员工申请认定工伤→进行工伤认定→伤残者进行伤残等级鉴定（劳动能力鉴定）→死亡者发死亡补助</strong></p><p><img alt="工伤认定环节流程图" data-src="https://pic2.zhimg.com/80/v2-6947d490b41330a7751732694e65c8a5_1440w.jpg"></p><p>我们需要注意到的是：<strong>工伤认定属于行政行为，不申请，不认定</strong>。</p><p>如果企业不给我们认定，我们<strong>一定要自己或者叫近亲属或者找工会帮忙认定</strong>。</p><p>并且，<strong>超过一年，不予认定。</strong></p><h5 id="可以认定为工伤的情形"><a href="#可以认定为工伤的情形" class="headerlink" title="可以认定为工伤的情形"></a>可以认定为工伤的情形</h5><p>两种情况，一个是<strong>得病</strong>，一个就是<strong>受伤</strong>。</p><p><strong>得病，指的是得了法定的职业病­</strong>——即劳动者在职业活动中因<strong>接触粉尘</strong>、<strong>放射性物质</strong>、和<strong>其他有毒有害物质</strong>因素而引起的疾病，包括十大类，具体如下表：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-45946ea860e8995ba7fce2ee62737d2a_1440w.jpg"></p><p>可以看到，职业病主要时跟有毒有害的物质有关，而<strong>“996”加班</strong>，<strong>积劳成疾的码农等，每日空调、WiFi、零食相伴，就算累成狗累出心脏病也不算职业病。</strong></p><p><strong>受伤，就是受到身体伤害或者是暴力伤害</strong>，具体可以认定为工伤的情形如下：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-8b40ee81808920a4695902e01655a72a_1440w.jpg"></p><p>特别需要注意的是：</p><p><strong>上班期间救火等负伤也算工伤，还有当兵回来旧伤复发的也算工伤。</strong></p><p><strong>还有，上班期间收到暴力伤害，也算工伤，比如执行本职工作时被人打击报复受伤，可以认定为工伤。</strong></p><p><strong>第三：如何进行工伤等级鉴定（劳动能力鉴定）？</strong></p><p>工伤认定和工伤等级鉴定是有区别的，</p><p>小伤小病进行工伤认定之后，如果医治一段时间还没有好，并且影响工作和生活自理，就需要申请工伤等级鉴定。</p><p>但工伤等级鉴定需要满足一定的<strong>医疗期规定</strong>，具体如下表：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-eb4f259cac7b29a4a51c351e515a3664_1440w.jpg"></p><h5 id="工伤等级鉴定"><a href="#工伤等级鉴定" class="headerlink" title="工伤等级鉴定"></a>工伤等级鉴定</h5><p>工伤等级鉴定的大致流程如下图所示：</p><p><img alt data-src="https://pic4.zhimg.com/80/v2-68648fbc1e68dad56cf4a69a373f71d7_1440w.jpg"></p><h5 id="工伤等级鉴定需要准备的材料"><a href="#工伤等级鉴定需要准备的材料" class="headerlink" title="工伤等级鉴定需要准备的材料"></a>工伤等级鉴定需要准备的材料</h5><p>工伤等级鉴定需要准备的材料如下表：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-d7ccd19609b4633c0f1953ff90573a7e_1440w.jpg"></p><p>特别需要注意的是，<strong>进行工伤等级鉴定前，需要先进行工伤认定</strong>，只有工伤认定书出来后才能进行工伤等级鉴定。</p><p><strong>第四：工伤鉴定中“上下班途中”是如何规定的？</strong></p><p>《工伤保险条例》中规定，<strong>上下班途中遇到自己非主责的交通意外可以认定工伤。</strong></p><p><strong>但什么才叫上下班途中？</strong>几点到几点？还是从哪儿到哪儿？</p><p>最高院在《最高人民法院关于审理工伤保险 行政案件若干问题的规定》中，对“上下班途中”进行了明确的定义：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-164ec28aeadc16a5dc14ad4c2419422e_1440w.jpg"></p><p><strong>第五：工伤保险待遇的领取标准</strong></p><p>企业给你交，只要交上，当月就可以报销，</p><p>但也不能断，断了的话可以走仲裁，仲裁不过走司法程序。</p><p>记住这句话就行了。</p><h4 id="失业保险"><a href="#失业保险" class="headerlink" title="失业保险"></a>失业保险</h4><p>失业保险，顾名思义，就是在我们失业的时候给我们补贴和培训，帮助我们尽快上岗的保险。</p><p>其中，失业是指被辞退，一定企业方解除合同，并不是我们主动辞职。</p><p><strong>第一：失业保险都有哪些待遇？</strong></p><p>参保失业保险的人，失业了可以拿到的补助有<strong>失业保险金</strong>、<strong>医疗补助金</strong>、<strong>死亡后的丧葬补助金</strong>和<strong>抚恤金</strong>，</p><p>还有一定的<strong>职业培训和工作介绍</strong>。</p><p>一般而言，失业保险金<strong>只能拿到当地最低工资的一部分，缴纳时间越长，可领时间也越长。</strong></p><p>但<strong>只能领取12-24个月</strong>。具体能领多少个月，由当地<strong>省级政府规定</strong>。</p><p>以上海市为例：</p><p><strong>缴纳期限</strong></p><p><img alt data-src="https://pic4.zhimg.com/80/v2-178ecad9000541bd3cdc3de96c409fbb_1440w.jpg"></p><p><strong>领取标准</strong></p><p><img alt data-src="https://pic1.zhimg.com/80/v2-214e336f7e4e42779d79d873fd3823d8_1440w.jpg"></p><p>在上海，<strong>非本地户口也不能领取</strong>，</p><p>如果失业了，<strong>只能把失业保险账户转到户籍地去按照户籍地的标准领</strong>。</p><p><strong>第二：失业保险待遇的领取标准</strong></p><p>至少缴满<strong>一年</strong>才能领取。</p><p>如果连续两次失业，第二次领取失业金时，之前的缴费年限清零计算，所以两次失业必须间隔一年以上才能领取。</p><p>但你之前如果没领满12个月，可以累加到下一次，两次累加不能超过24个月。</p><p>小时候，我妈常对我说“<strong>晴带雨伞，饱带饥粮</strong>”，做人一定要有危机感，不能躺在功劳簿上睡大觉，止步不前。</p><p>活得太过安逸，为人所耻，在国家给的失业保险金上面就能看得出来。</p><h4 id="住房公积金"><a href="#住房公积金" class="headerlink" title="住房公积金"></a>住房公积金</h4><p>住房公积金就是国家强制你和企业每个月拿出一点钱，存到一个账户里，然后把所有人的账户里的钱汇集到一起形成一个基金，以后你买房子就可以从这个基金里面贷款，拿到的利率全国最低，没有之一。</p><p>从我们贷款买房者的角度去讲讲最需要关注的这几个点：</p><h5 id="住房公积金的利率有多低？"><a href="#住房公积金的利率有多低？" class="headerlink" title="住房公积金的利率有多低？"></a>住房公积金的利率有多低？</h5><p>公积金存贷利率都由国家统一规定，各地一样，普遍比商业贷款利率低。目前是5年以上按3.25%年利算，5年（含5年）以下按2.75%年利算，贷一年的话不分期，一年以后连本带利一起还。同期商贷1年是4.35%，2-5年是4.75%，5年以上4.9%。这个利率是非常低了，工商银行5年期定期存款利率也才正好是2.75%。和公积金贷款5年以下的一样，5年以上的和它相比，利差只有0.5%，某些专给小微企业贷款的融资租赁平台，实际贷款利率高达20%多，连我们常见的支付宝借呗年利都是18.25%。</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-c7a8dd0ea94d3d54de0d39a5c76ae66a_1440w.jpg"></p><h5 id="住房公积金能贷多久？"><a href="#住房公积金能贷多久？" class="headerlink" title="住房公积金能贷多久？"></a>住房公积金能贷多久？</h5><p>最长不能超过30年，以夫妻双方年龄大的为准，男性贷款年龄加上房贷时间不能超过65岁，女性不能超过60岁。</p><p>与楼龄也有关系，砖混结构的楼龄加上贷款年限不能超过47，钢混结构的楼龄加上贷款年限不能超过57。</p><h5 id="住房公积金能贷多少？"><a href="#住房公积金能贷多少？" class="headerlink" title="住房公积金能贷多少？"></a>住房公积金能贷多少？</h5><p>额度计算比较复杂，有四种计算额度的方法，取<strong>四种方法里面最低的结果</strong>作为我们<strong>可以拿到的贷款额度</strong>。</p><p>举个例子：</p><p>王华，工资8000，单位和个人按照 7% 的比例缴纳住房公积金，交了10年，目前无负债。</p><p>现在看中一套90平的房子，付完3成首付还要再贷款100万才能买下来，他打算贷款30年。</p><p><strong>问：王华用公积金贷款能贷多少钱？</strong></p><p><img alt data-src="https://pic1.zhimg.com/80/v2-b7fef69873e6253baf002429c68bdfa4_1440w.jpg"></p><p>计算公式为：</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-92b63b4e2464967ca85c001944b378d5_1440w.jpg"></p><p>还款能力系数是扣掉当地最低消费水平后收入剩余部分占总收入的比重。</p><p>按公式计算王华的可贷额度为：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-b55880b574fcd47787f1be31985f7e38_1440w.jpg"></p><p>如果夫妻一起贷，并且妻子跟王华财务状况一样，那么贷款额度可以翻倍——<strong>男女结婚后共同努力可以住更大的房子！</strong></p><p><img alt data-src="https://pic2.zhimg.com/80/v2-710fd739fab35405189f1486d35ffa75_1440w.jpg"></p><p>计算公式为：</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-c892f6debd2e54853a1d1eaec33c6b44_1440w.jpg"></p><p>上海的政策来说，90平以下的首套房，首付20%，可贷比率为80%，</p><p>90平以上的首付30%，可贷比例为70%。</p><p>前面王华已经付了三成首付，想要贷款的100万已经是70%了。</p><p>所以额度就只有100万。</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-43a638d5140c2bff14e966cd25a71baa_1440w.jpg"></p><p>各地对住房公积金购房贷款额度有限制，上海的政策如下：</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-eccba2f8b3089fce7235cb8cd27919c6_1440w.jpg"></p><p>那么，如果王华<strong>一个人买</strong>，<strong>最高只能</strong>用公积金贷款<strong>50万</strong>，<strong>夫妻一起买，最高可以用公积金贷款100万。</strong></p><p><img alt data-src="https://pic2.zhimg.com/80/v2-20635855ac97ae932d55c79b9ecfd3e9_1440w.jpg"></p><p>此方法是指公积金贷款额度最高不能超过个人公积金账户存储额的n倍，这里的n，各地规定不一，上海目前是15倍。</p><p>每年进入王华公积金账户的钱，当年的按照0.35%年利按月复利计息，往年留存下来的本息和按照1.35%的年利按一年期定存计息。</p><p>最后计算出来王华10年后公积金账户余额为14.3万，余额的15倍为214.7万。</p><p><img alt data-src="https://pic1.zhimg.com/80/v2-f57569d54fe65706ad77e34a21ee71d4_1440w.jpg"></p><p>四种方法比较下来，<strong>最高贷款限额算法 &lt; 还款能力算法 &lt; 可贷比例算法 &lt; 账户存储额倍数算法</strong></p><p>那么，如果此次买房，王华一个人用公积金贷款的话，只能贷款50万，夫妻可贷100万。</p><h5 id="住房公积金该不该取出来？"><a href="#住房公积金该不该取出来？" class="headerlink" title="住房公积金该不该取出来？"></a>住房公积金该不该取出来？</h5><p>大家都知道，公积金可以取出来，但如果取出太多，可能账户余额的15倍会低于上海贷款最高限额。所以，如果王华要取出来公积金，也得保证余额的15倍至少高于当地最高限额50万。剩下的，能取就取出来吧。</p><h5 id="住房公积金和商贷比例控制在多少比较合适？"><a href="#住房公积金和商贷比例控制在多少比较合适？" class="headerlink" title="住房公积金和商贷比例控制在多少比较合适？"></a>住房公积金和商贷比例控制在多少比较合适？</h5><p>当公积金贷款不能满足我们时，我们一般都选择商贷和公积金组合贷。但是比例控制在多少比较合适呢？</p><p>一张图说明白：</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-6229d29b923d774a5226168aa5d698b5_1440w.jpg"></p><p>可以看出，商贷用得越多，总还款额越多，月供也越多，所以能全用公积金贷款的额度就用公积金贷款的额度，不够再用商贷。</p><h5 id="住房公积金贷多少年比较合适？"><a href="#住房公积金贷多少年比较合适？" class="headerlink" title="住房公积金贷多少年比较合适？"></a>住房公积金贷多少年比较合适？</h5><p>公积金最长贷30年，也可以不贷那么多年。</p><p>具体贷多少年，由个人的资金状况和还款计划决定，从纯金融的角度来说，存在一个合理的答案如下：</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-29649d064765def05bb1bcaa1eb00e91_1440w.jpg"></p><p>可以看出，前期月供压力很大，但是到15年的时候已经下降到7000左右，夫妻二人承担压力就小很多了，时间再长一点月供下降不多，但是还款总额就要增加几十万了。</p><p>所以，贷款时间控制在15年最合适。</p><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><h4 id="养老金能否拿回来"><a href="#养老金能否拿回来" class="headerlink" title="养老金能否拿回来"></a>养老金能否拿回来</h4><p>我们国家养老金实行的是“现收现付”制，现在的年轻人交钱给现在的老年人用，但老龄化越来越严重，年轻人养老负担越来越大。</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-8f21164249323eb72cd36699141174c1_1440w.jpg"></p><p>个人账户养老金的利率也从原来的2/3%一下子提高到现在的8%左右，前面王华的养老金也计算到了6万多，远比他退休前的工资要高很多。于是有的人就担心，老年人那么多，我交这么多年，<strong>轮到我的时候还能领得了养老金吗？</strong></p><p><strong>结论是：肯定能领到，只是没有我们计算的那么多。</strong></p><p>因为国家采用了几大措施用来保障大家退休之后能够拿到因该得的养老金：</p><p><strong>第一个措施：控制养老金替代率</strong></p><p><img alt data-src="https://pic3.zhimg.com/80/v2-c273b879a65514a34c9703be126e811a_1440w.jpg"></p><p>养老金替代率就是我们领到的养老金占退休时当地平均工资的比例，越高越能保障我们的晚年生活。</p><p>根据劳动和社会保障部《关于完善企业职工基本养老保险制度宣传提纲》的总体思路，未来基本养老金目标替代率为59.2%。</p><p>所以，我们不会领到6万多的养老金，也不会领到过低的养老金，而是在当地工资的6成左右。</p><p><strong>第二个措施：养老金记账利率盯住工资增长率</strong></p><p>很多人都抱怨养老金个人账户收益率太低。确实，原来的养老金的利率普遍在2/3%左右，但是工资水平却每年以10%左右的水平增长，如此增长下去，几十年以后养老金相比于工资就会少得可怜。于是国家将养老金记账利率调高到8%，远远超过很多市场上的理财产品收益。</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-5a758f9218a5c177153c85ee6372c405_1440w.jpg"></p><p>用来盯住工资增长率，换句话说，也就是养老金增长跟随通胀走，这样才能保证以后我们拿到的养老金能够扛住通胀，不会贬值幅度过大。</p><p><strong>第三个措施：多方筹款，保证有钱给大家养老</strong></p><p>工资水平年年在涨，养老金水平也每个月都在增加。</p><p><img alt data-src="https://pic3.zhimg.com/80/v2-3bf8abd1624ecfe7fbf175c7e3bebe46_1440w.jpg"></p><p>再加上公务员养老和职工养老并轨，全部参保人员的养老金计息一下子提高了很多，这样养老金基金的负债压力越来越大，已经产生了巨大的缺口。于是国家<strong>采取多方筹款</strong>的方式用来解决现在老人的养老金支付问题</p><ul><li><strong>第一是成立全国社保基金用来在老龄化高峰之时弥补和调剂养老金统筹基金的缺口。</strong>全国社保基金的资金来源于<strong>中央财政预算拨款，国有资产划转，个人账户基金并入、地方政府委托基金</strong>。2018年末，社保基金资产总额22,353.78亿元，并且社保基金自成立以来的年均投资收益率7.82％，累计投资收益额9,552.16亿元。</li><li>第二就是<strong>全国企业缴纳的基本养老金统筹基金，企业缴纳本人工资的16%用来支付养老金。</strong>2018年末，基本养老保险基金资产总额7,032.82亿元。自2016年12月受托运营以来，累计投资收益额186.83亿元，实现的收益率在4%左右。</li></ul><p>所以，国家通过各种方式筹集资金，为我们的养老大事做足准备，虽然压力很大，年年都有窟窿，但只要国家在的一天，我们就有养老的一天。只是，养老金的水平要照顾到目标替代率和工资上涨速度，可能未来会增长趋缓，最坏的情况也就是下降到目标替代率。</p><p><strong>因此，养老金肯定能拿回来，只是不像我们计算的那样多。</strong></p><h4 id="高工资低公积金现象"><a href="#高工资低公积金现象" class="headerlink" title="高工资低公积金现象"></a>高工资低公积金现象</h4><p>很多人都会发现，为什么自己每个月到手一两万，但是住房公积金怎么只有几百块？相比别人，为什么他的工资才五六千，公积金每个月都有一两千？实际上，不仅是公积金，整个社保都一样，我们缴费多少，取决于两个因素：<strong>缴费基数和缴费比率，</strong></p><p>本质上来说，缴费比率国家已经定死了，可以自由选择的那一两个百分点在基数相同的情况下差别不大，真正导致缴费差别大的是缴费基数——<strong>有的公司会偷偷给你把缴费基数降低！</strong></p><p>常见的手段主要有两种：</p><ul><li><strong>公司用好几张银行卡给你发工资</strong></li><li><strong>公司给你的工资底薪很低，绩效很高。</strong></li></ul><p>如何操作呢？</p><p>比如王华的工资是10000，但是公司要求他用四张银行卡来发工资，于是王华提交了他自己、姐姐、妹妹、老婆的银行卡。然后公司将10000块分成四份转到四张卡里，只有王华的2500走的是工资账户，其他三张则是网转。这样王华的账上工资一下子变成了2500，那自然同样的缴费比率，10000和2500相比，差了好几级。</p><p>或者是公司将王华的10000工资分成5000的工资和5000的绩效，每次就只按照5000的工资来缴纳社保，5000绩效则通过网络转账的方式。</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-d61469a45920762049d00ba7b45902a1_1440w.jpg"></p><p>但实际上，我们前面说了，社保缴纳基数也包括绩效工资，奖金，等等，所以，公司这样做是违法的，如果你想告，一告一个准。虽然，到手的工资高了，但是几十年下来，这么一大个福利你就拿得少了，公司少给你缴纳的社保也是多发给你工资的几倍，最终吃亏的是你自己。</p><h4 id="北漂一族养老金领取问题"><a href="#北漂一族养老金领取问题" class="headerlink" title="北漂一族养老金领取问题"></a>北漂一族养老金领取问题</h4><p>对于北上广深的“漂一族”来说，去往大城市就业，机会多，资源多，福利好。但总有一个问题避免不开，那就是在哪里退休的问题——养老保险对与退休金的领取地有特别的规定：国家规定：养老金的领取，遵循<strong>“户籍地优先，从长从后”</strong>的原则。</p><p>用王华举例子：</p><ul><li>王华<strong>户籍在湖南</strong>老家，在<strong>湖南交</strong>的养老保险，最后也在<strong>湖南退休</strong>，退休后养老保险肯定在湖南老家<strong>按照湖南</strong>老家的标准领取。</li><li>王华<strong>户籍在湖南</strong>，但<strong>在上海缴了</strong>至少<strong>10年</strong>的社保，最后在<strong>上海退休</strong>，退休后养老金<strong>按照上海标准</strong>发放。</li><li>王华<strong>户籍在湖南</strong>，<strong>在上海缴满了十年</strong>的养老保险，然后他<strong>转到北京</strong>工作，<strong>缴纳五年</strong>社保后退休（退休地不满十年），那王华的养老保险<strong>关系</strong>将会被<strong>转回上海</strong>，然后<strong>按照上海的标准</strong>领取养老保险金。</li><li>王华<strong>户籍在湖南</strong>，在<strong>上海</strong>干了<strong>5年</strong>，在<strong>北京</strong>干了<strong>7年</strong>，在<strong>深圳</strong>干了<strong>6年</strong>，然后<strong>退休</strong>（退休前每个地方<strong>都没有缴满十年</strong>），那王华的养老保险账户将被<strong>转到湖南老家</strong>，<strong>按照湖南的标准</strong>领取养老金。</li><li>王华<strong>户籍在湖南</strong>，全国各地转来转去，最后<strong>退休了</strong>，但是养老保险<strong>还没缴满15年</strong>，可以按照规定确定一个补缴地，<strong>在补缴地缴满并领取</strong>养老金。</li></ul><p><img alt data-src="https://pic3.zhimg.com/80/v2-3e251ef64a26b699fd48945517796f7e_1440w.jpg"></p><p>从上面的政策规定，我们最好的选择是在上海缴纳满10年，或者在上海退休又或者有上海户籍，才能拿到上海的养老金。</p><p>但假设王华从23岁毕业来上海，缴满10年的话，到33岁了，这个时候应该是结婚的年龄了，（有数据显示，大城市结婚年龄普遍在31岁左右）。假设王华10年内工资平均下来每月1.3万。</p><p>那么，在上海郊区买一套普普通通的商住楼，需要不吃不喝<strong>89个月（7.5年），</strong>如果节省得好，每个月剩下来4000攒首付，需要<strong>290个月（24年），况且这24年中房价得涨到多高？</strong></p><p><img alt="上海青浦区——乡下某套房价格" data-src="https://pic1.zhimg.com/80/v2-e526070fd97e70b55ba5bc071234a44c_1440w.jpg"></p><p><strong>年龄已经快要被剩下了，但首付还没攒到一半。</strong></p><p>面对买不起的房子和娶不起的新娘，我相信很多人都会撑不到10年就离开了。那我们就再也没希望拿到上海的养老金了。如果你不是实力非凡，其实大部分人在北上广呆个几年就各回各家了。这点从上海常住总人口的增长就可以看出来。</p><p><img alt data-src="https://pic4.zhimg.com/80/v2-fcf77c552e7199dc5e5edd9ce8e711cf_1440w.jpg"></p><p>这几年，上海总人口不仅没有增长，还大有负增长的趋势。可悲的是“<strong>融不进的城市，买不起的房，拿不了的户口，娶不起的新娘</strong>”是当代绝大部分“漂一族”的真实状况；但更可悲的是，年轻人在大城市工作的那么些年，缴纳的五险一金有80%的人群公司都没有给他们足额缴纳；但更更可悲的是，就算企业为你正常缴纳社保，你也不一定能用上。<strong>企业给你交得再多，也不过是养那些有当地户口，在当地买房的人罢了。</strong></p><h4 id="离职后社保的处理"><a href="#离职后社保的处理" class="headerlink" title="离职后社保的处理"></a>离职后社保的处理</h4><p><strong>第一：社保断缴后的影响？</strong></p><p>不管什么原因，我相信大家离职一定有大家的道理，离职后社保就会断缴，但社保一旦断缴，损失惨重。</p><p><strong>1. 买房落户、买车拍牌等会受影响</strong>。</p><p>以上海为例：</p><p>想拿到上海户籍，在社保缴纳方面每正常缴纳满一年积累3分，中间断缴后再补缴的部分不能计入积分。</p><p>想要在上海买房，社保方面需要<strong>连续</strong>缴纳满5年，中间断缴，或再补缴都不行。</p><p>想要拿到沪牌竞拍资格，需要本市户籍或者有居住证的前提下还要已经<strong>连续</strong>缴满3年社会保险，断缴或者补缴都不行。</p><p><strong>2.医保待遇会受到影响。</strong></p><p>以上海为例：</p><p>断缴之后，有3个月缓冲期，缓冲期内不享受医保。如果断缴超过3个月，缴费年限直接清零重算，</p><p>而上海医保必须至少要缴满15年，退休之后才能享受免费的医保待遇。所以一旦断缴超过3个月，之前的年限就浪费了。</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-4cff339ab8c6f34c560164446610d515_1440w.jpg"></p><p><strong>3.生孩子报销也会受到影响。</strong></p><p>根据规定，生育保险只有单位<strong>连续</strong>为你缴纳9个月或者累计缴纳12个月才可以享受生育保险。</p><p>就算累计缴纳12个月了，一旦不缴纳，次月一日起就不能享受生育保险了。</p><p><img alt data-src="https://pic2.zhimg.com/80/v2-4ebdb2d83363ee0cc583434cf0798979_1440w.jpg"></p><p><strong>4.对养老金的影响。</strong></p><p>国家规定至少缴满15年，退休之后才能领取养老金。</p><p>如果中间有断缴的，断缴时间不能计入缴费年限，只有员工足额补齐所欠保费和利息之后，才开始继续计算缴费年限。并且，存在中断养老金缴费情况的人，退休后领取养老金数额要比没有断缴的人一个月少几十块。</p><p><strong>第二：社保断缴后怎么处理？</strong></p><p>社保断缴有好几种原因：</p><p><strong>第一个，公司故意不给你缴。</strong></p><p>这个简单，留好证据，举报就行。如果我们<strong>不知道具体的怎么搜集证据、怎么在进行维权等事宜</strong>，我们可以<strong>打电话询问的机构</strong>是：<strong>人力资源和社会保障局咨询服务中心——</strong>承担劳动保障监察、医疗保险、人事人才等电话举报的受理工作。</p><p>我们可以进行举报投诉的机构是：<strong>劳动保障监察总队——</strong>受理举报投诉和平日里监督各用人单位是否遵守劳动保障法律法规的。</p><p><strong>第二个：离职后断缴。</strong></p><p>第一个方法就是尽快找到下家，在15号上家公司交完社保后再离职，在下一个15号前找到下家入职就行。</p><p>第二个方法也可以跟上家或者开公司的朋友谈一谈，把自己的社保挂在别人的公司上缴纳，但成功率由个人决定。</p><p>第三个方法就是花点钱（平均50-60一个月）找一家靠谱的社保代缴机构帮你缴纳。</p><p>如果已经断了，要赶快打电话（12333）问社保机构该怎么补缴，失业险影响不大，但养老和医保影响巨大，一定要赶快行动。</p><p><strong>第三：换工作城市社保怎么转移？</strong></p><p>如果在同一个统筹区内换工作，比如王华在上海换到上海，唯一需要做的就是把社保账号和公积金账号记住，交给下一加的人事部门就可以了，你可以登录当地社保局和住房公积金管理中心查到你的账号。</p><p>如果从上海换到北京，那就跨了统筹区，这个时候，个人账户里的全部储蓄额都将转走，而统筹账户则按照你缴纳工资总额的12%进行转移。但跨区转移有年龄限制：</p><ul><li>如果王华从上海转回湖南老家，多大年龄都可以转，</li><li>如果王华从上海转到北京，50岁以下（女性40岁以下）可以转；50岁以上则只能办理临时账户缴纳社保，社保关系还保留在上海。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;又逢周末，上期的 &lt;a href&gt;「生日快乐」&lt;/a&gt; 告诉自己要恢复「朝花夕拾」的更新，隔了一周，从今天开始。本期「朝花夕拾」的题目来自于前两天听「贤者时间」的一期播客，本文内容与播客内容基本无关，仅仅作为飘飞思绪的引子。恰逢立冬，白昼渐短，宵寒渐长，开始切身的感受到了节气的变化。凛冬将至，蛰伏开始。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;秋风吹尽旧庭柯，黄叶丹枫客里过。 &lt;/p&gt;
&lt;p&gt;一点禅灯半轮月，今宵寒较昨宵多。&lt;/p&gt;
&lt;/blockquote&gt;

    &lt;div id=&quot;aplayer-AOQLEmmb&quot; class=&quot;aplayer aplayer-tag-marker meting-tag-marker&quot; data-id=&quot;1331892086&quot; data-server=&quot;netease&quot; data-type=&quot;song&quot; data-mode=&quot;circulation&quot; data-autoplay=&quot;false&quot; data-mutex=&quot;true&quot; data-listmaxheight=&quot;340px&quot; data-preload=&quot;auto&quot; data-theme=&quot;#555&quot;&gt;&lt;/div&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2020-11-01_sunset.png" type="image" />
    
    
      <category term="朝花夕拾" scheme="http://houmin.cc/categories/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE/"/>
    
    
      <category term="摄影" scheme="http://houmin.cc/tags/%E6%91%84%E5%BD%B1/"/>
    
      <category term="生活" scheme="http://houmin.cc/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="保险" scheme="http://houmin.cc/tags/%E4%BF%9D%E9%99%A9/"/>
    
      <category term="立冬" scheme="http://houmin.cc/tags/%E7%AB%8B%E5%86%AC/"/>
    
      <category term="五险一金" scheme="http://houmin.cc/tags/%E4%BA%94%E9%99%A9%E4%B8%80%E9%87%91/"/>
    
      <category term="二十四节气" scheme="http://houmin.cc/tags/%E4%BA%8C%E5%8D%81%E5%9B%9B%E8%8A%82%E6%B0%94/"/>
    
  </entry>
  
  <entry>
    <title>【Kubernetes】Admission Webhook</title>
    <link href="http://houmin.cc/posts/a4ebbce/"/>
    <id>http://houmin.cc/posts/a4ebbce/</id>
    <published>2020-11-04T10:57:01.000Z</published>
    <updated>2020-11-30T11:11:13.804Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2019-10-18_city.jpg" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="k8s" scheme="http://houmin.cc/tags/k8s/"/>
    
      <category term="webhook" scheme="http://houmin.cc/tags/webhook/"/>
    
  </entry>
  
  <entry>
    <title>【Kubernetes】Volume</title>
    <link href="http://houmin.cc/posts/8d2f7dac/"/>
    <id>http://houmin.cc/posts/8d2f7dac/</id>
    <published>2020-10-26T09:41:42.000Z</published>
    <updated>2020-11-30T11:12:01.471Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>在 Kubernetes 集群中，虽然无状态的服务非常常见，但是在实际的生产中仍然会需要在集群中部署一些有状态的节点，比如一些存储中间件、消息队列等等。</p><a id="more"></a><p>然而 Kubernetes 中的每一个容器随时都可能因为某些原因而被删除和重启，容器中的文件也会随着它的删除而丢失，所以我们需要对集群中的某些文件和数据进行『持久化』；除此之外，由于同一个 Pod 中的多个 Container 可能也会有共享文件的需求，比如通过共享文件目录的方式为 nginx 生成需要代理的静态文件，所以我们需要一种方式来解决这两个问题。</p><p>作为 Kubernetes 集群中除了 Pod 和 Service 之外最常见的基本对象，Volume 不仅能够解决 Container 中文件的临时性问题，也能够让同一个 Pod 中的多个 Container 共享文件。</p><blockquote><p>这篇文章并不会介绍 Kubernetes 中 Volume 的使用方法和 API，而是会着重介绍 Volume 的工作原理，包含其创建过程、多种 Volume 实现的异同以及如何与云服务提供商进行适配。</p></blockquote><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes 中的 Volume 种类非常多，它不仅要支持临时的、易失的磁盘文件，还需要解决持久存储的问题；第一个问题往往都比较容易解决，后者作为持久存储在很多时候都需要与云服务商提供的存储方案打交道，如果是 Kubernetes 中已经支持的存储类型倒是还好，遇到不支持的类型还是比较麻烦的。</p><p><img alt="kubernetes-storage" data-src="https://img.draveness.me/2019-01-14-kubernetes-storage.png"></p><p>除了卷和持久卷之外，Kubernetes 还有另外一种更加复杂的概念 - 动态存储供应，它能够允许存储卷按需进行创建，不再需要集群的管理员手动调用云服务商提供的接口或者界面创建新的存储卷。</p><p>集群中的每一个卷在被 Pod 使用时都会经历四个操作，也就是附着（Attach）、挂载（Mount）、卸载（Unmount）和分离（Detach）。</p><p>如果 Pod 中使用的是 EmptyDir、HostPath 这种类型的卷，那么这些卷并不会经历附着和分离的操作，它们只会被挂载和卸载到某一个的 Pod 中，不过如果使用的云服务商提供的存储服务，这些持久卷只有附着到某一个节点之后才可以被挂在到相应的目录下，不过在其他节点使用这些卷时，该存储资源也需要先与当前的节点分离。</p><h3 id="卷"><a href="#卷" class="headerlink" title="卷"></a>卷</h3><p>在这一节中提到的卷（Volume）其实是一个比较特定的概念，它并不是一个持久化存储，可能会随着 Pod 的删除而删除，常见的卷就包括 EmptyDir、HostPath、ConfigMap 和 Secret，这些卷与所属的 Pod 具有相同的生命周期，它们可以通过如下的方式挂载到 Pod 下面的某一个目录中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/busybox</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/cache</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/hostpath</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data/configmap</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">special-volume</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data/secret</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hostpath-volume</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/data/hostpath</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">    <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">special-config</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">secret-config</span></span><br></pre></td></tr></table></figure><p>需要注意的是，当我们将 ConfigMap 或者 Secret 『包装』成卷并挂载到某个目录时，我们其实创建了一些新的 Volume，这些 Volume 并不是 Kubernetes 中的对象，它们只存在于当前 Pod 中，随着 Pod 的删除而删除，但是需要注意的是这些『临时卷』的删除并不会导致相关 <code>ConfigMap</code> 或者 <code>Secret</code> 对象的删除。</p><p>从上面我们其实可以看出 Volume 没有办法脱离 Pod 而生存，它与 Pod 拥有完全相同的生命周期，而且它们也不是 Kubernetes 对象，所以 Volume 的主要作用还是用于跨节点或者容器对数据进行同步和共享。</p><h3 id="持久卷"><a href="#持久卷" class="headerlink" title="持久卷"></a>持久卷</h3><p>临时的卷没有办法解决数据持久存储的问题，想要让数据能够持久化，首先就需要将 Pod 和卷的声明周期分离，这也就是引入持久卷 <code>PersistentVolume(PV)</code> 的原因。我们可以将 <code>PersistentVolume</code> 理解为集群中资源的一种，它与集群中的节点 Node 有些相似，PV 为 Kubernete 集群提供了一个如何提供并且使用存储的抽象，与它一起被引入的另一个对象就是 <code>PersistentVolumeClaim(PVC)</code>，这两个对象之间的关系与节点和 Pod 之间的关系差不多：</p><p><img alt="kubernetes-pv-and-pvc" data-src="https://img.draveness.me/2019-01-14-kubernetes-pv-and-pvc.png"></p><p><code>PersistentVolume</code> 是集群中的一种被管理员分配的存储资源，而 <code>PersistentVolumeClaim</code> 表示用户对存储资源的申请，它与 Pod 非常相似，PVC 消耗了持久卷资源，而 Pod 消耗了节点上的 CPU 和内存等物理资源。</p><p>因为 PVC 允许用户消耗抽象的存储资源，所以用户需要不同类型、属性和性能的 PV 就是一个比较常见的需求了，在这时我们可以通过 <code>StorageClass</code> 来提供不同种类的 PV 资源，上层用户就可以直接使用系统管理员提供好的存储类型。</p><h4 id="访问模式"><a href="#访问模式" class="headerlink" title="访问模式"></a>访问模式</h4><p>Kubernetes 中的 PV 提供三种不同的访问模式，分别是 <code>ReadWriteOnce</code>、<code>ReadOnlyMany</code> 和 <code>ReadWriteMany</code>，这三种模式的含义和用法我们可以通过它们的名字推测出来：</p><ul><li><code>ReadWriteOnce</code> 表示当前卷可以被一个节点使用读写模式挂载；</li><li><code>ReadOnlyMany</code> 表示当前卷可以被多个节点使用只读模式挂载；</li><li><code>ReadWriteMany</code> 表示当前卷可以被多个节点使用读写模式挂载；</li></ul><p>不同的卷插件对于访问模式其实有着不同的支持，AWS 上的 <code>AWSElasticBlockStore</code> 和 GCP 上的 <code>GCEPersistentDisk</code> 就只支持 <code>ReadWriteOnce</code> 方式的挂载，不能同时挂载到多个节点上，但是 <code>CephFS</code> 就同时支持这三种访问模式。</p><h4 id="回收策略"><a href="#回收策略" class="headerlink" title="回收策略"></a>回收策略</h4><p>当某个服务使用完某一个卷之后，它们会从 apiserver 中删除 PVC 对象，这时 Kubernetes 就需要对卷进行回收（Reclaim），持久卷也同样包含三种不同的回收策略，这三种回收策略会指导 Kubernetes 选择不同的方式对使用过的卷进行处理。</p><p><img alt="kubernetes-pv-reclaiming-strategy" data-src="https://img.draveness.me/2019-01-14-kubernetes-pv-reclaiming-strategy.png"></p><p>第一种回收策略就是保留（Retain）PV 中的数据，如果希望 PV 能够被重新使用，系统管理员需要删除被使用的 <code>PersistentVolume</code> 对象并手动清除存储和相关存储上的数据。</p><p>另一种常见的回收策略就是删除（Delete），当 PVC 被使用者删除之后，如果当前卷支持删除的回收策略，那么 PV 和相关的存储会被自动删除，如果当前 PV 上的数据确实不再需要，那么将回收策略设置成 Delete 能够节省手动处理的时间并快速释放无用的资源。</p><h4 id="存储供应"><a href="#存储供应" class="headerlink" title="存储供应"></a>存储供应</h4><p>Kubernetes 集群中包含了很多的 PV 资源，而 PV 资源有两种供应的方式，一种是静态的，另一种是动态的，静态存储供应要求集群的管理员预先创建一定数量的 PV，然后使用者通过 PVC 的方式对 PV 资源的使用进行声明和申请；但是当系统管理员创建的 PV 对象不能满足使用者的需求时，就会进入动态存储供应的逻辑，供应的方式是基于集群中的 <code>StorageClass</code> 对象，当然这种动态供应的方式也可以通过配置进行关闭。</p><h2 id="管理"><a href="#管理" class="headerlink" title="管理"></a>管理</h2><p>Volume 的创建和管理在 Kubernetes 中主要由卷管理器 <code>VolumeManager</code> 和 <code>AttachDetachController</code> 和 <code>PVController</code> 三个组件负责。其中卷管理器会负责卷的创建和管理的大部分工作，而 <code>AttachDetachController</code> 主要负责对集群中的卷进行 Attach 和 Detach，<code>PVController</code> 负责处理持久卷的变更，文章接下来的内容会详细介绍这几部分之间的关系、工作原理以及它们是如何协作的。</p><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>作者在 <a href="https://draveness.me/kubernetes-pod" target="_blank" rel="external nofollow noopener noreferrer">详解 Kubernetes Pod 的实现原理</a> 一文中曾简单介绍过 kubelet 和 Pod 的关系，前者会负责后者的创建和管理，kubelet 中与 Pod 相关的信息都是从 apiserver 中获取的：</p><pre class="mermaid">graph LR    apiserver-.->u    u((updates))-.->kubelet    kubelet-.->podWorkers    podWorkers-.->worker1    podWorkers-.->worker2    style u fill:#fffede,stroke:#ebebb7</pre><p>两者的通信会使用一个 <code>kubetypes.PodUpdate</code> 类型的 Channel，kubelet 从 apiserver 中获取 Pod 时也会通过字段过滤器 <code>fields.OneTermEqualSelector(api.PodHostField, string(nodeName))</code> 仅选择被调度到 kubelet 所在节点上的 Pod：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSourceApiserver</span><span class="params">(c clientset.Interface, nodeName types.NodeName, updates <span class="keyword">chan</span>&lt;- <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">lw := cache.NewListWatchFromClient(c.CoreV1().RESTClient(), <span class="string">"pods"</span>, metav1.NamespaceAll, fields.OneTermEqualSelector(api.PodHostField, <span class="keyword">string</span>(nodeName)))</span><br><span class="line">newSourceApiserverFromLW(lw, updates)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有对 Pod 的变更最终都会通知给具体的 PodWorker，这些 Worker 协程会调用 kubelet <code>syncPod</code> 函数完成对 Pod 的同步：</p><pre class="mermaid">sequenceDiagram    participant PW as PodWorker    participant K as Kubelet    participant VL as VolumeManager    participant DSOWP as DesiredStateOfWorldPopulator    participant ASOW as ActualStateOfWorld    PW->>+K: syncPod    K->>+VL: WaitForAttachAndMount    VL-xDSOWP: ReprocessPod    loop verifyVolumesMounted        VL->>+ASOW: getUnmountedVolumes        ASOW-->>-VL: Volumes    end    VL-->>-K: Attached/Timeout    K-->>-PW: return</pre><p>在一个 100 多行的 <code>syncPod</code> 方法中，kubelet 会调用 <code>WaitForAttachAndMount</code> 方法，等待某一个 Pod 中的全部卷已经被成功地挂载：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">syncPod</span><span class="params">(o syncPodOptions)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">pod := o.pod</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">if</span> !kl.podIsTerminated(pod) &#123;</span><br><span class="line">kl.volumeManager.WaitForAttachAndMount(pod)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法会将当前的 Pod 加入需要重新处理卷挂载的队列并在循环中持续调用 <code>verifyVolumesMounted</code> 方法来比较期望挂载的卷和实际挂载卷的区别，这个循环会等待两者变得完全相同或者超时后才会返回，当前方法的返回一般也意味着 Pod 中的全部卷已经挂载成功了。</p><h3 id="卷管理器"><a href="#卷管理器" class="headerlink" title="卷管理器"></a>卷管理器</h3><p>当前节点卷的管理就都是由 <code>VolumeManager</code> 来负责了，在 Kubernetes 集群中的每一个节点（Node）上的 kubelet 启动时都会运行一个 <code>VolumeManager</code> Goroutine，它会负责在当前节点上的 Pod 和 Volume 发生变动时对 Volume 进行挂载和卸载等操作。</p><pre class="mermaid">graph TD    subgraph Node        VolumeManager-.->Kubelet        DesiredStateOfWorldPopulator-.->VolumeManager        Reconciler-.->VolumeManager    end</pre><p>这个组件会在运行时启动两个 Goroutine 来管理节点中的卷，其中一个是 <code>DesiredStateOfWorldPopulator</code>，另一个是 <code>Reconciler</code>：</p><pre class="mermaid">graph LR    VM(VolumeManager)-. run .->R(Reconciler)    VM-. run .->DSWP(DesiredStateOfWorldPopulator)    DSWP-. update .->DSW[DesiredStateOfWorld]    ASW[ActualStateOfWorld]-. get .->DSWP    DSW-. get .->R    R-. update .->ASW    DSWP-. getpods .->PodManager    style ASW fill:#fffede,stroke:#ebebb7    style DSW fill:#fffede,stroke:#ebebb7</pre><p>如上图所示，这里的 <code>DesiredStateOfWorldPopulator</code> 和 <code>Reconciler</code> 两个 Goroutine 会通过图中两个的 <code>XXXStateOfWorld</code> 状态进行通信，<code>DesiredStateOfWorldPopulator</code> 主要负责从 Kubernetes 节点中获取新的 Pod 对象并更新 <code>DesiredStateOfWorld</code> 结构；而后者会根据实际状态和当前状态的区别对当前节点的状态进行迁移，也就是通过 <code>DesiredStateOfWorld</code> 中状态的变更更新 <code>ActualStateOfWorld</code> 中的内容。</p><p>卷管理器中的两个 Goroutine，一个根据工程师的需求更新节点的期望状态 <code>DesiredStateOfWorld</code>，另一个 Goroutine 保证节点向期望状态『迁移』，也就是说 <code>DesiredStateOfWorldPopulator</code> 是卷管理器中的生产者，而 <code>Reconciler</code> 是消费者，接下来我们会分别介绍这两个 Goroutine 的工作和实现。</p><h4 id="DesiredStateOfWorldPopulator"><a href="#DesiredStateOfWorldPopulator" class="headerlink" title="DesiredStateOfWorldPopulator"></a>DesiredStateOfWorldPopulator</h4><p>作为卷管理器中的消费者，<code>DesiredStateOfWorldPopulator</code> 会根据工程师的请求不断修改当前节点的期望状态，我们可以通过以下的时序图来了解它到底做了哪些工作：</p><pre class="mermaid">sequenceDiagram    participant DSOWP as DesiredStateOfWorldPopulator    participant ASOW as ActualStateOfWorld    participant DSOW as DesiredStateOfWorld    participant PM as PodManager    participant VPM as VolumePluginManager    loop populatorLoop        DSOWP->>+DSOWP: findAndAddNewPods        DSOWP->>+ASOW: GetMountedVolumes        ASOW-->>-DSOWP: mountedVolume        DSOWP->>+PM: GetPods        PM-->>-DSOWP: pods        loop Every Pod            DSOWP->>+DSOW: AddPodToVolume            DSOW->>+VPM: FindPluginBySpec            VPM-->>-DSOW: volumePlugin            DSOW-->>-DSOWP: volumeName        end        deactivate DSOWP        DSOWP->>+DSOWP: findAndRemoveDeletedPods        DSOWP->>+DSOW: GetVolumesToMount        DSOW-->>-DSOWP: volumeToMount        loop Every Volume            DSOWP->>+PM: GetPodByUID            PM-->>-DSOWP: pods            DSOWP->>DSOW: DeletePodFromVolume        end        deactivate DSOWP    end</pre><p>整个 <code>DesiredStateOfWorldPopulator</code> 运行在一个大的循环 <code>populatorLoop</code> 中，当前循环会通过两个方法 <code>findAndAddNewPods</code> 和 <code>findAndRemoveDeletedPods</code> 分别获取节点中被添加的新 Pod 或者已经被删除的老 Pod，获取到 Pod 之后会根据当前的状态修改期望状态：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(dswp *desiredStateOfWorldPopulator)</span> <span class="title">findAndAddNewPods</span><span class="params">()</span></span> &#123;</span><br><span class="line">mountedVolumesForPod := <span class="built_in">make</span>(<span class="keyword">map</span>[volumetypes.UniquePodName]<span class="keyword">map</span>[<span class="keyword">string</span>]cache.MountedVolume)</span><br><span class="line"></span><br><span class="line">processedVolumesForFSResize := sets.NewString()</span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> dswp.podManager.GetPods() &#123;</span><br><span class="line"><span class="keyword">if</span> dswp.isPodTerminated(pod) &#123;</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">&#125;</span><br><span class="line">dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>就像时序图和代码中所描述的，<code>DesiredStateOfWorldPopulator</code> 会从 <code>PodManager</code> 中获取当前节点中的 Pod，随后调用 <code>processPodVolumes</code> 方法为将所有的 Pod 对象加入 <code>DesiredStateOfWorld</code> 结构中：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(dswp *desiredStateOfWorldPopulator)</span> <span class="title">processPodVolumes</span><span class="params">(pod *v1.Pod, mountedVolumesForPod <span class="keyword">map</span>[volumetypes.UniquePodName]<span class="keyword">map</span>[<span class="keyword">string</span>]cache.MountedVolume, processedVolumesForFSResize sets.String)</span></span> &#123;</span><br><span class="line">uniquePodName := util.GetUniquePodName(pod)</span><br><span class="line"><span class="keyword">if</span> dswp.podPreviouslyProcessed(uniquePodName) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mountsMap, devicesMap := dswp.makeVolumeMap(pod.Spec.Containers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, podVolume := <span class="keyword">range</span> pod.Spec.Volumes &#123;</span><br><span class="line">pvc, volumeSpec, volumeGidValue, _ := dswp.createVolumeSpec(podVolume, pod.Name, pod.Namespace, mountsMap, devicesMap)</span><br><span class="line">dswp.desiredStateOfWorld.AddPodToVolume(uniquePodName, pod, volumeSpec, podVolume.Name, volumeGidValue)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dswp.markPodProcessed(uniquePodName)</span><br><span class="line">dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>findAndAddNewPods</code> 方法做的主要就是将节点中加入的新 Pod 添加到 <code>DesiredStateOfWorld</code> 中，而另一个方法 <code>findAndRemoveDeletedPods</code> 其实也做着类似的事情，它会将已经被删除的节点从 <code>DesiredStateOfWorld</code> 中剔除，总而言之 <code>DesiredStateOfWorldPopulator</code> 就是将当前节点的期望状态同步到 <code>DesiredStateOfWorld</code> 中，等待消费者的处理。</p><h4 id="Reconciler"><a href="#Reconciler" class="headerlink" title="Reconciler"></a>Reconciler</h4><p><code>VolumeManager</code> 持有的另一个 Goroutine <code>Reconciler</code> 会负责对当前节点上的 Volume 进行管理，它在正常运行时会启动 <code>reconcile</code> 循环，在这个方法中会分三次对当前状态和期望状态不匹配的卷进行卸载、挂载等操作：</p><pre class="mermaid">sequenceDiagram    participant R as Reconciler    participant ASOW as ActualStateOfWorld    participant DSOW as DesiredStateOfWorld    participant OE as OperationExecutor    loop reconcile        R->>+ASOW: GetMountedVolumes        activate R        ASOW-->>-R: MountedVolumes        R->>DSOW: PodExistsInVolume        R->>OE: UnmountVolume        deactivate R        R->>+DSOW: GetVolumesToMount        activate R        DSOW-->>-R: volumeToMount        R->>ASOW: PodExistsInVolume        R->>OE: AttachVolume/MountVolume        deactivate R        R->>+ASOW: GetUnmountedVolumes        activate R        R->>DSOW: VolumeExists        R->>OE: UnmountDevice/DetachVolume        deactivate R    end</pre><p>在当前的循环中首先会保证应该被卸载但是仍然在节点中存在的卷被卸载，然后将应该挂载的卷挂载到合适的位置，最后将设备与节点分离或者卸载，所有挂载和卸载的操作都是通过 <code>OperationExecutor</code> 完成的，这个结构体负责调用相应的插件执行操作，我们会在文章的后面展开进行介绍。</p><h3 id="附着分离控制器"><a href="#附着分离控制器" class="headerlink" title="附着分离控制器"></a>附着分离控制器</h3><p>除了 <code>VolumeManager</code> 之外，另一个负责管理 Kubernetes 卷的组件就是 <code>AttachDetachController</code> 了，引入这个组件的目的主要是：</p><ol><li>让卷的挂载和卸载能够与节点的可用性脱离；<ul><li>一旦节点或者 kubelet 宕机，附着（Attach）在当前节点上的卷应该能够被分离（Detach），分离之后的卷就能够再次附着到其他节点上；</li></ul></li><li>保证云服务商秘钥的安全；<ul><li>如果每一个 kubelet 都需要触发卷的附着和分离逻辑，那么每一个节点都应该有操作卷的权限，但是这些权限应该只由主节点掌握，这样能够降低秘钥泄露的风险；</li></ul></li><li>提高卷附着和分离部分代码的稳定性；</li></ol><blockquote><p>这些内容都是在 Kubernetes 官方项目的 GitHub issue <a href="https://github.com/kubernetes/kubernetes/issues/20262" target="_blank" rel="external nofollow noopener noreferrer">Detailed Design for Volume Attach/Detach Controller #20262</a> 中讨论的，想要了解 <code>AttachDetachController</code> 出现的原因可以阅读相关的内容。</p></blockquote><p>每一个 <code>AttachDetachController</code> 其实也包含 <code>Reconciler</code> 和 <code>DesiredStateOfWorldPopulator</code> 两个组件，这两个组件虽然与 <code>VolumeManager</code> 中的两个组件重名，实现的功能也非常类似，与 <code>VolumeManager</code> 具有几乎相同的数据流向，但是这两个 Goroutine 是跑在 Kubernetes 主节点中的，所以实现上可能一些差异：</p><pre class="mermaid">graph LR    ADC(AttachDetachController)-. run .->R(Reconciler)    ADC-. run .->DSWP(DesiredStateOfWorldPopulator)    DSWP-. update .->DSW[DesiredStateOfWorld]    ASW[ActualStateOfWorld]-. get .->DSWP    DSW-. get .->R    R-. update .->ASW    DSWP-. getpods .->PodManager    style ASW fill:#fffede,stroke:#ebebb7    style DSW fill:#fffede,stroke:#ebebb7</pre><p>首先，无论是 <code>Reconciler</code> 还是 <code>DesiredStateOfWorldPopulator</code>，它们同步的就不再只是某个节点上 Pod 的信息了，它们需要对整个集群中的 Pod 对象负责，相关数据也不再是通过 apiserver 拉取了，而是使用 <code>podInformer</code> 在 Pod 对象发生变更时调用相应的方法。</p><h4 id="DesiredStateOfWorldPopulator-1"><a href="#DesiredStateOfWorldPopulator-1" class="headerlink" title="DesiredStateOfWorldPopulator"></a>DesiredStateOfWorldPopulator</h4><p>作为 <code>AttachDetachController</code> 启动的 Goroutine，<code>DesiredStateOfWorldPopulator</code> 的主要作用是从当前集群的状态中获取 Pod 对象并修改 <code>DesiredStateOfWorld</code> 结构，与 <code>VolumeManager</code> 中的同名 Goroutine 起到相同的作用，作为整个链路的生产者，它们只是在实现上由于处理 Pod 范围的不同有一些区别：</p><pre class="mermaid">sequenceDiagram    participant DSOWP as DesiredStateOfWorldPopulator    participant ASOW as ActualStateOfWorld    participant DSOW as DesiredStateOfWorld    participant PL as PodLister    participant VPM as VolumePluginManager    loop populatorLoopFunc        DSOWP->>+DSOWP: findAndRemoveDeletedPods        DSOWP->>+DSOW: GetPodToAdd        DSOW-->>-DSOWP: podsToAdd        loop Every Pod            DSOWP->>+PL: GetPod            alt PodNotFound                PL-->>-DSOWP: return                DSOWP->>DSOW: DeletePod            else            end        end        deactivate DSOWP        DSOWP->>+DSOWP: findAndAddActivePods        DSOWP->>+PL: List        PL-->>-DSOWP: pods        loop Every Pod            DSOWP->>+VPM: FindAttachablePluginBySpec            VPM-->>-DSOW: attachableVolumePlugin            DSOWP->>+DSOW: AddPod/DeletePod            DSOW-->>-DSOWP: volumeName        end        deactivate DSOWP   end</pre><p><code>AttachDetachController</code> 中的 <code>DesiredStateOfWorldPopulator</code> 协程就主要会先处理 Pod 的删除逻辑，添加 Pod 的逻辑都是根据 <code>listPodsRetryDuration</code> 的设置周期性被触发的，所以从这里我们就能看到 <code>AttachDetachController</code> 其实主要还是处理被删除 Pod 中 Volume 的分离工作，当节点或者 kubelet 宕机时能够将节点中的卷进行分离，保证 Pod 在其他节点重启时不会出现问题。</p><h4 id="Reconciler-1"><a href="#Reconciler-1" class="headerlink" title="Reconciler"></a>Reconciler</h4><p>另一个用于调节当前状态与期望状态的 Goroutine 在执行它内部的循环时，也会优先处理分离卷的逻辑，后处理附着卷的工作，整个时序图与 <code>VolumeManager</code> 中的 <code>Reconciler</code> 非常相似：</p><pre class="mermaid">sequenceDiagram    participant R as Reconciler    participant ASOW as ActualStateOfWorld    participant DSOW as DesiredStateOfWorld    participant OE as OperationExecutor    loop reconcile        R->>+ASOW: GetAttachedVolumes        activate R        ASOW-->>-R: attachedVolumes        R->>+DSOW: VolumeExists        alt VolumeNotExists            DSOW-->>-R: return            R->>OE: DetachVolume            deactivate R        else        end        R->>+DSOW: GetVolumesToAttach        activate R        DSOW-->>-R: volumeToAttach        R->>+ASOW: VolumeNodeExists        alt VolumeNotExists            ASOW-->>-R: return            R->>OE: AttachVolume        else        end        deactivate R    end</pre><p>这里处理的工作其实相对更少一些，<code>Reconciler</code> 会将期望状态中的卷与实际状态进行比较，然后分离需要分离的卷、附着需要附着的卷，逻辑非常的清晰和简单。</p><h3 id="持久卷控制器"><a href="#持久卷控制器" class="headerlink" title="持久卷控制器"></a>持久卷控制器</h3><p>作为集群中与 PV 和 PVC 打交道的控制器，持久卷控制器同时运行着三个 Goroutine 用于处理相应的逻辑，其中 <code>Resync</code> 协程负责从 Kubernetes 集群中同步 PV 和 PVC 的信息，而另外两个工作协程主要负消费队列中的任务：</p><pre class="mermaid">graph LR    PVC(PVController)-.->R(Resync)    PVC-.->VW(VolumeWorker)    R-. enqueue .->VQ(VolumeQueue)    R-. enqueue .->CQ(ClaimQueue)    VQ-. dequeue .->VW    CQ-. dequeue .->CW    PVC-.->CW(ClaimWorker)    style VQ fill:#fffede,stroke:#ebebb7    style CQ fill:#fffede,stroke:#ebebb7</pre><p>这两个工作协程主要负责对需要绑定或者解绑的 PV 和 PVC 进行处理，例如，当用户创建了新的 PVC 对象时，从集群中查找该 PVC 选择的 PV 并绑定到当前的 PVC 上。</p><h4 id="VolumeWorker"><a href="#VolumeWorker" class="headerlink" title="VolumeWorker"></a>VolumeWorker</h4><p><code>VolumeWorker</code> 协程中执行的最重要的方法其实就是 <code>syncVolume</code>，在这个方法中会根据当前 PV 对象的规格对 PV 和 PVC 进行绑定或者解绑：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ctrl *PersistentVolumeController)</span> <span class="title">syncVolume</span><span class="params">(volume *v1.PersistentVolume)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> volume.Spec.ClaimRef == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> volume.Spec.ClaimRef.UID == <span class="string">""</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> claim *v1.PersistentVolumeClaim</span><br><span class="line">claimName := claimrefToClaimKey(volume.Spec.ClaimRef)</span><br><span class="line">obj, _, _ := ctrl.claims.GetByKey(claimName)</span><br><span class="line">claim, _ = obj.(*v1.PersistentVolumeClaim)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> claim != <span class="literal">nil</span> &amp;&amp; claim.UID != volume.Spec.ClaimRef.UID &#123;</span><br><span class="line">claim = <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> claim == <span class="literal">nil</span> &#123;</span><br><span class="line">ctrl.reclaimVolume(volume)</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> claim.Spec.VolumeName == <span class="string">""</span> &#123;</span><br><span class="line">ctrl.claimQueue.Add(claimToClaimKey(claim))</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> claim.Spec.VolumeName == volume.Name &#123;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">if</span> metav1.HasAnnotation(volume.ObjectMeta, annDynamicallyProvisioned) &amp;&amp; volume.Spec.PersistentVolumeReclaimPolicy == v1.PersistentVolumeReclaimDelete &#123;</span><br><span class="line">ctrl.reclaimVolume(volume)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ctrl.unbindVolume(volume)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果当前 PV 没有绑定的 PVC 对象，那么这里的 <code>reclaimVolume</code> 可能会将当前的 PV 对象根据回收策略将其放回资源池等待重用、回收或者保留；而 <code>unbindVolume</code> 会删除 PV 与 PVC 之间的关系并更新 apiserver 中保存的 Kubernetes 对象数据。</p><h4 id="ClaimWorker"><a href="#ClaimWorker" class="headerlink" title="ClaimWorker"></a>ClaimWorker</h4><p><code>ClaimWorker</code> 就是控制器用来决定如何处理一个 PVC 对象的方法了，它会在一个 PVC 对象被创建、更新或者同步时被触发，<code>syncClaim</code> 会根据当前对象中的注解决定调用 <code>syncUnboundClaim</code> 或者 <code>syncBoundClaim</code> 方法来处理相应的逻辑：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ctrl *PersistentVolumeController)</span> <span class="title">syncClaim</span><span class="params">(claim *v1.PersistentVolumeClaim)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> !metav1.HasAnnotation(claim.ObjectMeta, annBindCompleted) &#123;</span><br><span class="line"><span class="keyword">return</span> ctrl.syncUnboundClaim(claim)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> ctrl.syncBoundClaim(claim)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>syncUnboundClaim</code> 会处理绑定没有结束的 PVC 对象，如果当前 PVC 对象没有对应合适的 PV 存在，那么就会调用 <code>provisionClaim</code> 尝试从集群中获取新的 PV 供应，如果能够找到 PV 对象，就会通过 <code>bind</code> 方法将两者绑定：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ctrl *PersistentVolumeController)</span> <span class="title">syncUnboundClaim</span><span class="params">(claim *v1.PersistentVolumeClaim)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> claim.Spec.VolumeName == <span class="string">""</span> &#123;</span><br><span class="line">delayBinding, err := ctrl.shouldDelayBinding(claim)</span><br><span class="line"></span><br><span class="line">volume, err := ctrl.volumes.findBestMatchForClaim(claim, delayBinding)</span><br><span class="line"><span class="keyword">if</span> volume == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">switch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> delayBinding:</span><br><span class="line"><span class="keyword">case</span> v1helper.GetPersistentVolumeClaimClass(claim) != <span class="string">""</span>:</span><br><span class="line">ctrl.provisionClaim(claim)</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ctrl.bind(volume, claim)</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">obj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)</span><br><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line">volume, _ := obj.(*v1.PersistentVolume)</span><br><span class="line"><span class="keyword">if</span> volume.Spec.ClaimRef == <span class="literal">nil</span> &#123;</span><br><span class="line">ctrl.bind(volume, claim)</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> isVolumeBoundToClaim(volume, claim) &#123;</span><br><span class="line">ctrl.bind(volume, claim)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>绑定的过程其实就是将 PV 和 PVC 之间建立起新的关系，更新 Spec 中的数据让两者能够通过引用 Ref 找到另一个对象并将更新后的 Kubernetes 对象存储到 apiserver 中。</p><p>另一个用于绑定 PV 和 PVC 对象的方法就是 <code>syncBoundClaim</code> 了，相比于 <code>syncUnboundClaim</code> 方法，该方法的实现更为简单，直接从缓存中尝试获取对应的 PV 对象：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ctrl *PersistentVolumeController)</span> <span class="title">syncBoundClaim</span><span class="params">(claim *v1.PersistentVolumeClaim)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> claim.Spec.VolumeName == <span class="string">""</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line">obj, found, _ := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName)</span><br><span class="line"><span class="keyword">if</span> found &#123;</span><br><span class="line">volume, _ := obj.(*v1.PersistentVolume)</span><br><span class="line"><span class="keyword">if</span> volume.Spec.ClaimRef == <span class="literal">nil</span> &#123;</span><br><span class="line">ctrl.bind(volume, claim)</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> volume.Spec.ClaimRef.UID == claim.UID &#123;</span><br><span class="line">ctrl.bind(volume, claim)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果找到了 PV 对象并且该对象没有绑定的 PVC 或者当前 PV 和 PVC 已经存在了引用就会调用 <code>bind</code> 方法对两者进行绑定。</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>无论是 <code>VolumeWorker</code> 还是 <code>ClaimWorker</code> 最终都可能会通过 apiserver 更新集群中 etcd 的数据，当然它们也会调用一些底层的插件获取新的存储供应、删除或者重用一些持久卷，我们会在下面介绍插件的工作原理。</p><h2 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h2><p>Kubernetes 中的所有对卷的操作最终基本都是通过 <code>OperationExecutor</code> 来完成的，这个组件包含了用于附着、挂载、卸载和分离几个常见的操作以及对设备进行操作的一些方法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> OperationExecutor <span class="keyword">interface</span> &#123;</span><br><span class="line">AttachVolume(volumeToAttach VolumeToAttach, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error</span><br><span class="line">DetachVolume(volumeToDetach AttachedVolume, verifySafeToDetach <span class="keyword">bool</span>, actualStateOfWorld ActualStateOfWorldAttacherUpdater) error</span><br><span class="line">MountVolume(waitForAttachTimeout time.Duration, volumeToMount VolumeToMount, actualStateOfWorld ActualStateOfWorldMounterUpdater, isRemount <span class="keyword">bool</span>) error</span><br><span class="line">UnmountVolume(volumeToUnmount MountedVolume, actualStateOfWorld ActualStateOfWorldMounterUpdater, podsDir <span class="keyword">string</span>) error</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现 <code>OperationExecutor</code> 接口的私有结构体会通过 <code>OperatorGenerator</code> 来生成一个用于挂载和卸载卷的方法，并将这个方法包装在一个 <code>GeneratedOperations</code> 结构中，在这之后操作执行器会启动一个新的 Goroutine 用于执行生成好的方法：</p><pre class="mermaid">graph LR    OE(OperationExexutor)-. 1. 获取相关方法 .->OG(OperationGenerator)    OG-. 2. 根据 Spec 获取插件 .->VM(VolumePluginManager)    VM-. 3. 返回 VolumePlugin .->OG    OG-. 4. 构建方法 .->OG    OG-. 5. 生成一个 Operation 结构 .->OE    OE-. 6. 运行 Operation .->NPO(NestedPendingOperations)    NPO-. 7. 启动 Goroutine 运行生成的方法 .->Goroutine</pre><p><code>VolumePluginManager</code> 和 <code>VolumePlugin</code> 这两个组件在整个流程中帮我们屏蔽了底层不同类型卷的实现差异，我们能直接在上层调用完全相同的接口，剩下的逻辑都由底层的插件来负责。</p><p>Kubernetes 提供了插件的概念，通过 <code>Volume</code> 和 <code>Mounter</code> 两个接口支持卷类型的扩展，作为存储提供商或者不同类型的文件系统，我们都可以通过实现以上的两个接口成为 Kubernetes 存储系统中一个新的存储类型：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> VolumePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">Init(host VolumeHost) error</span><br><span class="line">GetPluginName() <span class="keyword">string</span></span><br><span class="line">GetVolumeName(spec *Spec) (<span class="keyword">string</span>, error)</span><br><span class="line">NewMounter(spec *Spec, podRef *v1.Pod, opts VolumeOptions) (Mounter, error)</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Mounter <span class="keyword">interface</span> &#123;</span><br><span class="line">Volume</span><br><span class="line">CanMount() error</span><br><span class="line">SetUp(fsGroup *<span class="keyword">int64</span>) error</span><br><span class="line">SetUpAt(dir <span class="keyword">string</span>, fsGroup *<span class="keyword">int64</span>) error</span><br><span class="line">GetAttributes() Attributes</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这一节中我们将介绍几种不同卷插件的实现，包括最常见的 EmptyDir、ConfigMap、Secret 和 Google 云上的 GCEPersistentDisk，这一节会简单介绍不同卷插件的实现方式，想要了解详细实现的读者可以阅读相关的源代码。</p><h3 id="EmptyDir"><a href="#EmptyDir" class="headerlink" title="EmptyDir"></a>EmptyDir</h3><p>EmptyDir 是 Kubernetes 中最简单的卷了，当我们为一个 Pod 设置一个 EmptyDir 类型的卷时，其实就是在当前 Pod 对应的目录创建了一个空的文件夹，这个文件夹会随着 Pod 的删除而删除。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ed *emptyDir)</span> <span class="title">SetUpAt</span><span class="params">(dir <span class="keyword">string</span>, fsGroup *<span class="keyword">int64</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">ed.setupDir(dir)</span><br><span class="line">volume.SetVolumeOwnership(ed, fsGroup)</span><br><span class="line">volumeutil.SetReady(ed.getMetaDir())</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ed *emptyDir)</span> <span class="title">setupDir</span><span class="params">(dir <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := os.MkdirAll(dir, perm); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>SetUpAt</code> 方法其实就实现了对这种类型卷的创建工作，每当 Pod 被分配到了某个节点上，对应的文件目录就会通过 <code>MkdirAll</code> 方法创建，如果使用者配置了 medium 字段，也会选择使用相应的文件系统挂载到当前目录上，例如：tmpfs、nodev 等。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">k8s.gcr.io/test-webserver</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/cache</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p>我们经常会使用 EmptyDir 类型的卷在多个容器之间共享文件、充当缓存或者保留一些临时的日志，总而言之，这是一种经常被使用的卷类型。</p><h3 id="ConfigMap-和-Secret"><a href="#ConfigMap-和-Secret" class="headerlink" title="ConfigMap 和 Secret"></a>ConfigMap 和 Secret</h3><p>另一种比较常见的卷就是 ConfigMap 了，首先，ConfigMap 本身就是 Kubernetes 中常见的对象了，其中的 <code>data</code> 就是一个存储了从文件名到文件内容的字段，这里的 ConfigMap 对象被挂载到文件目录时就会创建一个名为 <code>redis-config</code> 的文件，然后将文件内容写入该文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">redis-config:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">maxmemory</span> <span class="string">2mb</span></span><br><span class="line">    <span class="string">maxmemory-policy</span> <span class="string">allkeys-lru</span></span><br></pre></td></tr></table></figure><p>在对 ConfigMap 类型的卷进行挂载时，总共需要完成三部分工作，首先从 apiserver 中获取当前 ConfigMap 对象，然后根据当前的 ConfigMap 生成一个从文件名到文件内容的键值对，最后构造一个 Writer 并执行 <code>Write</code> 方法写入内容：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(b *configMapVolumeMounter)</span> <span class="title">SetUpAt</span><span class="params">(dir <span class="keyword">string</span>, fsGroup *<span class="keyword">int64</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">configMap, _ := b.getConfigMap(b.pod.Namespace, b.source.Name)</span><br><span class="line"></span><br><span class="line">totalBytes := totalBytes(configMap)</span><br><span class="line">payload, _ := MakePayload(b.source.Items, configMap, b.source.DefaultMode, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">writerContext := fmt.Sprintf(<span class="string">"pod %v/%v volume %v"</span>, b.pod.Namespace, b.pod.Name, b.volName)</span><br><span class="line">writer, _ := volumeutil.NewAtomicWriter(dir, writerContext)</span><br><span class="line">writer.Write(payload)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在涉及挂载的函数几个中，作者想要着重介绍的也就是在底层直接与文件系统打交道的 <code>writePayloadToDir</code> 方法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(w *AtomicWriter)</span> <span class="title">writePayloadToDir</span><span class="params">(payload <span class="keyword">map</span>[<span class="keyword">string</span>]FileProjection, dir <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">for</span> userVisiblePath, fileProjection := <span class="keyword">range</span> payload &#123;</span><br><span class="line">content := fileProjection.Data</span><br><span class="line">mode := os.FileMode(fileProjection.Mode)</span><br><span class="line">fullPath := path.Join(dir, userVisiblePath)</span><br><span class="line">baseDir, _ := filepath.Split(fullPath)</span><br><span class="line"></span><br><span class="line">os.MkdirAll(baseDir, os.ModePerm)</span><br><span class="line">ioutil.WriteFile(fullPath, content, mode)</span><br><span class="line">os.Chmod(fullPath, mode)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个方法使用了 <code>os</code> 包提供的接口完成了拼接文件名、创建相应文件目录、写入文件并且修改文件模式的工作，将 ConfigMap <code>data</code> 中的数据映射到了一个文件夹中，达到了让 Pod 中的容器可以直接通过文件系统获取内容的目的。</p><p>对于另一个非常常见的卷类型 Secret，Kubernetes 其实也做了几乎完全相同的工作，也是先获取 Secret 对象，然后构建最终写入到文件的键值对，最后初始化一个 Writer 并调用它的 <code>Write</code> 方法，从这里我们也能看出在卷插件这一层对于 ConfigMap 和 Secret 的处理几乎完全相同，并没有出现需要对 Secret 对象中的内容进行解密的工作。</p><h3 id="GCEPersistentDisk"><a href="#GCEPersistentDisk" class="headerlink" title="GCEPersistentDisk"></a>GCEPersistentDisk</h3><p>最后一个要介绍的卷与上面的几种都非常的不同，它在底层使用的是云服务商提供的网络磁盘，想要在一个节点上使用云磁盘其实总共需要两个步骤，首先是要将云磁盘附着到当前的节点上，这部分的工作其实就是由 <code>gcePersistentDiskAttacher</code> 完成的，每当调用 <code>AttachDisk</code> 方法时，最终都会执行云服务商提供的接口，将磁盘附着到相应的节点实例上：</p><pre class="mermaid">sequenceDiagram    participant GPDA as gcePersistentDiskAttacher    participant C as Cloud    participant GCESM as gceServiceManager    participant I as GCEInstances    GPDA->>+C: DiskIsAttached    alt NotAttached        C-->>-GPDA: return NotAttached        GPDA->>+C: AttachDisk        C->>+GCESM: AttachDiskOnCloudProvider        GCESM->>+I: AttachDisk        I-->>-GCESM: return        GCESM-->>-C: return        C-->>-GPDA: return    else    end</pre><p>在方法的最后会将该请求包装成一个 HTTP 的方法调用向 <code>https://www.googleapis.com/compute/v1/projects/{project}/zones/{zone}/instances/{resourceId}/attachDisk</code> 链接发出一个 POST 请求，这个请求会将某个 GCE 上的磁盘附着到目标实例上，详细的内容可以阅读 <a href="https://cloud.google.com/compute/docs/reference/rest/v1/instances/attachDisk" target="_blank" rel="external nofollow noopener noreferrer">相关文档</a>。</p><p>一旦当前的磁盘被附着到了当前节点上，我们就能跟使用其他的插件一样，把磁盘挂载到某个目录上，完成从附着到挂载的全部操作。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Volume 和存储系统是 Kubernetes 非常重要的一部分，它能够帮助我们在多个容器之间共享文件，同时也能够为集群提供持久存储的功能，假如 Kubernetes 没有用于持久存储的对象，我们也很难在集群中运行有状态的服务，例如：消息队列、分布式存储等。</p><p>对于刚刚使用 Kubernetes 的开发者来说，Volume、PV 和 PVC 确实是比较难以理解的概念，但是这却是深入使用 Kubernetes 必须要了解和掌握的，希望这篇文章能够帮助各位读者更好地理解存储系统底层的实现原理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Kubernetes 集群中，虽然无状态的服务非常常见，但是在实际的生产中仍然会需要在集群中部署一些有状态的节点，比如一些存储中间件、消息队列等等。&lt;/p&gt;
    
    </summary>
    
    <content src="http://houmin.cc/https://cosmos-1251905798.cos.ap-beijing.myqcloud.com/blog/2019-10-18_city.jpg" type="image" />
    
    
      <category term="术业专攻" scheme="http://houmin.cc/categories/%E6%9C%AF%E4%B8%9A%E4%B8%93%E6%94%BB/"/>
    
    
      <category term="k8s" scheme="http://houmin.cc/tags/k8s/"/>
    
      <category term="volume" scheme="http://houmin.cc/tags/volume/"/>
    
  </entry>
  
</feed>
